{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature and Target Distribution Analysis\n",
    "\n",
    "This notebook analyzes the distribution of features in relation to target values, focusing on identifying non-Gaussian distributions and unusual patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "\n",
    "# Set plot style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette(\"deep\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['axes.grid'] = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Explore Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../../data/data/train.parquet'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Load dataset\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m../../data/data/train.parquet\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDataset shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdf.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Display first few rows\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/pandas/io/parquet.py:667\u001b[39m, in \u001b[36mread_parquet\u001b[39m\u001b[34m(path, engine, columns, storage_options, use_nullable_dtypes, dtype_backend, filesystem, filters, **kwargs)\u001b[39m\n\u001b[32m    664\u001b[39m     use_nullable_dtypes = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    665\u001b[39m check_dtype_backend(dtype_backend)\n\u001b[32m--> \u001b[39m\u001b[32m667\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimpl\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    668\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    669\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    670\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfilters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    671\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    672\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_nullable_dtypes\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_nullable_dtypes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    673\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    674\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    675\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    676\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/pandas/io/parquet.py:267\u001b[39m, in \u001b[36mPyArrowImpl.read\u001b[39m\u001b[34m(self, path, columns, filters, use_nullable_dtypes, dtype_backend, storage_options, filesystem, **kwargs)\u001b[39m\n\u001b[32m    264\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m manager == \u001b[33m\"\u001b[39m\u001b[33marray\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    265\u001b[39m     to_pandas_kwargs[\u001b[33m\"\u001b[39m\u001b[33msplit_blocks\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m267\u001b[39m path_or_handle, handles, filesystem = \u001b[43m_get_path_or_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    268\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    269\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    270\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    271\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    272\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    273\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    274\u001b[39m     pa_table = \u001b[38;5;28mself\u001b[39m.api.parquet.read_table(\n\u001b[32m    275\u001b[39m         path_or_handle,\n\u001b[32m    276\u001b[39m         columns=columns,\n\u001b[32m   (...)\u001b[39m\u001b[32m    279\u001b[39m         **kwargs,\n\u001b[32m    280\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/pandas/io/parquet.py:140\u001b[39m, in \u001b[36m_get_path_or_handle\u001b[39m\u001b[34m(path, fs, storage_options, mode, is_dir)\u001b[39m\n\u001b[32m    130\u001b[39m handles = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    131\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    132\u001b[39m     \u001b[38;5;129;01mnot\u001b[39;00m fs\n\u001b[32m    133\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_dir\n\u001b[32m   (...)\u001b[39m\u001b[32m    138\u001b[39m     \u001b[38;5;66;03m# fsspec resources can also point to directories\u001b[39;00m\n\u001b[32m    139\u001b[39m     \u001b[38;5;66;03m# this branch is used for example when reading from non-fsspec URLs\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m140\u001b[39m     handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    141\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpath_or_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\n\u001b[32m    142\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    143\u001b[39m     fs = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    144\u001b[39m     path_or_handle = handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/pandas/io/common.py:882\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    873\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(\n\u001b[32m    874\u001b[39m             handle,\n\u001b[32m    875\u001b[39m             ioargs.mode,\n\u001b[32m   (...)\u001b[39m\u001b[32m    878\u001b[39m             newline=\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    879\u001b[39m         )\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m882\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    883\u001b[39m     handles.append(handle)\n\u001b[32m    885\u001b[39m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '../../data/data/train.parquet'"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Load dataset\n",
    "df = pd.read_parquet('data/train.parquet')\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "\n",
    "# Display first few rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Identify target columns and primary target column\n",
    "target_cols = [col for col in df.columns if col.startswith('target_')]\n",
    "feature_cols = [col for col in df.columns if col.startswith('feature_')]\n",
    "\n",
    "print(f\"Number of features: {len(feature_cols)}\")\n",
    "print(f\"Number of targets: {len(target_cols)}\")\n",
    "\n",
    "# For simplicity, we'll use a single target column if it exists, or the first target column\n",
    "if 'target' in df.columns:\n",
    "    target_col = 'target'\n",
    "else:\n",
    "    target_col = target_cols[0]\n",
    "    \n",
    "print(f\"Using target column: {target_col}\")\n",
    "\n",
    "# Check distribution of target values\n",
    "target_distribution = df[target_col].value_counts().sort_index()\n",
    "print(\"\\nTarget value distribution:\")\n",
    "print(target_distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Visualize target distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(df[target_col], kde=True)\n",
    "plt.title(f'Distribution of Target Values - {target_col}')\n",
    "plt.xlabel('Target Value')\n",
    "plt.ylabel('Count')\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Feature Distribution Analysis\n",
    "\n",
    "Let's analyze the distribution of individual features to identify patterns and non-Gaussian distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to check if a distribution is approximately Gaussian\n",
    "def check_gaussian(feature_series):\n",
    "    # For large datasets, take a random sample\n",
    "    sample = feature_series.sample(min(5000, len(feature_series)))\n",
    "    # Use Shapiro-Wilk test for normality\n",
    "    stat, p = stats.shapiro(sample)\n",
    "    return p > 0.05  # Return True if Gaussian (p > 0.05)\n",
    "\n",
    "# Function to calculate skewness and percentage of zeros\n",
    "def calculate_distribution_stats(feature_series):\n",
    "    feature_skew = stats.skew(feature_series)\n",
    "    zero_pct = (feature_series == 0).mean() * 100\n",
    "    return feature_skew, zero_pct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Analyze the first 20 features\n",
    "sample_features = feature_cols[:20]\n",
    "feature_stats = []\n",
    "\n",
    "for feature in sample_features:\n",
    "    is_gaussian = check_gaussian(df[feature])\n",
    "    skewness, zero_pct = calculate_distribution_stats(df[feature])\n",
    "    \n",
    "    feature_stats.append({\n",
    "        'Feature': feature.replace('feature_', ''),\n",
    "        'Is Gaussian': is_gaussian,\n",
    "        'Zero %': f\"{zero_pct:.2f}%\",\n",
    "        'Skewness': f\"{skewness:.4f}\",\n",
    "        'Mean': f\"{df[feature].mean():.4f}\",\n",
    "        'Std': f\"{df[feature].std():.4f}\"\n",
    "    })\n",
    "\n",
    "# Display results as a DataFrame\n",
    "distribution_df = pd.DataFrame(feature_stats)\n",
    "distribution_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Visualize distributions of sample features\n",
    "plt.figure(figsize=(16, 12))\n",
    "\n",
    "for i, feature in enumerate(sample_features[:9]):  # Show first 9 features\n",
    "    plt.subplot(3, 3, i+1)\n",
    "    sns.histplot(df[feature], kde=True, bins=min(20, df[feature].nunique()))\n",
    "    skewness, zero_pct = calculate_distribution_stats(df[feature])\n",
    "    plt.title(f\"{feature.replace('feature_', '')}\\nSkew: {skewness:.2f}, Zero%: {zero_pct:.1f}%\")\n",
    "    plt.xlabel('Value')\n",
    "    plt.ylabel('Count')\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Features with Non-Gaussian Distributions\n",
    "\n",
    "Let's identify and visualize features that deviate from a Gaussian distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Find features with extreme distributions (high skewness or high percentage of zeros)\n",
    "extreme_features = []\n",
    "\n",
    "# Analyze the first 100 features (or all if less than 100)\n",
    "features_to_check = feature_cols[:min(100, len(feature_cols))]\n",
    "\n",
    "for feature in features_to_check:\n",
    "    skewness, zero_pct = calculate_distribution_stats(df[feature])\n",
    "    \n",
    "    # Consider a feature extreme if it has high skewness or many zeros\n",
    "    if abs(skewness) > 1.0 or zero_pct > 30:\n",
    "        extreme_features.append({\n",
    "            'Feature': feature,\n",
    "            'Skewness': skewness,\n",
    "            'Zero %': zero_pct,\n",
    "            'Mean': df[feature].mean(),\n",
    "            'Std': df[feature].std()\n",
    "        })\n",
    "\n",
    "# Sort by skewness (absolute value)\n",
    "extreme_features.sort(key=lambda x: abs(x['Skewness']), reverse=True)\n",
    "\n",
    "# Display extreme features\n",
    "if extreme_features:\n",
    "    extreme_df = pd.DataFrame(extreme_features)\n",
    "    print(f\"Found {len(extreme_df)} features with non-Gaussian distributions\")\n",
    "    extreme_df.head(10)  # Show top 10\n",
    "else:\n",
    "    print(\"No features with extreme distributions found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the most non-Gaussian features\n",
    "if extreme_features:\n",
    "    plt.figure(figsize=(16, 10))\n",
    "    for i, feature_data in enumerate(extreme_features[:6]):  # Top 6 most extreme features\n",
    "        feature = feature_data['Feature']\n",
    "        plt.subplot(2, 3, i+1)\n",
    "        sns.histplot(df[feature], kde=True, bins=min(20, df[feature].nunique()))\n",
    "        plt.title(f\"{feature.replace('feature_', '')}\\nSkew: {feature_data['Skewness']:.2f}, Zero%: {feature_data['Zero %']:.1f}%\")\n",
    "        plt.xlabel('Value')\n",
    "        plt.ylabel('Count')\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Relationship Between Features and Target\n",
    "\n",
    "Now let's analyze how feature distributions vary with different target values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Calculate correlation between features and target\n",
    "# Select a sample of features to keep visualization manageable\n",
    "if len(feature_cols) > 20:\n",
    "    correlation_features = feature_cols[:20] \n",
    "else:\n",
    "    correlation_features = feature_cols\n",
    "\n",
    "# Calculate correlations\n",
    "correlation_df = df[correlation_features + [target_col]].corr()\n",
    "\n",
    "# Get correlation with target only\n",
    "target_correlation = correlation_df[target_col].drop(target_col).sort_values(ascending=False)\n",
    "\n",
    "print(\"Top features correlated with target:\")\n",
    "print(target_correlation.head(10))\n",
    "print(\"\\nBottom features correlated with target:\")\n",
    "print(target_correlation.tail(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Visualize correlation heatmap\n",
    "plt.figure(figsize=(12, 10))\n",
    "mask = np.triu(correlation_df.corr())\n",
    "sns.heatmap(\n",
    "    correlation_df, \n",
    "    annot=True, \n",
    "    cmap='coolwarm', \n",
    "    fmt=\".2f\", \n",
    "    linewidths=0.5,\n",
    "    mask=mask\n",
    ")\n",
    "plt.title('Correlation Between Features and Target')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Select top 3 most correlated features (positive and negative)\n",
    "top_positive = target_correlation.head(3).index.tolist()\n",
    "top_negative = target_correlation.tail(3).index.tolist()\n",
    "top_correlated = top_positive + top_negative\n",
    "\n",
    "# Create box plots to show distribution by target value\n",
    "plt.figure(figsize=(16, 12))\n",
    "\n",
    "for i, feature in enumerate(top_correlated):\n",
    "    plt.subplot(2, 3, i+1)\n",
    "    sns.boxplot(x=target_col, y=feature, data=df)\n",
    "    plt.title(f\"{feature.replace('feature_', '')}\\nCorrelation: {target_correlation[feature]:.4f}\")\n",
    "    plt.xlabel('Target Value')\n",
    "    plt.ylabel('Feature Value')\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualizing Feature Distributions by Target Value\n",
    "\n",
    "Let's examine how the most interesting features are distributed across different target values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot feature distribution by target value\n",
    "def plot_feature_by_target(feature):\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    \n",
    "    # KDE plot - distribution by target value\n",
    "    plt.subplot(1, 2, 1)\n",
    "    for target_val in sorted(df[target_col].unique()):\n",
    "        subset = df[df[target_col] == target_val][feature]\n",
    "        sns.kdeplot(subset, label=f'Target={target_val}')\n",
    "    \n",
    "    plt.title(f'Distribution of {feature.replace(\"feature_\", \"\")}\\nby Target Value')\n",
    "    plt.xlabel('Feature Value')\n",
    "    plt.ylabel('Density')\n",
    "    plt.legend()\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Violin plot\n",
    "    plt.subplot(1, 2, 2)\n",
    "    sns.violinplot(x=target_col, y=feature, data=df, inner=\"quartile\")\n",
    "    plt.title(f'Violin Plot of {feature.replace(\"feature_\", \"\")}\\nby Target Value')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Analyze top correlated features\n",
    "for feature in top_correlated[:3]:  # Analyze top 3\n",
    "    plot_feature_by_target(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's also look at features with high percentage of zeros\n",
    "if extreme_features:\n",
    "    # Sort by zero percentage\n",
    "    zero_features = sorted(extreme_features, key=lambda x: x['Zero %'], reverse=True)\n",
    "    \n",
    "    for feature_data in zero_features[:3]:  # Analyze top 3\n",
    "        feature = feature_data['Feature']\n",
    "        plot_feature_by_target(feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature Value Distribution Across All Features\n",
    "\n",
    "Let's examine the general pattern of feature values across all features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Create a dataframe to hold aggregated statistics about feature distributions\n",
    "feature_meta = []\n",
    "\n",
    "# Only analyze a subset if there are many features\n",
    "features_to_analyze = feature_cols[:min(500, len(feature_cols))]\n",
    "\n",
    "for feature in features_to_analyze:\n",
    "    skewness, zero_pct = calculate_distribution_stats(df[feature])\n",
    "    \n",
    "    feature_meta.append({\n",
    "        'Feature': feature,\n",
    "        'Zero %': zero_pct,\n",
    "        'Mean': df[feature].mean(),\n",
    "        'Median': df[feature].median(),\n",
    "        'Std': df[feature].std(),\n",
    "        'Skewness': skewness,\n",
    "        'Target Correlation': df[[feature, target_col]].corr().iloc[0, 1]\n",
    "    })\n",
    "\n",
    "feature_meta_df = pd.DataFrame(feature_meta)\n",
    "\n",
    "# Show summary statistics\n",
    "feature_meta_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Visualize distribution of feature properties\n",
    "plt.figure(figsize=(18, 10))\n",
    "\n",
    "plt.subplot(2, 3, 1)\n",
    "sns.histplot(feature_meta_df['Mean'], kde=True)\n",
    "plt.title('Distribution of Feature Means')\n",
    "\n",
    "plt.subplot(2, 3, 2)\n",
    "sns.histplot(feature_meta_df['Std'], kde=True)\n",
    "plt.title('Distribution of Feature Standard Deviations')\n",
    "\n",
    "plt.subplot(2, 3, 3)\n",
    "sns.histplot(feature_meta_df['Skewness'], kde=True)\n",
    "plt.title('Distribution of Feature Skewness')\n",
    "\n",
    "plt.subplot(2, 3, 4)\n",
    "sns.histplot(feature_meta_df['Zero %'], kde=True)\n",
    "plt.title('Distribution of Zero Percentages')\n",
    "\n",
    "plt.subplot(2, 3, 5)\n",
    "sns.histplot(feature_meta_df['Target Correlation'].abs(), kde=True)\n",
    "plt.title('Distribution of |Target Correlation|')\n",
    "\n",
    "plt.subplot(2, 3, 6)\n",
    "sns.scatterplot(x='Skewness', y='Zero %', data=feature_meta_df, alpha=0.6)\n",
    "plt.title('Skewness vs. Zero Percentage')\n",
    "plt.axhline(y=20, color='r', linestyle='--')\n",
    "plt.axvline(x=0, color='r', linestyle='--')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Conclusion and Feature Recommendations\n",
    "\n",
    "Based on our analysis, here's a summary of the feature distributions and their relationship with target values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Identify features with non-Gaussian distributions and high correlation with target\n",
    "high_correlation_threshold = 0.1\n",
    "high_skew_threshold = 0.5\n",
    "high_zero_threshold = 25.0\n",
    "\n",
    "# Find interesting features\n",
    "interesting_features = feature_meta_df[\n",
    "    (feature_meta_df['Target Correlation'].abs() > high_correlation_threshold) | \n",
    "    (feature_meta_df['Skewness'].abs() > high_skew_threshold) | \n",
    "    (feature_meta_df['Zero %'] > high_zero_threshold)\n",
    "].copy()\n",
    "\n",
    "# Add a column indicating why a feature is interesting\n",
    "def get_reason(row):\n",
    "    reasons = []\n",
    "    if abs(row['Target Correlation']) > high_correlation_threshold:\n",
    "        reasons.append(\"High correlation\")\n",
    "    if abs(row['Skewness']) > high_skew_threshold:\n",
    "        reasons.append(\"Skewed distribution\")\n",
    "    if row['Zero %'] > high_zero_threshold:\n",
    "        reasons.append(\"Many zeros\")\n",
    "    return \", \".join(reasons)\n",
    "\n",
    "interesting_features['Reason'] = interesting_features.apply(get_reason, axis=1)\n",
    "\n",
    "# Sort by absolute correlation with target\n",
    "interesting_features = interesting_features.sort_values(\n",
    "    by='Target Correlation', \n",
    "    key=abs, \n",
    "    ascending=False\n",
    ")\n",
    "\n",
    "# Display the most interesting features\n",
    "print(f\"Found {len(interesting_features)} interesting features\")\n",
    "interesting_features[['Feature', 'Target Correlation', 'Zero %', 'Skewness', 'Reason']].head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Final recommendation and summary\n",
    "print(\"Feature Distribution Analysis Summary:\")\n",
    "print(\"----------------------------------------\")\n",
    "print(f\"Total features analyzed: {len(feature_meta_df)}\")\n",
    "print(f\"Features with high correlation to target: {len(feature_meta_df[feature_meta_df['Target Correlation'].abs() > high_correlation_threshold])}\")\n",
    "print(f\"Features with highly skewed distributions: {len(feature_meta_df[feature_meta_df['Skewness'].abs() > high_skew_threshold])}\")\n",
    "print(f\"Features with high percentage of zeros: {len(feature_meta_df[feature_meta_df['Zero %'] > high_zero_threshold])}\")\n",
    "\n",
    "# Get the top 3 features most correlated with target\n",
    "top_features = feature_meta_df.sort_values('Target Correlation', key=abs, ascending=False).head(3)['Feature'].tolist()\n",
    "print(\"\\nTop 3 features most correlated with target:\")\n",
    "for idx, feature in enumerate(top_features, 1):\n",
    "    print(f\"{idx}. {feature.replace('feature_', '')}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
