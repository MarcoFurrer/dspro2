{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24c40b3d",
   "metadata": {},
   "source": [
    "# Simplified Optimizer Evaluation\n",
    "\n",
    "This notebook implements a streamlined workflow for optimizer evaluation with three core steps:\n",
    "\n",
    "1. Train models with different optimizers via subprocess calls to main.py\n",
    "2. Scan for experiment results in the logs directory\n",
    "3. Create performance comparison matrices for visualization\n",
    "\n",
    "This approach is more maintainable and focused on optimizer performance analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8c67f8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project Root: /Users/marcofurrer/Documents/github/dspro2\n",
      "Logs Directory: /Users/marcofurrer/Documents/github/dspro2/logs\n",
      "Metrics Directory: /Users/marcofurrer/Documents/github/dspro2/logs/metrics\n",
      "Exports Directory: /Users/marcofurrer/Documents/github/dspro2/exports\n",
      "Models to evaluate: ['Base', 'Wide', 'Advanced']\n",
      "Optimizers to evaluate: ['Adam', 'ImprovedAdam', 'Nadam', 'RMSprop', 'SGD', 'Adadelta']\n",
      "Selected models to evaluate: ['Base', 'Wide', 'Advanced']\n",
      "Selected optimizers to evaluate: ['Adam', 'ImprovedAdam', 'Nadam', 'RMSprop', 'SGD']\n"
     ]
    }
   ],
   "source": [
    "# Import utility functions\n",
    "from optimizer_utils import (\n",
    "    # Constants\n",
    "    PROJECT_ROOT, LOGS_DIR, METRICS_DIR, EXPORTS_DIR, MAIN_SCRIPT,\n",
    "    MODELS, OPTIMIZERS, EPOCHS, BATCH_SIZE,\n",
    "    \n",
    "    # Functions\n",
    "    train_model, run_experiments, scan_for_experiment_results,\n",
    "    load_experiment_results, get_summary_metrics,\n",
    "    plot_learning_curves, create_performance_matrix, find_best_optimizer\n",
    ")\n",
    "\n",
    "# Import standard libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Set up models and optimizers to evaluate\n",
    "selected_models = ['Base', 'Wide', 'Advanced']\n",
    "selected_optimizers = ['Adam', 'ImprovedAdam', 'Nadam', 'RMSprop', 'SGD']\n",
    "\n",
    "print(f\"Selected models to evaluate: {selected_models}\")\n",
    "print(f\"Selected optimizers to evaluate: {selected_optimizers}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9122961e",
   "metadata": {},
   "source": [
    "## Step 1: Train Models via Subprocess\n",
    "\n",
    "We'll use subprocess calls to train models with different optimizers. You can choose to run all combinations or specific ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98c3afb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Base model with Adam optimizer\n",
      "Running: python /Users/marcofurrer/Documents/github/dspro2/main.py --model base --optimizer Adam --epochs 15 --batch_size 32\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m model, optimizer \u001b[38;5;129;01min\u001b[39;00m pairs:\n\u001b[1;32m     13\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTraining \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m model with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moptimizer\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m optimizer\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 14\u001b[0m         result_path \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mResults saved to: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/Documents/github/dspro2/notebooks/optimizer_eval/optimizer_utils.py:101\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model_name, optimizer_name, epochs, batch_size)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRunning: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(cmd)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    100\u001b[0m \u001b[38;5;66;03m# Execute the command\u001b[39;00m\n\u001b[0;32m--> 101\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43msubprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcmd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcapture_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;66;03m# Print the result\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28mprint\u001b[39m(result\u001b[38;5;241m.\u001b[39mstdout)\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/subprocess.py:507\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    505\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Popen(\u001b[38;5;241m*\u001b[39mpopenargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;28;01mas\u001b[39;00m process:\n\u001b[1;32m    506\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 507\u001b[0m         stdout, stderr \u001b[38;5;241m=\u001b[39m \u001b[43mprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcommunicate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    508\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m TimeoutExpired \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    509\u001b[0m         process\u001b[38;5;241m.\u001b[39mkill()\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/subprocess.py:1134\u001b[0m, in \u001b[0;36mPopen.communicate\u001b[0;34m(self, input, timeout)\u001b[0m\n\u001b[1;32m   1131\u001b[0m     endtime \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1133\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1134\u001b[0m     stdout, stderr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_communicate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mendtime\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1135\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1136\u001b[0m     \u001b[38;5;66;03m# https://bugs.python.org/issue25942\u001b[39;00m\n\u001b[1;32m   1137\u001b[0m     \u001b[38;5;66;03m# See the detailed comment in .wait().\u001b[39;00m\n\u001b[1;32m   1138\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/subprocess.py:1979\u001b[0m, in \u001b[0;36mPopen._communicate\u001b[0;34m(self, input, endtime, orig_timeout)\u001b[0m\n\u001b[1;32m   1972\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_timeout(endtime, orig_timeout,\n\u001b[1;32m   1973\u001b[0m                         stdout, stderr,\n\u001b[1;32m   1974\u001b[0m                         skip_check_and_raise\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   1975\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(  \u001b[38;5;66;03m# Impossible :)\u001b[39;00m\n\u001b[1;32m   1976\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_check_timeout(..., skip_check_and_raise=True) \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   1977\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfailed to raise TimeoutExpired.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m-> 1979\u001b[0m ready \u001b[38;5;241m=\u001b[39m \u001b[43mselector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1980\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_timeout(endtime, orig_timeout, stdout, stderr)\n\u001b[1;32m   1982\u001b[0m \u001b[38;5;66;03m# XXX Rewrite these to use non-blocking I/O on the file\u001b[39;00m\n\u001b[1;32m   1983\u001b[0m \u001b[38;5;66;03m# objects; they are no longer using C stdio!\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/selectors.py:416\u001b[0m, in \u001b[0;36m_PollLikeSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    414\u001b[0m ready \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    415\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 416\u001b[0m     fd_event_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_selector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpoll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Option 1: Run specific model-optimizer combinations\n",
    "# Set pairs to a list of (model, optimizer) tuples you want to train\n",
    "pairs = [\n",
    "    ('Base', 'Adam'),\n",
    "    ('Wide', 'ImprovedAdam')\n",
    "]\n",
    "\n",
    "# Set run_specific to True to execute this cell\n",
    "run_specific = True\n",
    "\n",
    "if run_specific and pairs:\n",
    "    for model, optimizer in pairs:\n",
    "        print(f\"\\nTraining {model} model with {optimizer} optimizer\")\n",
    "        result_path = train_model(model, optimizer, epochs=EPOCHS, batch_size=BATCH_SIZE)\n",
    "        print(f\"Results saved to: {result_path}\")\n",
    "else:\n",
    "    print(\"Skipping specific experiments. Set run_specific=True to execute.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee67ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 2: Run all combinations of models and optimizers\n",
    "run_all = False  # Set to True to run all experiments\n",
    "\n",
    "if run_all:\n",
    "    print(f\"Running all combinations - {len(selected_models) * len(selected_optimizers)} experiments\")\n",
    "    experiment_results = run_experiments(selected_models, selected_optimizers, \n",
    "                                        epochs=EPOCHS, batch_size=BATCH_SIZE)\n",
    "else:\n",
    "    print(\"Skipping full experiment suite. Set run_all=True to execute all combinations.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3c6322",
   "metadata": {},
   "source": [
    "## Step 2: Scan for Experiment Results\n",
    "\n",
    "Now we'll scan the logs directory to find all existing experiment results. This will find both results from the experiments we just ran and any previous ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533f8e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scan for experiment results\n",
    "print(\"Scanning for experiment results in logs directory and exports...\")\n",
    "experiment_paths = scan_for_experiment_results()\n",
    "\n",
    "# Display summary of found experiments\n",
    "if experiment_paths:\n",
    "    print(f\"\\nFound {len(experiment_paths)} experiment results:\")\n",
    "    # Group by model\n",
    "    model_groups = {}\n",
    "    for (model, optimizer) in experiment_paths:\n",
    "        if model not in model_groups:\n",
    "            model_groups[model] = []\n",
    "        model_groups[model].append(optimizer)\n",
    "    \n",
    "    # Display grouped results\n",
    "    for model, optimizers in model_groups.items():\n",
    "        print(f\"\\n{model} model with optimizers: {', '.join(optimizers)}\")\n",
    "else:\n",
    "    print(\"No experiment results found. Please run training first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593a05a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load metrics from experiment results\n",
    "print(\"Loading metrics from experiment results...\")\n",
    "all_metrics = load_experiment_results(experiment_paths)\n",
    "\n",
    "if not all_metrics.empty:\n",
    "    # Display the first few rows\n",
    "    print(\"Metrics preview:\")\n",
    "    display(all_metrics.head())\n",
    "    \n",
    "    # Get unique models and optimizers in the data\n",
    "    found_models = sorted(all_metrics['model'].unique())\n",
    "    found_optimizers = sorted(all_metrics['optimizer'].unique())\n",
    "    \n",
    "    print(f\"\\nFound metrics for models: {found_models}\")\n",
    "    print(f\"Found metrics for optimizers: {found_optimizers}\")\n",
    "    \n",
    "    # Calculate summary metrics\n",
    "    print(\"\\nCalculating summary metrics...\")\n",
    "    summary_metrics = get_summary_metrics(all_metrics)\n",
    "    display(summary_metrics)\n",
    "else:\n",
    "    print(\"No metrics data found. Please check experiment results.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c367b1b",
   "metadata": {},
   "source": [
    "## Step 3: Create Performance Comparison Matrix\n",
    "\n",
    "Now we'll generate performance comparison matrices to visualize how different optimizers perform across model architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93c845e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot learning curves for validation loss\n",
    "if not all_metrics.empty:\n",
    "    print(\"Plotting validation loss learning curves...\")\n",
    "    plot_learning_curves(all_metrics, 'val_loss')\n",
    "else:\n",
    "    print(\"No metrics data available for plotting learning curves.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ead8aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot learning curves for validation accuracy (if available)\n",
    "if not all_metrics.empty and 'val_accuracy' in all_metrics.columns:\n",
    "    print(\"Plotting validation accuracy learning curves...\")\n",
    "    plot_learning_curves(all_metrics, 'val_accuracy')\n",
    "else:\n",
    "    print(\"No validation accuracy data available for plotting learning curves.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9a8a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create performance matrices for different metrics\n",
    "if not summary_metrics.empty:\n",
    "    print(\"Creating performance matrices...\")\n",
    "    \n",
    "    # Validation Loss Matrix\n",
    "    print(\"\\nMinimum Validation Loss by Model and Optimizer:\")\n",
    "    loss_matrix = create_performance_matrix(\n",
    "        summary_metrics, \n",
    "        'min_val_loss', \n",
    "        'Minimum Validation Loss by Model and Optimizer'\n",
    "    )\n",
    "    \n",
    "    # Validation Accuracy Matrix (if available)\n",
    "    if 'max_val_accuracy' in summary_metrics.columns and not summary_metrics['max_val_accuracy'].isna().all():\n",
    "        print(\"\\nMaximum Validation Accuracy by Model and Optimizer:\")\n",
    "        acc_matrix = create_performance_matrix(\n",
    "            summary_metrics, \n",
    "            'max_val_accuracy', \n",
    "            'Maximum Validation Accuracy by Model and Optimizer'\n",
    "        )\n",
    "    \n",
    "    # Convergence Speed Matrix\n",
    "    print(\"\\nConvergence Epoch by Model and Optimizer:\")\n",
    "    conv_matrix = create_performance_matrix(\n",
    "        summary_metrics, \n",
    "        'convergence_epoch', \n",
    "        'Convergence Epoch by Model and Optimizer'\n",
    "    )\n",
    "    \n",
    "    # Training Time Matrix (if available)\n",
    "    if 'training_time' in summary_metrics.columns and not summary_metrics['training_time'].isna().all():\n",
    "        print(\"\\nTotal Training Time (s) by Model and Optimizer:\")\n",
    "        time_matrix = create_performance_matrix(\n",
    "            summary_metrics,\n",
    "            'training_time',\n",
    "            'Total Training Time (s) by Model and Optimizer'\n",
    "        )\n",
    "else:\n",
    "    print(\"No summary metrics available for creating performance matrices.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f633e84",
   "metadata": {},
   "source": [
    "## Optimizer Comparison Results\n",
    "\n",
    "Let's identify the best optimizers for each model architecture based on different metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e2937e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find best optimizer for different metrics\n",
    "if not summary_metrics.empty:\n",
    "    print(\"Best optimizer by validation loss:\")\n",
    "    display(find_best_optimizer(summary_metrics, 'min_val_loss', True))\n",
    "    \n",
    "    if 'max_val_accuracy' in summary_metrics.columns and not summary_metrics['max_val_accuracy'].isna().all():\n",
    "        print(\"\\nBest optimizer by validation accuracy:\")\n",
    "        display(find_best_optimizer(summary_metrics, 'max_val_accuracy', False))\n",
    "    \n",
    "    print(\"\\nBest optimizer by convergence speed:\")\n",
    "    display(find_best_optimizer(summary_metrics, 'convergence_epoch', True))\n",
    "    \n",
    "    if 'training_time' in summary_metrics.columns and not summary_metrics['training_time'].isna().all():\n",
    "        print(\"\\nBest optimizer by training time:\")\n",
    "        display(find_best_optimizer(summary_metrics, 'training_time', True))\n",
    "else:\n",
    "    print(\"No summary metrics available for finding best optimizer.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e962da7",
   "metadata": {},
   "source": [
    "## Conclusions and Recommendations\n",
    "\n",
    "Based on our analysis, we can summarize the performance of different optimizers across model architectures:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e0c63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a ranking of optimizers based on multiple metrics\n",
    "if not summary_metrics.empty:\n",
    "    # Group by optimizer and calculate average ranks\n",
    "    optimizer_ranks = {}\n",
    "    \n",
    "    # For validation loss (lower is better)\n",
    "    for model in summary_metrics['model'].unique():\n",
    "        model_data = summary_metrics[summary_metrics['model'] == model]\n",
    "        # Rank optimizers for this model (rank 1 is best)\n",
    "        ranked = model_data.sort_values('min_val_loss')['optimizer'].tolist()\n",
    "        \n",
    "        for i, opt in enumerate(ranked):\n",
    "            if opt not in optimizer_ranks:\n",
    "                optimizer_ranks[opt] = {'loss_rank': [], 'conv_rank': [], 'total': 0, 'count': 0}\n",
    "            optimizer_ranks[opt]['loss_rank'].append(i+1)\n",
    "    \n",
    "    # For convergence epoch (lower is better)\n",
    "    for model in summary_metrics['model'].unique():\n",
    "        model_data = summary_metrics[summary_metrics['model'] == model]\n",
    "        # Rank optimizers for this model (rank 1 is best)\n",
    "        ranked = model_data.sort_values('convergence_epoch')['optimizer'].tolist()\n",
    "        \n",
    "        for i, opt in enumerate(ranked):\n",
    "            optimizer_ranks[opt]['conv_rank'].append(i+1)\n",
    "    \n",
    "    # Calculate average ranks\n",
    "    rank_data = []\n",
    "    for opt, ranks in optimizer_ranks.items():\n",
    "        avg_loss_rank = sum(ranks['loss_rank']) / len(ranks['loss_rank']) if ranks['loss_rank'] else 0\n",
    "        avg_conv_rank = sum(ranks['conv_rank']) / len(ranks['conv_rank']) if ranks['conv_rank'] else 0\n",
    "        overall_rank = (avg_loss_rank + avg_conv_rank) / 2\n",
    "        \n",
    "        rank_data.append({\n",
    "            'optimizer': opt,\n",
    "            'avg_loss_rank': avg_loss_rank,\n",
    "            'avg_conv_rank': avg_conv_rank,\n",
    "            'overall_rank': overall_rank\n",
    "        })\n",
    "    \n",
    "    # Create DataFrame and sort by overall rank\n",
    "    rank_df = pd.DataFrame(rank_data).sort_values('overall_rank')\n",
    "    \n",
    "    print(\"Overall optimizer ranking (lower is better):\")\n",
    "    display(rank_df)\n",
    "    \n",
    "    # Plot the rankings\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(y='optimizer', x='overall_rank', data=rank_df, palette='viridis')\n",
    "    plt.title('Overall Optimizer Ranking (Lower is Better)')\n",
    "    plt.xlabel('Average Rank')\n",
    "    plt.ylabel('Optimizer')\n",
    "    plt.grid(True, axis='x', linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No summary metrics available for creating rankings.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d161727",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the results for future reference\n",
    "if not summary_metrics.empty:\n",
    "    # Create a timestamp\n",
    "    from datetime import datetime\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    # Create output directory if it doesn't exist\n",
    "    from pathlib import Path\n",
    "    output_dir = Path(\"optimizer_results\")\n",
    "    output_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Save the summary metrics\n",
    "    summary_metrics.to_csv(output_dir / f\"optimizer_comparison_summary_{timestamp}.csv\", index=False)\n",
    "    \n",
    "    # Save the ranking if available\n",
    "    if 'rank_df' in locals():\n",
    "        rank_df.to_csv(output_dir / f\"optimizer_ranking_{timestamp}.csv\", index=False)\n",
    "        \n",
    "    # Save performance matrices if they exist\n",
    "    matrices = {\n",
    "        'loss_matrix': 'validation_loss',\n",
    "        'acc_matrix': 'validation_accuracy',\n",
    "        'conv_matrix': 'convergence_epoch',\n",
    "        'time_matrix': 'training_time'\n",
    "    }\n",
    "    \n",
    "    for var_name, file_prefix in matrices.items():\n",
    "        if var_name in locals():\n",
    "            locals()[var_name].to_csv(output_dir / f\"{file_prefix}_matrix_{timestamp}.csv\")\n",
    "        \n",
    "    print(f\"Results saved with timestamp {timestamp} in {output_dir}\")\n",
    "else:\n",
    "    print(\"No results to save.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e401e6b6",
   "metadata": {},
   "source": [
    "## Summary of Findings\n",
    "\n",
    "Based on our analysis, we can draw the following conclusions about optimizers and their performance across different model architectures:\n",
    "\n",
    "1. **Best Overall Optimizer**: [Fill in based on results]\n",
    "\n",
    "2. **Model-Specific Recommendations**:\n",
    "   - For **Base** model: [Fill based on results]\n",
    "   - For **Wide** model: [Fill based on results]\n",
    "   - For **Advanced** model: [Fill based on results]\n",
    "\n",
    "3. **Performance Characteristics**:\n",
    "   - **Convergence Speed**: [Fill based on results]\n",
    "   - **Final Performance**: [Fill based on results]\n",
    "   - **Training Efficiency**: [Fill based on results]\n",
    "\n",
    "4. **Practical Recommendations**:\n",
    "   - For quick prototyping: [Fill based on results]\n",
    "   - For final model training: [Fill based on results]\n",
    "   - For complex architectures: [Fill based on results]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
