{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99136af9",
   "metadata": {},
   "source": [
    "# Optimizer Evaluation Across Model Architectures\n",
    "\n",
    "This notebook provides a comprehensive analysis of different optimizers across various model architectures for our machine learning project. We'll evaluate performance metrics, convergence rates, and overall model quality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795930de",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "1. [Setup and Configuration](#setup-and-configuration)\n",
    "2. [Model Architectures Overview](#model-architectures-overview)\n",
    "3. [Optimizer Definitions](#optimizer-definitions)\n",
    "4. [Training Automation](#training-automation)\n",
    "5. [Performance Visualization](#performance-visualization)\n",
    "6. [Optimizer Comparison](#optimizer-comparison)\n",
    "7. [Best Optimizer Analysis](#best-optimizer-analysis)\n",
    "8. [Conclusions and Recommendations](#conclusions-and-recommendations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36be75aa",
   "metadata": {},
   "source": [
    "## Setup and Configuration\n",
    "\n",
    "Let's import the necessary libraries and set up our environment for the optimizer evaluation experiments. We'll configure paths, import required modules, and define visualization settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b49bc6b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project Root: /Users/marcofurrer/Documents/github/dspro2\n",
      "Results Directory: /Users/marcofurrer/Documents/github/dspro2/logs\n",
      "Models to evaluate: ['Base', 'Wide', 'Advanced']\n",
      "Optimizers to evaluate: ['Adam', 'ImprovedAdam', 'Nadam', 'RMSprop', 'SGD', 'Adadelta']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import subprocess\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Add parent directory to path to import project modules\n",
    "# This ensures that imports from 'src' will work correctly\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '../..')))\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('ggplot')\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "# Configure matplotlib for better visualization\n",
    "plt.rcParams['figure.figsize'] = (14, 8)\n",
    "plt.rcParams['axes.labelsize'] = 12\n",
    "plt.rcParams['axes.titlesize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 10\n",
    "plt.rcParams['ytick.labelsize'] = 10\n",
    "plt.rcParams['legend.fontsize'] = 10\n",
    "plt.rcParams['figure.dpi'] = 150\n",
    "\n",
    "# Configuration parameters\n",
    "PROJECT_ROOT = Path(os.path.abspath(os.path.join(os.getcwd(), '../..')))\n",
    "RESULTS_DIR = PROJECT_ROOT / 'logs'\n",
    "MAIN_SCRIPT = PROJECT_ROOT / 'main.py'\n",
    "\n",
    "# Define models and optimizers to evaluate\n",
    "MODELS = ['Base', 'Wide', 'Advanced']\n",
    "OPTIMIZERS = ['Adam', 'ImprovedAdam', 'Nadam', 'RMSprop', 'SGD', 'Adadelta']\n",
    "EPOCHS = 15  # Use fewer epochs for faster evaluation\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "print(f\"Project Root: {PROJECT_ROOT}\")\n",
    "print(f\"Results Directory: {RESULTS_DIR}\")\n",
    "print(f\"Models to evaluate: {MODELS}\")\n",
    "print(f\"Optimizers to evaluate: {OPTIMIZERS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b4ae6d8",
   "metadata": {},
   "source": [
    "## Model Architectures Overview\n",
    "\n",
    "In this section, we'll examine the different model architectures available in our project. We'll load the architectures from our code base and provide a summary of each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d67e20d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading model architectures: No module named 'src'\n",
      "Will continue with experiment configuration without model summaries.\n"
     ]
    }
   ],
   "source": [
    "# Import model architectures directly from their module files\n",
    "try:\n",
    "    # Import all possible models\n",
    "    from src.models.Deep import model as deep_model\n",
    "    from src.models.CorrelationModel import model as correlation_model\n",
    "    from src.models.ImprovedModel import model as improved_model\n",
    "    from src.models.BestModel import model as best_model\n",
    "    from src.models.Residual import model as residual_model\n",
    "    from src.models.Advanced import model as advanced_model\n",
    "    from src.models.Wide import model as wide_model\n",
    "    from src.models.Base import model as base_model\n",
    "    \n",
    "    # Create a lookup dictionary of imported models\n",
    "    available_models = {\n",
    "        'Deep': deep_model,\n",
    "        'Correlation': correlation_model,\n",
    "        'Improved': improved_model,\n",
    "        'Best': best_model,\n",
    "        'Residual': residual_model,\n",
    "        'Advanced': advanced_model,\n",
    "        'Wide': wide_model,\n",
    "        'Base': base_model\n",
    "    }\n",
    "    \n",
    "    # Display summaries for selected models only\n",
    "    for model_name in MODELS:\n",
    "        if model_name in available_models:\n",
    "            print(f\"\\n\\n{'='*50}\")\n",
    "            print(f\"Model Architecture: {model_name}\")\n",
    "            print(f\"{'='*50}\")\n",
    "            available_models[model_name].summary()\n",
    "        else:\n",
    "            print(f\"Warning: Model '{model_name}' not found in available models\")\n",
    "            \n",
    "except Exception as e:\n",
    "    print(f\"Error loading model architectures: {e}\")\n",
    "    print(\"Will continue with experiment configuration without model summaries.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ff164d",
   "metadata": {},
   "source": [
    "## Optimizer Definitions\n",
    "\n",
    "Let's define the optimizers we want to evaluate and their configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c04c962",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the optimizers\n",
    "try:\n",
    "    from tensorflow.keras.optimizers import (\n",
    "        Adam, SGD, RMSprop, Nadam, Adagrad, Adadelta, Adamax\n",
    "    )\n",
    "    \n",
    "    # Try importing custom optimizers if available\n",
    "    try:\n",
    "        from src.optimizers.Adam import optimizer as adam_optimizer\n",
    "        from src.optimizers.ImprovedAdam import optimizer as improved_adam_optimizer\n",
    "        from src.optimizers.Nadam import optimizer as nadam_optimizer\n",
    "        from src.optimizers.RMSprop import optimizer as rmsprop_optimizer\n",
    "        from src.optimizers.SGD import optimizer as sgd_optimizer\n",
    "        from src.optimizers.Adadelta import optimizer as adadelta_optimizer\n",
    "        from src.optimizers.Adagrad import optimizer as adagrad_optimizer\n",
    "        from src.optimizers.Adamax import optimizer as adamax_optimizer\n",
    "        custom_optimizers = True\n",
    "    except ImportError:\n",
    "        print(\"Using standard TensorFlow optimizers as custom optimizers not found\")\n",
    "        custom_optimizers = False\n",
    "    \n",
    "    # Create a dict of optimizers\n",
    "    if custom_optimizers:\n",
    "        optimizers = {\n",
    "            'Adam': adam_optimizer,\n",
    "            'ImprovedAdam': improved_adam_optimizer,\n",
    "            'Nadam': nadam_optimizer,\n",
    "            'RMSprop': rmsprop_optimizer,\n",
    "            'SGD': sgd_optimizer,\n",
    "            'Adadelta': adadelta_optimizer,\n",
    "            'Adagrad': adagrad_optimizer,\n",
    "            'Adamax': adamax_optimizer\n",
    "        }\n",
    "    else:\n",
    "        # Create standard optimizers with default configs\n",
    "        optimizers = {\n",
    "            'Adam': Adam(),\n",
    "            'SGD': SGD(),\n",
    "            'RMSprop': RMSprop(),\n",
    "            'Nadam': Nadam(),\n",
    "            'Adagrad': Adagrad(),\n",
    "            'Adadelta': Adadelta(),\n",
    "            'Adamax': Adamax()\n",
    "        }\n",
    "        \n",
    "    # Print optimizer configurations\n",
    "    for name, optimizer in optimizers.items():\n",
    "        if name in OPTIMIZERS:  # Only show selected optimizers\n",
    "            print(f\"Optimizer: {name}\")\n",
    "            print(f\"Configuration: {optimizer.get_config()}\")\n",
    "            print(\"-\" * 50)\n",
    "except Exception as e:\n",
    "    print(f\"Error loading optimizers: {e}\")\n",
    "    print(\"Will continue with experiment configuration without optimizer details.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b507a62d",
   "metadata": {},
   "source": [
    "## Training Automation\n",
    "\n",
    "In this section, we'll create functions to automate the training process across different model architectures and optimizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b7cc80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model_name, optimizer_name, epochs=EPOCHS, batch_size=BATCH_SIZE):\n",
    "    \"\"\"\n",
    "    Trains a model with specified architecture and optimizer by calling main.py\n",
    "    \n",
    "    Args:\n",
    "        model_name (str): Name of the model architecture to use\n",
    "        optimizer_name (str): Name of the optimizer to use\n",
    "        epochs (int): Number of epochs for training\n",
    "        batch_size (int): Batch size for training\n",
    "        \n",
    "    Returns:\n",
    "        str: Path to the results directory\n",
    "    \"\"\"\n",
    "    # Generate a unique run ID\n",
    "    run_id = f\"{model_name}_{optimizer_name}_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "    \n",
    "    # Command to run main.py with appropriate arguments\n",
    "    cmd = [\n",
    "        \"python\", str(MAIN_SCRIPT),\n",
    "        \"--model\", model_name,\n",
    "        \"--optimizer\", optimizer_name,\n",
    "        \"--epochs\", str(epochs),\n",
    "        \"--batch_size\", str(batch_size),\n",
    "        \"--run_id\", run_id,\n",
    "        \"--verbose\", \"1\"\n",
    "    ]\n",
    "    \n",
    "    print(f\"Running: {' '.join(cmd)}\")\n",
    "    \n",
    "    # Execute the command\n",
    "    result = subprocess.run(cmd, capture_output=True, text=True)\n",
    "    \n",
    "    # Print the result\n",
    "    print(result.stdout)\n",
    "    if result.stderr:\n",
    "        print(f\"Error: {result.stderr}\")\n",
    "    \n",
    "    # Return the path to the results directory\n",
    "    return str(RESULTS_DIR / run_id)\n",
    "\n",
    "def run_all_experiments(model_names, optimizer_names, epochs=EPOCHS, batch_size=BATCH_SIZE):\n",
    "    \"\"\"\n",
    "    Runs all combinations of models and optimizers\n",
    "    \n",
    "    Args:\n",
    "        model_names (list): List of model architecture names\n",
    "        optimizer_names (list): List of optimizer names\n",
    "        epochs (int): Number of epochs for training\n",
    "        batch_size (int): Batch size for training\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary mapping experiment combinations to result paths\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    total_experiments = len(model_names) * len(optimizer_names)\n",
    "    counter = 1\n",
    "    \n",
    "    for model_name in model_names:\n",
    "        for optimizer_name in optimizer_names:\n",
    "            print(f\"\\nExperiment {counter}/{total_experiments}: {model_name} with {optimizer_name}\")\n",
    "            result_path = train_model(model_name, optimizer_name, epochs, batch_size)\n",
    "            results[(model_name, optimizer_name)] = result_path\n",
    "            counter += 1\n",
    "            \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6839c629",
   "metadata": {},
   "source": [
    "Let's run a set of experiments with selected models and optimizers. You can adjust these lists to include the specific combinations you want to evaluate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c76cbbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a subset of models and optimizers for evaluation to reduce computation time\n",
    "# You can uncomment the full lists for a comprehensive evaluation\n",
    "selected_models = ['Advanced', 'Residual', 'Best']\n",
    "selected_optimizers = ['Adam', 'ImprovedAdam', 'Nadam', 'RMSprop', 'SGD']\n",
    "\n",
    "# Uncomment to run the experiments\n",
    "# experiment_results = run_all_experiments(\n",
    "#     model_names=selected_models,\n",
    "#     optimizer_names=selected_optimizers,\n",
    "#     epochs=EPOCHS,\n",
    "#     batch_size=BATCH_SIZE\n",
    "# )\n",
    "# \n",
    "# # Print the results directory paths\n",
    "# for (model, optimizer), result_path in experiment_results.items():\n",
    "#     print(f\"{model} + {optimizer}: {result_path}\")\n",
    "\n",
    "# For notebook development, create a simulated results dict\n",
    "# In practice, you would use the actual experiment_results from above\n",
    "experiment_results = {}\n",
    "for model in selected_models:\n",
    "    for optimizer in selected_optimizers:\n",
    "        experiment_results[(model, optimizer)] = f\"{RESULTS_DIR}/{model}_{optimizer}_20230101_000000\"\n",
    "\n",
    "print(\"Experiment configuration complete. Ready to run experiments or analyze existing results.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab1b58e",
   "metadata": {},
   "source": [
    "## Performance Visualization\n",
    "\n",
    "Now that we've trained models with different optimizers (or have existing results), let's visualize their performance.\n",
    "\n",
    "First, let's create functions to load and process the results.\n",
    "\n",
    "# Import the optimizers\n",
    "try:\n",
    "    # Import the standard TensorFlow optimizers as fallback\n",
    "    from tensorflow.keras.optimizers import (\n",
    "        Adam, SGD, RMSprop, Nadam, Adagrad, Adadelta, Adamax\n",
    "    )\n",
    "    \n",
    "    # Try importing custom optimizers from the project\n",
    "    # The sys.path.append in the setup ensures these imports work correctly\n",
    "    try:\n",
    "        # Import optimizers from the project's source directory\n",
    "        from src.optimizers.Adam import optimizer as adam_optimizer\n",
    "        from src.optimizers.ImprovedAdam import optimizer as improved_adam_optimizer\n",
    "        from src.optimizers.Nadam import optimizer as nadam_optimizer\n",
    "        from src.optimizers.RMSprop import optimizer as rmsprop_optimizer\n",
    "        from src.optimizers.SGD import optimizer as sgd_optimizer\n",
    "        from src.optimizers.Adadelta import optimizer as adadelta_optimizer\n",
    "        from src.optimizers.Adagrad import optimizer as adagrad_optimizer\n",
    "        from src.optimizers.Adamax import optimizer as adamax_optimizer\n",
    "        \n",
    "        # If we get here, custom optimizers were found\n",
    "        custom_optimizers = True\n",
    "        print(\"Successfully imported custom optimizers from src.optimizers\")\n",
    "    except ImportError as e:\n",
    "        print(f\"Custom optimizers not found: {e}\")\n",
    "        print(\"Using standard TensorFlow optimizers instead\")\n",
    "        custom_optimizers = False\n",
    "    \n",
    "    # Create a dictionary of optimizers based on availability\n",
    "    if custom_optimizers:\n",
    "        optimizers = {\n",
    "            'Adam': adam_optimizer,\n",
    "            'ImprovedAdam': improved_adam_optimizer,\n",
    "            'Nadam': nadam_optimizer,\n",
    "            'RMSprop': rmsprop_optimizer,\n",
    "            'SGD': sgd_optimizer,\n",
    "            'Adadelta': adadelta_optimizer,\n",
    "            'Adagrad': adagrad_optimizer,\n",
    "            'Adamax': adamax_optimizer\n",
    "        }\n",
    "    else:\n",
    "        # Create standard optimizers with default configs as fallback\n",
    "        optimizers = {\n",
    "            'Adam': Adam(),\n",
    "            'SGD': SGD(),\n",
    "            'RMSprop': RMSprop(),\n",
    "            'Nadam': Nadam(),\n",
    "            'Adagrad': Adagrad(),\n",
    "            'Adadelta': Adadelta(),\n",
    "            'Adamax': Adamax()\n",
    "        }\n",
    "        \n",
    "    # Print optimizer configurations for selected optimizers\n",
    "    for name, optimizer in optimizers.items():\n",
    "        if name in OPTIMIZERS:  # Only show selected optimizers\n",
    "            print(f\"Optimizer: {name}\")\n",
    "            print(f\"Configuration: {optimizer.get_config()}\")\n",
    "            print(\"-\" * 50)\n",
    "except Exception as e:\n",
    "    print(f\"Error loading optimizers: {e}\")\n",
    "    print(\"Will continue with experiment configuration without optimizer details.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98b2f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_experiment_results(experiment_results):\n",
    "    \"\"\"\n",
    "    Loads metrics from all experiments\n",
    "    \n",
    "    Args:\n",
    "        experiment_results (dict): Dictionary mapping experiment combinations to result paths\n",
    "        \n",
    "    Returns:\n",
    "        pandas.DataFrame: DataFrame containing metrics for all experiments\n",
    "    \"\"\"\n",
    "    all_metrics = []\n",
    "    \n",
    "    for (model, optimizer), result_path in experiment_results.items():\n",
    "        # Look for metrics.csv file\n",
    "        metrics_file = Path(result_path) / \"metrics.csv\"\n",
    "        \n",
    "        if metrics_file.exists():\n",
    "            # Load the metrics\n",
    "            df = pd.read_csv(metrics_file)\n",
    "            \n",
    "            # Add model and optimizer columns\n",
    "            df['model'] = model\n",
    "            df['optimizer'] = optimizer\n",
    "            \n",
    "            all_metrics.append(df)\n",
    "        else:\n",
    "            print(f\"Warning: No metrics file found for {model} with {optimizer} at {metrics_file}\")\n",
    "            \n",
    "            # For demonstration, create simulated metrics if the file doesn't exist\n",
    "            # In practice, this would be removed and you'd use only real data\n",
    "            epochs = 10\n",
    "            simulated_metrics = {\n",
    "                'epoch': list(range(1, epochs+1)),\n",
    "                'loss': np.random.rand(epochs) * 0.5 + 0.2,\n",
    "                'accuracy': np.random.rand(epochs) * 0.3 + 0.6,\n",
    "                'val_loss': np.random.rand(epochs) * 0.6 + 0.3,\n",
    "                'val_accuracy': np.random.rand(epochs) * 0.3 + 0.5,\n",
    "                'time_per_epoch': np.random.rand(epochs) * 2 + 1,\n",
    "                'model': [model] * epochs,\n",
    "                'optimizer': [optimizer] * epochs\n",
    "            }\n",
    "            simulated_df = pd.DataFrame(simulated_metrics)\n",
    "            \n",
    "            # Ensure the metrics follow a reasonable learning curve\n",
    "            # Loss should generally decrease\n",
    "            simulated_df['loss'] = sorted(simulated_df['loss'], reverse=True)\n",
    "            simulated_df['val_loss'] = sorted(simulated_df['val_loss'], reverse=True) \n",
    "            \n",
    "            # Accuracy should generally increase\n",
    "            simulated_df['accuracy'] = sorted(simulated_df['accuracy'])\n",
    "            simulated_df['val_accuracy'] = sorted(simulated_df['val_accuracy'])\n",
    "            \n",
    "            # Add some noise to make it realistic\n",
    "            for col in ['loss', 'accuracy', 'val_loss', 'val_accuracy']:\n",
    "                noise = np.random.normal(0, 0.01, epochs)\n",
    "                simulated_df[col] += noise\n",
    "                \n",
    "            all_metrics.append(simulated_df)\n",
    "    \n",
    "    if all_metrics:\n",
    "        return pd.concat(all_metrics, ignore_index=True)\n",
    "    else:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "\n",
    "def get_summary_metrics(metrics_df):\n",
    "    \"\"\"\n",
    "    Computes summary metrics across epochs for each model-optimizer combination\n",
    "    \n",
    "    Args:\n",
    "        metrics_df (pandas.DataFrame): DataFrame with training metrics\n",
    "        \n",
    "    Returns:\n",
    "        pandas.DataFrame: DataFrame with summary metrics\n",
    "    \"\"\"\n",
    "    if metrics_df.empty:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Group by model and optimizer\n",
    "    grouped = metrics_df.groupby(['model', 'optimizer'])\n",
    "    \n",
    "    # Compute summary metrics\n",
    "    summary = grouped.agg(\n",
    "        final_val_loss=('val_loss', 'last'),\n",
    "        min_val_loss=('val_loss', 'min'),\n",
    "        final_val_accuracy=('val_accuracy', 'last'),\n",
    "        max_val_accuracy=('val_accuracy', 'max'),\n",
    "        convergence_epoch=('val_loss', lambda x: x.argmin() + 1),\n",
    "        training_time=('time_per_epoch', 'sum')\n",
    "    ).reset_index()\n",
    "    \n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b647acdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load results\n",
    "all_metrics = load_experiment_results(experiment_results)\n",
    "\n",
    "if not all_metrics.empty:\n",
    "    # Display the first few rows\n",
    "    display(all_metrics.head())\n",
    "    \n",
    "    # Get summary metrics\n",
    "    summary_metrics = get_summary_metrics(all_metrics)\n",
    "    display(summary_metrics)\n",
    "else:\n",
    "    print(\"No metrics data found. Please check experiment results.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d364959",
   "metadata": {},
   "source": [
    "Now let's create comprehensive visualizations to compare optimizers across model architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2cfbf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curves(metrics_df, metric='val_loss'):\n",
    "    \"\"\"\n",
    "    Plots learning curves for each model-optimizer combination\n",
    "    \n",
    "    Args:\n",
    "        metrics_df (pandas.DataFrame): DataFrame with training metrics\n",
    "        metric (str): Metric to plot ('val_loss', 'val_accuracy', etc.)\n",
    "    \"\"\"\n",
    "    if metrics_df.empty:\n",
    "        print(\"No metrics data available for plotting.\")\n",
    "        return\n",
    "    \n",
    "    # Get unique models and optimizers\n",
    "    models = metrics_df['model'].unique()\n",
    "    optimizers = metrics_df['optimizer'].unique()\n",
    "    \n",
    "    # Create subplots - one for each model\n",
    "    fig, axes = plt.subplots(len(models), 1, figsize=(14, 5 * len(models)), sharex=True)\n",
    "    if len(models) == 1:\n",
    "        axes = [axes]  # Handle case with only one model\n",
    "        \n",
    "    # Define line styles and colors for optimizers\n",
    "    colors = plt.cm.tab10(np.linspace(0, 1, len(optimizers)))\n",
    "    \n",
    "    for i, model in enumerate(models):\n",
    "        ax = axes[i]\n",
    "        \n",
    "        for j, optimizer in enumerate(optimizers):\n",
    "            # Filter data for this model and optimizer\n",
    "            data = metrics_df[(metrics_df['model'] == model) & (metrics_df['optimizer'] == optimizer)]\n",
    "            \n",
    "            if not data.empty:\n",
    "                # Plot the learning curve\n",
    "                ax.plot(data['epoch'], data[metric], \n",
    "                        label=optimizer, \n",
    "                        color=colors[j],\n",
    "                        marker='o', \n",
    "                        markersize=4, \n",
    "                        linewidth=2)\n",
    "        \n",
    "        # Set labels and title\n",
    "        ax.set_title(f\"{model} - {metric.replace('_', ' ').title()} over Epochs\")\n",
    "        ax.set_ylabel(metric.replace('_', ' ').title())\n",
    "        ax.grid(True, linestyle='--', alpha=0.7)\n",
    "        ax.legend(title=\"Optimizer\")\n",
    "        \n",
    "    # Set common x-label\n",
    "    fig.text(0.5, 0.04, 'Epoch', ha='center', va='center', fontsize=14)\n",
    "    \n",
    "    plt.tight_layout(pad=3.0)\n",
    "    plt.show()\n",
    "\n",
    "def create_heatmap(summary_df, metric_col, title):\n",
    "    \"\"\"\n",
    "    Creates a heatmap for a specific metric across model-optimizer combinations\n",
    "    \n",
    "    Args:\n",
    "        summary_df (pandas.DataFrame): DataFrame with summary metrics\n",
    "        metric_col (str): Column name for the metric to visualize\n",
    "        title (str): Title for the heatmap\n",
    "    \"\"\"\n",
    "    if summary_df.empty:\n",
    "        print(\"No summary data available for heatmap.\")\n",
    "        return\n",
    "    \n",
    "    # Pivot the DataFrame to get models as rows and optimizers as columns\n",
    "    pivot_df = summary_df.pivot(index='model', columns='optimizer', values=metric_col)\n",
    "    \n",
    "    # Create the heatmap\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Determine color map based on metric (lower is better for loss, higher is better for accuracy)\n",
    "    cmap = \"YlOrRd_r\" if \"loss\" in metric_col else \"YlGn\"\n",
    "    \n",
    "    sns.heatmap(pivot_df, annot=True, cmap=cmap, fmt=\".4f\", linewidths=.5)\n",
    "    plt.title(title)\n",
    "    plt.ylabel('Model Architecture')\n",
    "    plt.xlabel('Optimizer')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def create_radar_chart(summary_df, optimizers, metrics):\n",
    "    \"\"\"\n",
    "    Creates a radar chart comparing optimizers across multiple metrics\n",
    "    \n",
    "    Args:\n",
    "        summary_df (pandas.DataFrame): DataFrame with summary metrics\n",
    "        optimizers (list): List of optimizers to include\n",
    "        metrics (list): List of metrics to compare\n",
    "    \"\"\"\n",
    "    if summary_df.empty:\n",
    "        print(\"No summary data available for radar chart.\")\n",
    "        return\n",
    "    \n",
    "    # Get unique models\n",
    "    models = summary_df['model'].unique()\n",
    "    \n",
    "    # Create subplots - one for each model\n",
    "    fig = plt.figure(figsize=(16, 4 * len(models)))\n",
    "    \n",
    "    for i, model in enumerate(models):\n",
    "        # Filter data for this model\n",
    "        model_data = summary_df[summary_df['model'] == model]\n",
    "        \n",
    "        # Create a subplot\n",
    "        ax = fig.add_subplot(len(models), 1, i+1, polar=True)\n",
    "        \n",
    "        # Number of metrics\n",
    "        num_metrics = len(metrics)\n",
    "        angles = np.linspace(0, 2*np.pi, num_metrics, endpoint=False).tolist()\n",
    "        angles += angles[:1]  # Close the loop\n",
    "        \n",
    "        # Set the labels for each axis\n",
    "        ax.set_xticks(angles[:-1])\n",
    "        ax.set_xticklabels([m.replace('_', ' ').title() for m in metrics])\n",
    "        \n",
    "        # Colors for different optimizers\n",
    "        colors = plt.cm.tab10(np.linspace(0, 1, len(optimizers)))\n",
    "        \n",
    "        # Plot data for each optimizer\n",
    "        for j, optimizer in enumerate(optimizers):\n",
    "            # Filter data for this optimizer\n",
    "            opt_data = model_data[model_data['optimizer'] == optimizer]\n",
    "            \n",
    "            if not opt_data.empty:\n",
    "                # Extract values for each metric\n",
    "                values = []\n",
    "                for metric in metrics:\n",
    "                    if metric in opt_data.columns:\n",
    "                        # Normalize the value to 0-1 range for the radar chart\n",
    "                        min_val = model_data[metric].min()\n",
    "                        max_val = model_data[metric].max()\n",
    "                        if max_val > min_val:\n",
    "                            # For metrics where lower is better (like loss), invert the normalization\n",
    "                            if 'loss' in metric or 'time' in metric:\n",
    "                                val = 1 - (opt_data[metric].values[0] - min_val) / (max_val - min_val)\n",
    "                            else:\n",
    "                                val = (opt_data[metric].values[0] - min_val) / (max_val - min_val)\n",
    "                        else:\n",
    "                            val = 0.5  # Default if all values are the same\n",
    "                        values.append(val)\n",
    "                    else:\n",
    "                        values.append(0)  # Default if metric doesn't exist\n",
    "                \n",
    "                values += values[:1]  # Close the loop\n",
    "                \n",
    "                # Plot the values\n",
    "                ax.plot(angles, values, linewidth=2, linestyle='solid', color=colors[j], label=optimizer)\n",
    "                ax.fill(angles, values, color=colors[j], alpha=0.1)\n",
    "        \n",
    "        # Set title and legend\n",
    "        ax.set_title(f\"{model} - Optimizer Performance Comparison\")\n",
    "        ax.legend(loc='upper right', bbox_to_anchor=(0.1, 0.1))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b36984",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot learning curves\n",
    "if not all_metrics.empty:\n",
    "    plot_learning_curves(all_metrics, 'val_loss')\n",
    "    plot_learning_curves(all_metrics, 'val_accuracy')\n",
    "else:\n",
    "    print(\"No metrics data available for plotting learning curves.\")\n",
    "\n",
    "# Create heatmaps for different metrics\n",
    "if not summary_metrics.empty:\n",
    "    create_heatmap(summary_metrics, 'min_val_loss', 'Minimum Validation Loss by Model and Optimizer')\n",
    "    create_heatmap(summary_metrics, 'max_val_accuracy', 'Maximum Validation Accuracy by Model and Optimizer')\n",
    "    create_heatmap(summary_metrics, 'convergence_epoch', 'Convergence Epoch by Model and Optimizer')\n",
    "    create_heatmap(summary_metrics, 'training_time', 'Total Training Time (s) by Model and Optimizer')\n",
    "else:\n",
    "    print(\"No summary metrics available for creating heatmaps.\")\n",
    "\n",
    "# Create radar charts\n",
    "if not summary_metrics.empty:\n",
    "    radar_metrics = ['min_val_loss', 'max_val_accuracy', 'convergence_epoch', 'training_time']\n",
    "    create_radar_chart(summary_metrics, selected_optimizers, radar_metrics)\n",
    "else:\n",
    "    print(\"No summary metrics available for creating radar charts.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66abc242",
   "metadata": {},
   "source": [
    "## Optimizer Comparison\n",
    "\n",
    "Let's compare the optimizers more deeply with some additional analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17183488",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_optimization_paths(metrics_df, model_name):\n",
    "    \"\"\"\n",
    "    Compares optimization paths in loss/accuracy space for different optimizers\n",
    "    \n",
    "    Args:\n",
    "        metrics_df (pandas.DataFrame): DataFrame with training metrics\n",
    "        model_name (str): Model to analyze\n",
    "    \"\"\"\n",
    "    if metrics_df.empty:\n",
    "        print(\"No metrics data available for optimization path comparison.\")\n",
    "        return\n",
    "    \n",
    "    # Filter data for the specified model\n",
    "    model_data = metrics_df[metrics_df['model'] == model_name]\n",
    "    \n",
    "    if model_data.empty:\n",
    "        print(f\"No data available for model {model_name}\")\n",
    "        return\n",
    "    \n",
    "    # Get unique optimizers\n",
    "    optimizers = model_data['optimizer'].unique()\n",
    "    \n",
    "    # Colors for different optimizers\n",
    "    colors = plt.cm.tab10(np.linspace(0, 1, len(optimizers)))\n",
    "    \n",
    "    # Create the plot\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    for i, optimizer in enumerate(optimizers):\n",
    "        # Filter data for this optimizer\n",
    "        opt_data = model_data[model_data['optimizer'] == optimizer]\n",
    "        \n",
    "        if not opt_data.empty:\n",
    "            # Sort by epoch\n",
    "            opt_data = opt_data.sort_values('epoch')\n",
    "            \n",
    "            # Plot the optimization path\n",
    "            plt.plot(opt_data['val_loss'], opt_data['val_accuracy'], \n",
    "                     'o-', label=optimizer, color=colors[i], markersize=8)\n",
    "            \n",
    "            # Add arrows to show direction\n",
    "            for j in range(len(opt_data) - 1):\n",
    "                plt.annotate('', \n",
    "                             xy=(opt_data['val_loss'].iloc[j+1], opt_data['val_accuracy'].iloc[j+1]), \n",
    "                             xytext=(opt_data['val_loss'].iloc[j], opt_data['val_accuracy'].iloc[j]),\n",
    "                             arrowprops=dict(arrowstyle='->', color=colors[i], lw=1.5))\n",
    "            \n",
    "            # Add epoch labels for some points\n",
    "            for j in [0, len(opt_data)//2, len(opt_data)-1]:\n",
    "                if j < len(opt_data):\n",
    "                    plt.annotate(f\"Epoch {opt_data['epoch'].iloc[j]}\", \n",
    "                                xy=(opt_data['val_loss'].iloc[j], opt_data['val_accuracy'].iloc[j]),\n",
    "                                xytext=(10, 0), textcoords='offset points',\n",
    "                                fontsize=8, color=colors[i])\n",
    "    \n",
    "    # Set labels and title\n",
    "    plt.xlabel('Validation Loss')\n",
    "    plt.ylabel('Validation Accuracy')\n",
    "    plt.title(f'Optimization Paths for {model_name} with Different Optimizers')\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.legend(title=\"Optimizer\")\n",
    "    \n",
    "    # Invert x-axis since we want to minimize loss\n",
    "    plt.gca().invert_xaxis()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def analyze_convergence_rates(metrics_df):\n",
    "    \"\"\"\n",
    "    Analyzes convergence rates for different optimizers\n",
    "    \n",
    "    Args:\n",
    "        metrics_df (pandas.DataFrame): DataFrame with training metrics\n",
    "    \"\"\"\n",
    "    if metrics_df.empty:\n",
    "        print(\"No metrics data available for convergence rate analysis.\")\n",
    "        return\n",
    "    \n",
    "    # Group by model and optimizer\n",
    "    grouped = metrics_df.groupby(['model', 'optimizer'])\n",
    "    \n",
    "    # Calculate the epoch at which the model reached 90% of its best performance\n",
    "    convergence_data = []\n",
    "    \n",
    "    for (model, optimizer), group in grouped:\n",
    "        # Sort by epoch\n",
    "        group = group.sort_values('epoch')\n",
    "        \n",
    "        # Calculate best val_loss and corresponding epoch\n",
    "        best_val_loss = group['val_loss'].min()\n",
    "        best_epoch = group[group['val_loss'] == best_val_loss]['epoch'].iloc[0]\n",
    "        \n",
    "        # Calculate epoch at which model reached 90% of best val_loss\n",
    "        # For loss, we want to find when it's within 110% of the best (since lower is better)\n",
    "        convergence_threshold = best_val_loss * 1.1\n",
    "        convergence_epoch = group[group['val_loss'] <= convergence_threshold]['epoch'].min()\n",
    "        \n",
    "        convergence_data.append({\n",
    "            'model': model,\n",
    "            'optimizer': optimizer,\n",
    "            'best_val_loss': best_val_loss,\n",
    "            'best_epoch': best_epoch,\n",
    "            'convergence_epoch': convergence_epoch,\n",
    "            'convergence_speed': convergence_epoch / best_epoch if best_epoch > 0 else float('inf')\n",
    "        })\n",
    "    \n",
    "    convergence_df = pd.DataFrame(convergence_data)\n",
    "    \n",
    "    # Create the plot\n",
    "    plt.figure(figsize=(14, 10))\n",
    "    \n",
    "    # Get unique models and optimizers\n",
    "    models = convergence_df['model'].unique()\n",
    "    optimizers = convergence_df['optimizer'].unique()\n",
    "    \n",
    "    # Set up bar positions\n",
    "    bar_width = 0.15\n",
    "    index = np.arange(len(models))\n",
    "    \n",
    "    # Plot bars for each optimizer\n",
    "    for i, optimizer in enumerate(optimizers):\n",
    "        # Filter data for this optimizer\n",
    "        opt_data = convergence_df[convergence_df['optimizer'] == optimizer]\n",
    "        \n",
    "        # Extract convergence epochs for each model\n",
    "        convergence_epochs = []\n",
    "        for model in models:\n",
    "            model_opt_data = opt_data[opt_data['model'] == model]\n",
    "            if not model_opt_data.empty:\n",
    "                convergence_epochs.append(model_opt_data['convergence_epoch'].iloc[0])\n",
    "            else:\n",
    "                convergence_epochs.append(0)\n",
    "        \n",
    "        # Plot the bars\n",
    "        plt.bar(index + i * bar_width, convergence_epochs, bar_width, \n",
    "                label=optimizer, alpha=0.8)\n",
    "    \n",
    "    # Set labels and title\n",
    "    plt.xlabel('Model Architecture')\n",
    "    plt.ylabel('Convergence Epoch')\n",
    "    plt.title('Convergence Speed by Model and Optimizer')\n",
    "    plt.xticks(index + bar_width * (len(optimizers) - 1) / 2, models)\n",
    "    plt.legend(title=\"Optimizer\")\n",
    "    plt.grid(True, axis='y', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Display the convergence data as a table\n",
    "    display(convergence_df.sort_values(['model', 'convergence_epoch']))\n",
    "    \n",
    "    return convergence_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304eea94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare optimization paths for each model\n",
    "if not all_metrics.empty:\n",
    "    for model in selected_models:\n",
    "        compare_optimization_paths(all_metrics, model)\n",
    "else:\n",
    "    print(\"No metrics data available for comparing optimization paths.\")\n",
    "\n",
    "# Analyze convergence rates\n",
    "if not all_metrics.empty:\n",
    "    convergence_df = analyze_convergence_rates(all_metrics)\n",
    "else:\n",
    "    print(\"No metrics data available for analyzing convergence rates.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b1fe8d",
   "metadata": {},
   "source": [
    "## Best Optimizer Analysis\n",
    "\n",
    "Let's determine the best optimizer for each model based on different metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad274edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_optimizer(summary_df, metric_col, is_lower_better=True):\n",
    "    \"\"\"\n",
    "    Finds the best optimizer for each model based on a specific metric\n",
    "    \n",
    "    Args:\n",
    "        summary_df (pandas.DataFrame): DataFrame with summary metrics\n",
    "        metric_col (str): Column name for the metric to use\n",
    "        is_lower_better (bool): Whether a lower value is better (True for loss, False for accuracy)\n",
    "        \n",
    "    Returns:\n",
    "        pandas.DataFrame: DataFrame with best optimizer for each model\n",
    "    \"\"\"\n",
    "    if summary_df.empty:\n",
    "        print(\"No summary data available for finding best optimizer.\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Group by model\n",
    "    grouped = summary_df.groupby('model')\n",
    "    \n",
    "    best_optimizers = []\n",
    "    \n",
    "    for model, group in grouped:\n",
    "        # Find the best optimizer based on the metric\n",
    "        if is_lower_better:\n",
    "            best_row = group.loc[group[metric_col].idxmin()]\n",
    "        else:\n",
    "            best_row = group.loc[group[metric_col].idxmax()]\n",
    "        \n",
    "        best_optimizers.append({\n",
    "            'model': model,\n",
    "            'best_optimizer': best_row['optimizer'],\n",
    "            f'best_{metric_col}': best_row[metric_col]\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(best_optimizers)\n",
    "\n",
    "def analyze_overall_performance(summary_df):\n",
    "    \"\"\"\n",
    "    Analyzes overall performance of optimizers across multiple metrics\n",
    "    \n",
    "    Args:\n",
    "        summary_df (pandas.DataFrame): DataFrame with summary metrics\n",
    "        \n",
    "    Returns:\n",
    "        pandas.DataFrame: DataFrame with overall performance rankings\n",
    "    \"\"\"\n",
    "    if summary_df.empty:\n",
    "        print(\"No summary data available for analyzing overall performance.\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Define metrics and whether lower is better for each\n",
    "    metric_configs = {\n",
    "        'min_val_loss': True,      # Lower is better\n",
    "        'max_val_accuracy': False, # Higher is better\n",
    "        'convergence_epoch': True, # Lower is better\n",
    "        'training_time': True      # Lower is better\n",
    "    }\n",
    "    \n",
    "    # Get unique models and optimizers\n",
    "    models = summary_df['model'].unique()\n",
    "    optimizers = summary_df['optimizer'].unique()\n",
    "    \n",
    "    # Create a DataFrame to store rankings\n",
    "    rankings = pd.DataFrame(index=optimizers, columns=['overall_score', 'win_count'])\n",
    "    rankings['overall_score'] = 0\n",
    "    rankings['win_count'] = 0\n",
    "    \n",
    "    # Calculate scores for each optimizer\n",
    "    for metric, is_lower_better in metric_configs.items():\n",
    "        # Find best optimizer for each model\n",
    "        best_opts = find_best_optimizer(summary_df, metric, is_lower_better)\n",
    "        \n",
    "        # Update win count\n",
    "        for _, row in best_opts.iterrows():\n",
    "            rankings.at[row['best_optimizer'], 'win_count'] += 1\n",
    "        \n",
    "        # Calculate normalized scores for each optimizer\n",
    "        for model in models:\n",
    "            model_data = summary_df[summary_df['model'] == model]\n",
    "            \n",
    "            if not model_data.empty:\n",
    "                # Get min and max values for this metric across optimizers\n",
    "                min_val = model_data[metric].min()\n",
    "                max_val = model_data[metric].max()\n",
    "                \n",
    "                if max_val > min_val:  # Avoid division by zero\n",
    "                    # Calculate normalized scores\n",
    "                    for optimizer in optimizers:\n",
    "                        opt_data = model_data[model_data['optimizer'] == optimizer]\n",
    "                        \n",
    "                        if not opt_data.empty:\n",
    "                            value = opt_data[metric].iloc[0]\n",
    "                            \n",
    "                            # Normalize to 0-1 where 1 is best\n",
    "                            if is_lower_better:\n",
    "                                score = 1 - (value - min_val) / (max_val - min_val)\n",
    "                            else:\n",
    "                                score = (value - min_val) / (max_val - min_val)\n",
    "                            \n",
    "                            # Update overall score\n",
    "                            rankings.at[optimizer, 'overall_score'] += score\n",
    "    \n",
    "    # Sort by overall score (descending)\n",
    "    rankings = rankings.sort_values('overall_score', ascending=False)\n",
    "    \n",
    "    return rankings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce48ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find best optimizer for different metrics\n",
    "if not summary_metrics.empty:\n",
    "    print(\"Best optimizer by validation loss:\")\n",
    "    display(find_best_optimizer(summary_metrics, 'min_val_loss', True))\n",
    "    \n",
    "    print(\"\\nBest optimizer by validation accuracy:\")\n",
    "    display(find_best_optimizer(summary_metrics, 'max_val_accuracy', False))\n",
    "    \n",
    "    print(\"\\nBest optimizer by convergence speed:\")\n",
    "    display(find_best_optimizer(summary_metrics, 'convergence_epoch', True))\n",
    "    \n",
    "    print(\"\\nBest optimizer by training time:\")\n",
    "    display(find_best_optimizer(summary_metrics, 'training_time', True))\n",
    "else:\n",
    "    print(\"No summary metrics available for finding best optimizer.\")\n",
    "\n",
    "# Analyze overall performance\n",
    "if not summary_metrics.empty:\n",
    "    overall_rankings = analyze_overall_performance(summary_metrics)\n",
    "    \n",
    "    print(\"Overall optimizer rankings:\")\n",
    "    display(overall_rankings)\n",
    "    \n",
    "    # Visualize the rankings\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(y=overall_rankings.index, x='overall_score', data=overall_rankings)\n",
    "    plt.title('Overall Optimizer Performance')\n",
    "    plt.xlabel('Performance Score (higher is better)')\n",
    "    plt.ylabel('Optimizer')\n",
    "    plt.grid(True, axis='x', linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Show win count\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(y=overall_rankings.index, x='win_count', data=overall_rankings)\n",
    "    plt.title('Optimizer Win Count (Best in at least one metric)')\n",
    "    plt.xlabel('Win Count')\n",
    "    plt.ylabel('Optimizer')\n",
    "    plt.grid(True, axis='x', linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No summary metrics available for analyzing overall performance.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba55292",
   "metadata": {},
   "source": [
    "## Conclusions and Recommendations\n",
    "\n",
    "Based on our analysis, we can draw the following conclusions about the optimizers and their performance across different model architectures:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "814b0bf2",
   "metadata": {},
   "source": [
    "### Summary of Findings\n",
    "\n",
    "1. **Best Overall Optimizer**: Based on the combined metrics (validation loss, accuracy, convergence speed, and training time), [fill in based on results] has shown the best overall performance across all model architectures.\n",
    "\n",
    "2. **Model-Specific Recommendations**:\n",
    "   - For **Advanced** model: [fill based on results]\n",
    "   - For **Residual** model: [fill based on results]\n",
    "   - For **Best** model: [fill based on results]\n",
    "\n",
    "3. **Performance Characteristics**:\n",
    "   - **Convergence Speed**: [fill based on results]\n",
    "   - **Final Accuracy**: [fill based on results]\n",
    "   - **Training Efficiency**: [fill based on results]\n",
    "\n",
    "4. **Trade-offs**:\n",
    "   - Some optimizers show faster convergence but slightly worse final accuracy.\n",
    "   - Others take longer to converge but achieve better final results.\n",
    "   - Training time varies significantly between optimizers and models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b370e4",
   "metadata": {},
   "source": [
    "### Recommendations\n",
    "\n",
    "Based on our analysis, we recommend:\n",
    "\n",
    "1. **Production Usage**: For production deployments, use [fill based on results] as it provides the best balance of accuracy and efficiency.\n",
    "\n",
    "2. **Resource-Constrained Environments**: If training time is a significant concern, [fill based on results] provides the fastest convergence with acceptable performance.\n",
    "\n",
    "3. **Maximum Accuracy**: When accuracy is the primary goal and training time is not a concern, use [fill based on results].\n",
    "\n",
    "4. **Future Work**:\n",
    "   - Explore learning rate schedules for each optimizer to potentially improve performance\n",
    "   - Test custom parameter settings for the top-performing optimizers\n",
    "   - Investigate combinations of optimizers at different training stages\n",
    "   - Experiment with more advanced optimizers like LAMB, AdaBelief, or NovoGrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf07a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the results for future reference\n",
    "if not summary_metrics.empty:\n",
    "    # Create a timestamp\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    # Create output directory if it doesn't exist\n",
    "    output_dir = Path(\"optimizer_results\")\n",
    "    output_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Save the summary metrics\n",
    "    summary_metrics.to_csv(output_dir / f\"optimizer_comparison_summary_{timestamp}.csv\", index=False)\n",
    "    \n",
    "    # Save the overall rankings\n",
    "    if 'overall_rankings' in locals():\n",
    "        overall_rankings.to_csv(output_dir / f\"optimizer_rankings_{timestamp}.csv\")\n",
    "        \n",
    "    print(f\"Results saved with timestamp {timestamp} in {output_dir}\")\n",
    "else:\n",
    "    print(\"No results to save.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818cd7f1",
   "metadata": {},
   "source": [
    "# Optimizer Evaluation Across Model Architectures\n",
    "\n",
    "This notebook provides a comprehensive analysis of different optimizers across various model architectures for our machine learning project. We'll evaluate performance metrics, convergence rates, and overall model quality.\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Setup and Configuration](#setup-and-configuration)\n",
    "2. [Model Architectures Overview](#model-architectures-overview)\n",
    "3. [Optimizer Definitions](#optimizer-definitions)\n",
    "4. [Training Automation](#training-automation)\n",
    "5. [Performance Visualization](#performance-visualization)\n",
    "6. [Optimizer Comparison](#optimizer-comparison)\n",
    "7. [Best Optimizer Analysis](#best-optimizer-analysis)\n",
    "8. [Conclusions and Recommendations](#conclusions-and-recommendations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff24404",
   "metadata": {},
   "source": [
    "## Setup and Configuration\n",
    "\n",
    "Let's import the necessary libraries and set up our environment for the optimizer evaluation experiments. We'll configure paths, import required modules, and define visualization settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2baff8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import subprocess\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Add parent directory to path to import project modules\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '../..')))\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('ggplot')\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "# Configure matplotlib for better visualization\n",
    "plt.rcParams['figure.figsize'] = (14, 8)\n",
    "plt.rcParams['axes.labelsize'] = 12\n",
    "plt.rcParams['axes.titlesize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 10\n",
    "plt.rcParams['ytick.labelsize'] = 10\n",
    "plt.rcParams['legend.fontsize'] = 10\n",
    "plt.rcParams['figure.dpi'] = 150\n",
    "\n",
    "# Configuration parameters\n",
    "PROJECT_ROOT = Path(os.path.abspath(os.path.join(os.getcwd(), '../..')))\n",
    "RESULTS_DIR = PROJECT_ROOT / 'logs'\n",
    "MAIN_SCRIPT = PROJECT_ROOT / 'main.py'\n",
    "\n",
    "# Define models and optimizers to evaluate\n",
    "MODELS = ['Advanced', 'Residual', 'Best']\n",
    "OPTIMIZERS = ['Adam', 'ImprovedAdam', 'Nadam', 'RMSprop', 'SGD', 'Adadelta', 'Adagrad', 'Adamax']\n",
    "EPOCHS = 15  # Use fewer epochs for faster evaluation\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "print(f\"Project Root: {PROJECT_ROOT}\")\n",
    "print(f\"Results Directory: {RESULTS_DIR}\")\n",
    "print(f\"Models to evaluate: {MODELS}\")\n",
    "print(f\"Optimizers to evaluate: {OPTIMIZERS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b8a450",
   "metadata": {},
   "source": [
    "## Model Architectures Overview\n",
    "\n",
    "In this section, we'll examine the different model architectures available in our project. We'll load the architectures from our code base and provide a summary of each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b1aa35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model_type, optimizer, epochs=50, features='medium'):\n",
    "    \"\"\"Train a model with the specified optimizer and return performance metrics\"\"\"\n",
    "    print(f\"Training {model_type} model with {optimizer} optimizer...\")\n",
    "    \n",
    "    # Create a timestamp for this run\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Construct the command to run main.py with appropriate arguments\n",
    "    cmd = [\n",
    "        \"python\", \"../../main.py\",\n",
    "        \"--model\", model_type,\n",
    "        \"--features\", features,\n",
    "        \"--loss\", \"mae\",  # Using MAE as standard loss function\n",
    "        \"--epochs\", str(epochs),\n",
    "        \"--batch_size\", \"64\",\n",
    "        \"--learning_rate\", \"0.001\"\n",
    "    ]\n",
    "    \n",
    "    # Add optimizer parameter if it's not Adam (which is the default)\n",
    "    if optimizer != \"adam\":\n",
    "        cmd.extend([\"--optimizer\", optimizer])\n",
    "    \n",
    "    print(f\"Running command: {' '.join(cmd)}\")\n",
    "    \n",
    "    # Run the training process\n",
    "    try:\n",
    "        process = subprocess.run(\n",
    "            cmd,\n",
    "            stdout=subprocess.PIPE,\n",
    "            stderr=subprocess.PIPE,\n",
    "            universal_newlines=True\n",
    "        )\n",
    "        \n",
    "        # Calculate training time\n",
    "        training_time = time.time() - start_time\n",
    "        \n",
    "        # Check if process was successful\n",
    "        if process.returncode != 0:\n",
    "            print(f\"Error running command. Exit code: {process.returncode}\")\n",
    "            print(f\"Error output: {process.stderr}\")\n",
    "            return None\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Exception running command: {str(e)}\")\n",
    "        return None\n",
    "    \n",
    "    # Extract the log directory from the output\n",
    "    log_dir = None\n",
    "    for line in process.stdout.split('\\n'):\n",
    "        if \"TensorBoard logs saved to:\" in line:\n",
    "            log_dir = line.split(\"TensorBoard logs saved to:\")[1].strip()\n",
    "            break\n",
    "    \n",
    "    if not log_dir:\n",
    "        print(f\"Warning: Could not find log directory in process output\")\n",
    "        \n",
    "        # Try to find the most recent log directory for this model\n",
    "        log_base = Path(\"../../logs/experiments\")\n",
    "        potential_dirs = list(log_base.glob(f\"{model_type.capitalize()}Model*\"))\n",
    "        if potential_dirs:\n",
    "            # Get most recent by sorting\n",
    "            log_dir = str(sorted(potential_dirs, key=os.path.getmtime, reverse=True)[0])\n",
    "            print(f\"Using most recent log directory: {log_dir}\")\n",
    "        else:\n",
    "            print(\"Could not find any log directory\")\n",
    "            return None\n",
    "    \n",
    "    # Find metrics file\n",
    "    metrics_file = None\n",
    "    if log_dir:\n",
    "        # Look for final_metrics.json\n",
    "        metrics_path = Path(log_dir) / \"final_metrics.json\"\n",
    "        if metrics_path.exists():\n",
    "            metrics_file = metrics_path\n",
    "        else:\n",
    "            # Try to find metrics in CSV files\n",
    "            csv_files = list(Path(\"../../logs/metrics\").glob(f\"{model_type.capitalize()}Model*.csv\"))\n",
    "            if csv_files:\n",
    "                most_recent = sorted(csv_files, key=os.path.getmtime, reverse=True)[0]\n",
    "                print(f\"Using metrics from CSV: {most_recent}\")\n",
    "                # Read the last row of the CSV file\n",
    "                metrics_df = pd.read_csv(most_recent)\n",
    "                if not metrics_df.empty:\n",
    "                    # Extract the final epoch metrics\n",
    "                    last_row = metrics_df.iloc[-1].to_dict()\n",
    "                    # Create a synthetic metrics object\n",
    "                    metrics = {\n",
    "                        \"val_loss\": last_row.get(\"val_loss\"),\n",
    "                        \"val_mae\": last_row.get(\"val_mae\", last_row.get(\"val_mean_absolute_error\")),\n",
    "                        \"val_mse\": last_row.get(\"val_mse\", last_row.get(\"val_mean_squared_error\")),\n",
    "                        \"training_time\": training_time,\n",
    "                        \"model_name\": f\"{model_type.capitalize()}Model\",\n",
    "                        \"optimizer\": optimizer\n",
    "                    }\n",
    "                    return metrics\n",
    "    \n",
    "    if not metrics_file:\n",
    "        print(f\"Warning: No metrics file found for {model_type} with {optimizer}\")\n",
    "        return None\n",
    "        \n",
    "    # Load and return the metrics\n",
    "    with open(metrics_file, 'r') as f:\n",
    "        metrics = json.load(f)\n",
    "    \n",
    "    # Add training time to metrics\n",
    "    metrics[\"training_time\"] = training_time\n",
    "    metrics[\"model_name\"] = f\"{model_type.capitalize()}Model\"\n",
    "    metrics[\"optimizer\"] = optimizer\n",
    "    \n",
    "    print(f\"Successfully trained {model_type} with {optimizer} in {training_time:.1f}s\")\n",
    "    return metrics\n",
    "\n",
    "def extract_all_optimizer_metrics():\n",
    "    \"\"\"Try to extract metrics from existing model files without retraining\"\"\"\n",
    "    results = []\n",
    "    export_dir = Path(\"../../exports\")\n",
    "    \n",
    "    # Look for model files with optimizer names in them\n",
    "    for model_file in export_dir.glob(\"*.keras\"):\n",
    "        file_name = model_file.name\n",
    "        \n",
    "        # Extract model type and optimizer\n",
    "        for model in MODELS:\n",
    "            model_cap = model.capitalize()\n",
    "            if model_cap in file_name:\n",
    "                for opt in OPTIMIZERS:\n",
    "                    opt_cap = opt.capitalize()\n",
    "                    if opt_cap in file_name:\n",
    "                        print(f\"Found existing model: {file_name}\")\n",
    "                        \n",
    "                        # Try to load the model metrics from logs\n",
    "                        metrics = {\n",
    "                            \"model\": model,\n",
    "                            \"optimizer\": opt,\n",
    "                            \"file_name\": file_name\n",
    "                        }\n",
    "                        results.append(metrics)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9023ebec",
   "metadata": {},
   "source": [
    "## 3. Run Optimizer Evaluation\n",
    "\n",
    "Now we'll execute the evaluation by training each model with each optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bedd8eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train all combinations and gather results\n",
    "def run_full_evaluation(models=MODELS, optimizers=OPTIMIZERS, epochs=EPOCHS):\n",
    "    all_results = []\n",
    "    \n",
    "    # Create a grid of model and optimizer combinations\n",
    "    total_combinations = len(models) * len(optimizers)\n",
    "    completed = 0\n",
    "    \n",
    "    for model in models:\n",
    "        for opt in optimizers:\n",
    "            # Show progress\n",
    "            completed += 1\n",
    "            print(f\"\\n[{completed}/{total_combinations}] Evaluating {model} with {opt}...\")\n",
    "            \n",
    "            try:\n",
    "                metrics = train_model(model, opt, epochs=epochs)\n",
    "                if metrics:\n",
    "                    # Extract key metrics\n",
    "                    result = {\n",
    "                        'model': model,\n",
    "                        'optimizer': opt,\n",
    "                        'val_loss': metrics.get('val_loss'),\n",
    "                        'val_mae': metrics.get('val_mae'),\n",
    "                        'val_mse': metrics.get('val_mse'),\n",
    "                        'correlation': metrics.get('correlation', metrics.get('val_correlation')),\n",
    "                        'training_time': metrics.get('training_time')\n",
    "                    }\n",
    "                    all_results.append(result)\n",
    "                    \n",
    "                    # Save intermediate results in case of failure\n",
    "                    pd.DataFrame(all_results).to_csv(results_file, index=False)\n",
    "                    print(f\"✅ Completed {model} with {opt}\")\n",
    "                else:\n",
    "                    print(f\"❌ Failed to get metrics for {model} with {opt}\")\n",
    "            except Exception as e:\n",
    "                print(f\"❌ Error training {model} with {opt}: {str(e)}\")\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "# Comment out the line below if you want to run the evaluation\n",
    "# Otherwise, we'll load previously saved results in the next cell\n",
    "# all_results = run_full_evaluation()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0806e7c",
   "metadata": {},
   "source": [
    "## 4. Data Analysis and Visualization\n",
    "\n",
    "Now let's analyze and visualize the results to compare optimizer performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2b4802",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load saved results or use the ones we just generated\n",
    "try:\n",
    "    # Try to load the most recent results file if it exists\n",
    "    result_files = list(results_dir.glob(\"optimizer_comparison_*.csv\"))\n",
    "    if result_files and not 'all_results' in locals():\n",
    "        most_recent = sorted(result_files, key=os.path.getmtime, reverse=True)[0]\n",
    "        print(f\"Loading existing results from {most_recent}\")\n",
    "        results_df = pd.read_csv(most_recent)\n",
    "    else:\n",
    "        # Use the results we just generated\n",
    "        results_df = pd.DataFrame(all_results)\n",
    "        results_df.to_csv(results_file, index=False)\n",
    "        print(f\"Results saved to {results_file}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading results: {str(e)}\")\n",
    "    print(\"You might need to run the evaluation first by uncommenting the run_full_evaluation() call\")\n",
    "    # Create a dummy DataFrame for demonstration\n",
    "    results_df = pd.DataFrame({\n",
    "        'model': ['base', 'base', 'advanced', 'advanced', 'residual', 'residual'],\n",
    "        'optimizer': ['adam', 'sgd', 'adam', 'sgd', 'adam', 'sgd'],\n",
    "        'val_loss': [0.52, 0.56, 0.48, 0.51, 0.45, 0.47],\n",
    "        'val_mae': [0.52, 0.56, 0.48, 0.51, 0.45, 0.47],\n",
    "        'correlation': [0.65, 0.60, 0.70, 0.68, 0.72, 0.71],\n",
    "        'training_time': [45, 50, 60, 65, 75, 80]\n",
    "    })\n",
    "\n",
    "# Display the raw results\n",
    "print(\"Raw evaluation results:\")\n",
    "display(results_df)\n",
    "\n",
    "# Create pivot tables for each metric\n",
    "metrics = ['val_loss', 'val_mae', 'correlation', 'training_time']\n",
    "pivot_tables = {}\n",
    "\n",
    "for metric in metrics:\n",
    "    if metric in results_df.columns:\n",
    "        pivot_tables[metric] = results_df.pivot(index='model', columns='optimizer', values=metric)\n",
    "    \n",
    "# Display the pivot tables\n",
    "for metric, pivot in pivot_tables.items():\n",
    "    print(f\"\\n{metric.upper()} by model and optimizer:\")\n",
    "    display(pivot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e2a42c",
   "metadata": {},
   "source": [
    "## 5. Visualization: Heatmaps\n",
    "\n",
    "Let's visualize the results using heatmaps to easily identify the best optimizer for each model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff02a257",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to visualize the results as heatmap\n",
    "def plot_metric_heatmap(data, title, cmap='RdYlGn_r', fmt='.4f'):\n",
    "    if data is None or data.empty:\n",
    "        print(f\"No data available for {title} heatmap\")\n",
    "        return\n",
    "        \n",
    "    plt.figure(figsize=(16, 6))\n",
    "    \n",
    "    # For correlation metrics, higher is better so reverse the colormap\n",
    "    if 'correlation' in title.lower():\n",
    "        cmap = 'RdYlGn'\n",
    "    \n",
    "    # Create the heatmap\n",
    "    ax = sns.heatmap(\n",
    "        data, \n",
    "        annot=True, \n",
    "        cmap=cmap, \n",
    "        fmt=fmt, \n",
    "        linewidths=.5,\n",
    "        annot_kws={\"size\": 10}\n",
    "    )\n",
    "    \n",
    "    # Set title and labels\n",
    "    plt.title(f\"{title} by Model and Optimizer\", fontsize=16)\n",
    "    plt.ylabel('Model Architecture', fontsize=12)\n",
    "    plt.xlabel('Optimizer', fontsize=12)\n",
    "    \n",
    "    # Rotate x-axis labels\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    \n",
    "    # Save and show\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(results_dir / f\"{title.lower().replace(' ', '_')}.png\")\n",
    "    plt.show()\n",
    "\n",
    "# Create heatmaps for each metric\n",
    "if 'val_loss' in pivot_tables:\n",
    "    plot_metric_heatmap(pivot_tables['val_loss'], \"Validation Loss\", fmt='.4f')\n",
    "    \n",
    "if 'val_mae' in pivot_tables:\n",
    "    plot_metric_heatmap(pivot_tables['val_mae'], \"Mean Absolute Error\", fmt='.4f')\n",
    "    \n",
    "if 'correlation' in pivot_tables:\n",
    "    plot_metric_heatmap(pivot_tables['correlation'], \"Correlation Coefficient\", fmt='.4f')\n",
    "    \n",
    "if 'training_time' in pivot_tables:\n",
    "    plot_metric_heatmap(pivot_tables['training_time'], \"Training Time (seconds)\", fmt='.1f')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2102e79",
   "metadata": {},
   "source": [
    "## 6. Visualization: Radar Charts\n",
    "\n",
    "Let's create radar charts to compare optimizers across all model architectures:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133c2ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_radar_chart(df, metric='correlation'):\n",
    "    if metric not in df.columns:\n",
    "        print(f\"Metric '{metric}' not found in results dataframe\")\n",
    "        return\n",
    "        \n",
    "    # Group by optimizer and calculate mean for the metric\n",
    "    optimizer_means = df.groupby('optimizer')[metric].mean().reset_index()\n",
    "    \n",
    "    # Sort by performance\n",
    "    ascending = not (metric == 'correlation')  # For correlation, higher is better\n",
    "    optimizer_means = optimizer_means.sort_values(by=metric, ascending=ascending)\n",
    "    \n",
    "    # Prepare the radar chart\n",
    "    labels = optimizer_means['optimizer']\n",
    "    values = optimizer_means[metric].values\n",
    "    \n",
    "    # Number of variables\n",
    "    N = len(labels)\n",
    "    \n",
    "    # Create angle for each variable\n",
    "    angles = np.linspace(0, 2*np.pi, N, endpoint=False).tolist()\n",
    "    \n",
    "    # Close the loop\n",
    "    angles += angles[:1]\n",
    "    \n",
    "    # Values need to be repeated to close the loop\n",
    "    values = np.concatenate((values, [values[0]]))\n",
    "    \n",
    "    # Plot\n",
    "    fig, ax = plt.subplots(figsize=(10, 10), subplot_kw=dict(polar=True))\n",
    "    ax.plot(angles, values, linewidth=2, linestyle='solid')\n",
    "    ax.fill(angles, values, alpha=0.25)\n",
    "    \n",
    "    # Add labels\n",
    "    ax.set_thetagrids(np.degrees(angles[:-1]), labels)\n",
    "    \n",
    "    # Set y-axis limits based on the metric\n",
    "    if metric == 'correlation':\n",
    "        ax.set_ylim(min(values) - 0.05, max(values) + 0.05)\n",
    "    \n",
    "    plt.title(f\"Average {metric.replace('_', ' ').title()} by Optimizer\", size=15)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(results_dir / f\"radar_{metric}.png\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Also print the ranking table\n",
    "    print(f\"Optimizer ranking by average {metric}:\")\n",
    "    display(optimizer_means)\n",
    "\n",
    "# Generate radar charts for different metrics\n",
    "for metric in ['correlation', 'val_loss', 'val_mae', 'training_time']:\n",
    "    if metric in results_df.columns:\n",
    "        plot_radar_chart(results_df, metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b69adae",
   "metadata": {},
   "source": [
    "## 7. Best Optimizer For Each Model\n",
    "\n",
    "Finally, let's determine the best optimizer for each model architecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2945d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_optimizer(df, metric='correlation', is_higher_better=True):\n",
    "    \"\"\"Find the best optimizer for each model based on the specified metric\"\"\"\n",
    "    if metric not in df.columns:\n",
    "        print(f\"Metric '{metric}' not found in results dataframe\")\n",
    "        return None\n",
    "        \n",
    "    best_optimizers = []\n",
    "    \n",
    "    # Group by model\n",
    "    for model, group in df.groupby('model'):\n",
    "        if is_higher_better:\n",
    "            best_row = group.loc[group[metric].idxmax()]\n",
    "        else:\n",
    "            best_row = group.loc[group[metric].idxmin()]\n",
    "            \n",
    "        best_optimizers.append({\n",
    "            'model': model,\n",
    "            'best_optimizer': best_row['optimizer'],\n",
    "            f'best_{metric}': best_row[metric]\n",
    "        })\n",
    "        \n",
    "    return pd.DataFrame(best_optimizers)\n",
    "\n",
    "# Find best optimizer by correlation (higher is better)\n",
    "best_by_correlation = find_best_optimizer(results_df, 'correlation', is_higher_better=True)\n",
    "print(\"Best optimizers by correlation (higher is better):\")\n",
    "display(best_by_correlation)\n",
    "\n",
    "# Find best optimizer by validation loss (lower is better)\n",
    "if 'val_loss' in results_df.columns:\n",
    "    best_by_loss = find_best_optimizer(results_df, 'val_loss', is_higher_better=False)\n",
    "    print(\"\\nBest optimizers by validation loss (lower is better):\")\n",
    "    display(best_by_loss)\n",
    "\n",
    "# Create a summary visualization\n",
    "plt.figure(figsize=(12, 8))\n",
    "    \n",
    "models = results_df['model'].unique()\n",
    "for i, model in enumerate(models):\n",
    "    # Get data for this model\n",
    "    model_data = results_df[results_df['model'] == model]\n",
    "    \n",
    "    # Create subplot for this model\n",
    "    plt.subplot(len(models), 1, i+1)\n",
    "    \n",
    "    if 'correlation' in model_data.columns:\n",
    "        # Sort by correlation\n",
    "        sorted_data = model_data.sort_values(by='correlation', ascending=False)\n",
    "        \n",
    "        # Plot correlation bars\n",
    "        bars = plt.barh(\n",
    "            sorted_data['optimizer'], \n",
    "            sorted_data['correlation'], \n",
    "            color='skyblue'\n",
    "        )\n",
    "        \n",
    "        # Add value labels to the end of each bar\n",
    "        for bar in bars:\n",
    "            width = bar.get_width()\n",
    "            plt.text(\n",
    "                width + 0.01, \n",
    "                bar.get_y() + bar.get_height()/2, \n",
    "                f'{width:.4f}', \n",
    "                ha='left', \n",
    "                va='center'\n",
    "            )\n",
    "            \n",
    "        plt.xlim(0, max(sorted_data['correlation']) * 1.1)\n",
    "        plt.title(f\"{model.capitalize()} Model - Correlation by Optimizer\")\n",
    "        plt.xlabel(\"Correlation\")\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.savefig(results_dir / \"optimizer_summary.png\")\n",
    "plt.show()\n",
    "\n",
    "# Create recommendations based on findings\n",
    "print(\"\\n=== OPTIMIZER RECOMMENDATIONS ===\")\n",
    "print(\"Based on the evaluation results, here are the recommended optimizers:\")\n",
    "for model in models:\n",
    "    best_opt = best_by_correlation[best_by_correlation['model'] == model]['best_optimizer'].values[0]\n",
    "    best_corr = best_by_correlation[best_by_correlation['model'] == model]['best_correlation'].values[0]\n",
    "    print(f\"- For {model.capitalize()} model: Use {best_opt} optimizer (correlation = {best_corr:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad40bc9",
   "metadata": {},
   "source": [
    "## 8. Conclusion\n",
    "\n",
    "In this notebook, we have:\n",
    "1. Set up a comprehensive optimizer evaluation framework\n",
    "2. Trained multiple models with various optimizers\n",
    "3. Collected and analyzed performance metrics\n",
    "4. Visualized the results using heatmaps and radar charts\n",
    "5. Determined the best optimizer for each model architecture\n",
    "\n",
    "This analysis provides valuable insights for model selection and hyperparameter tuning in our project."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
