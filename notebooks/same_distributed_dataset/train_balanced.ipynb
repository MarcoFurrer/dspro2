{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9710307b",
   "metadata": {},
   "source": [
    "# Train Model with Balanced Dataset\n",
    "\n",
    "This notebook trains models using the balanced dataset created in `create_dataset.ipynb`. The balanced dataset ensures equal representation of all target categories (0, 0.25, 0.5, 0.75, 1) with 20% probability each.\n",
    "\n",
    "**Training Approach:**\n",
    "- Use existing model architectures from the codebase\n",
    "- Train with the balanced dataset to prevent distribution bias\n",
    "- Monitor performance across all target ranges\n",
    "- Save trained models for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "998fdbc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: /Users/marcofurrer/Documents/github/dspro2\n",
      "✅ All imports successful!\n",
      "TensorFlow version: 2.15.0\n"
     ]
    }
   ],
   "source": [
    "# Setup and imports\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the project root to the path so we can import from src\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '../..'))\n",
    "sys.path.append(project_root)\n",
    "print(f\"Project root: {project_root}\")\n",
    "\n",
    "# Standard libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "# TensorFlow\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam, SGD, RMSprop\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.losses import MeanAbsoluteError\n",
    "\n",
    "# Project imports\n",
    "from src.data_handler import DataHandler\n",
    "from src.model_evaluator import ModelEvaluator\n",
    "\n",
    "# Import model architectures\n",
    "from src.models.BestModel import model as best_model\n",
    "from src.models.CorrelationModel import model as correlation_model\n",
    "from src.models.Deep import model as deep_model\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Configure matplotlib\n",
    "plt.style.use('ggplot')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "print(\"✅ All imports successful!\")\n",
    "print(f\"TensorFlow version: {tf.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0706b49e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading original dataset...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 98\u001b[0m\n\u001b[1;32m     95\u001b[0m     plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[1;32m     97\u001b[0m \u001b[38;5;66;03m# Load and analyze original dataset\u001b[39;00m\n\u001b[0;32m---> 98\u001b[0m original_df \u001b[38;5;241m=\u001b[39m \u001b[43mload_and_analyze_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;66;03m# Create balanced dataset\u001b[39;00m\n\u001b[1;32m    101\u001b[0m balanced_df \u001b[38;5;241m=\u001b[39m create_balanced_dataset(original_df)\n",
      "Cell \u001b[0;32mIn[7], line 6\u001b[0m, in \u001b[0;36mload_and_analyze_dataset\u001b[0;34m(data_path)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Load and analyze the original dataset distribution\"\"\"\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading original dataset...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdf\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumns: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(df\u001b[38;5;241m.\u001b[39mcolumns)[:\u001b[38;5;241m10\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# Show first 10 columns\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/github/dspro2/.venv/lib/python3.9/site-packages/pandas/io/parquet.py:509\u001b[0m, in \u001b[0;36mread_parquet\u001b[0;34m(path, engine, columns, storage_options, use_nullable_dtypes, dtype_backend, **kwargs)\u001b[0m\n\u001b[1;32m    506\u001b[0m     use_nullable_dtypes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    507\u001b[0m check_dtype_backend(dtype_backend)\n\u001b[0;32m--> 509\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimpl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    510\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    511\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    512\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    513\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_nullable_dtypes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_nullable_dtypes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    514\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    515\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    516\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/github/dspro2/.venv/lib/python3.9/site-packages/pandas/io/parquet.py:230\u001b[0m, in \u001b[0;36mPyArrowImpl.read\u001b[0;34m(self, path, columns, use_nullable_dtypes, dtype_backend, storage_options, **kwargs)\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    227\u001b[0m     pa_table \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi\u001b[38;5;241m.\u001b[39mparquet\u001b[38;5;241m.\u001b[39mread_table(\n\u001b[1;32m    228\u001b[0m         path_or_handle, columns\u001b[38;5;241m=\u001b[39mcolumns, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    229\u001b[0m     )\n\u001b[0;32m--> 230\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mpa_table\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_pandas\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mto_pandas_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    232\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m manager \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    233\u001b[0m         result \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39m_as_manager(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray\u001b[39m\u001b[38;5;124m\"\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/Documents/github/dspro2/.venv/lib/python3.9/site-packages/pyarrow/array.pxi:830\u001b[0m, in \u001b[0;36mpyarrow.lib._PandasConvertible.to_pandas\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/Documents/github/dspro2/.venv/lib/python3.9/site-packages/pyarrow/table.pxi:3990\u001b[0m, in \u001b[0;36mpyarrow.lib.Table._to_pandas\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/Documents/github/dspro2/.venv/lib/python3.9/site-packages/pyarrow/pandas_compat.py:820\u001b[0m, in \u001b[0;36mtable_to_blockmanager\u001b[0;34m(options, table, categories, ignore_metadata, types_mapper)\u001b[0m\n\u001b[1;32m    818\u001b[0m _check_data_column_metadata_consistency(all_columns)\n\u001b[1;32m    819\u001b[0m columns \u001b[38;5;241m=\u001b[39m _deserialize_column_index(table, all_columns, column_indexes)\n\u001b[0;32m--> 820\u001b[0m blocks \u001b[38;5;241m=\u001b[39m \u001b[43m_table_to_blocks\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcategories\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mext_columns_dtypes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    822\u001b[0m axes \u001b[38;5;241m=\u001b[39m [columns, index]\n\u001b[1;32m    823\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m BlockManager(blocks, axes)\n",
      "File \u001b[0;32m~/Documents/github/dspro2/.venv/lib/python3.9/site-packages/pyarrow/pandas_compat.py:1169\u001b[0m, in \u001b[0;36m_table_to_blocks\u001b[0;34m(options, block_table, categories, extension_columns)\u001b[0m\n\u001b[1;32m   1164\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_table_to_blocks\u001b[39m(options, block_table, categories, extension_columns):\n\u001b[1;32m   1165\u001b[0m     \u001b[38;5;66;03m# Part of table_to_blockmanager\u001b[39;00m\n\u001b[1;32m   1166\u001b[0m \n\u001b[1;32m   1167\u001b[0m     \u001b[38;5;66;03m# Convert an arrow table to Block from the internal pandas API\u001b[39;00m\n\u001b[1;32m   1168\u001b[0m     columns \u001b[38;5;241m=\u001b[39m block_table\u001b[38;5;241m.\u001b[39mcolumn_names\n\u001b[0;32m-> 1169\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mpa\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtable_to_blocks\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblock_table\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcategories\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1170\u001b[0m \u001b[43m                                    \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mextension_columns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeys\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1171\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [_reconstruct_block(item, columns, extension_columns)\n\u001b[1;32m   1172\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m result]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ===== STEP 1: CREATE BALANCED DATASET =====\n",
    "\n",
    "def load_and_analyze_dataset(data_path=\"../../data/train.parquet\"):\n",
    "    \"\"\"Load and analyze the original dataset distribution\"\"\"\n",
    "    print(\"Loading original dataset...\")\n",
    "    df = pd.read_parquet(data_path)\n",
    "    \n",
    "    print(f\"Dataset shape: {df.shape}\")\n",
    "    print(f\"Columns: {list(df.columns)[:10]}...\")  # Show first 10 columns\n",
    "    \n",
    "    # Analyze target distribution\n",
    "    target_counts = df['target'].value_counts().sort_index()\n",
    "    target_percentages = df['target'].value_counts(normalize=True).sort_index() * 100\n",
    "    \n",
    "    print(\"\\nCurrent target distribution:\")\n",
    "    for target, count in target_counts.items():\n",
    "        percentage = target_percentages[target]\n",
    "        print(f\"  {target}: {count:,} samples ({percentage:.1f}%)\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def create_balanced_dataset(df, target_column='target', save_path=\"../../data/train_balanced.parquet\"):\n",
    "    \"\"\"Create a balanced dataset with equal representation of all target categories\"\"\"\n",
    "    print(\"\\nCreating balanced dataset...\")\n",
    "    \n",
    "    # Get current distribution\n",
    "    target_counts = df[target_column].value_counts()\n",
    "    min_count = target_counts.min()\n",
    "    \n",
    "    print(f\"Minimum category count: {min_count:,}\")\n",
    "    print(f\"Will downsample all categories to {min_count:,} samples each\")\n",
    "    \n",
    "    # Create balanced dataset by downsampling\n",
    "    balanced_dfs = []\n",
    "    for target_value in df[target_column].unique():\n",
    "        target_subset = df[df[target_column] == target_value]\n",
    "        if len(target_subset) > min_count:\n",
    "            # Downsample\n",
    "            target_subset = target_subset.sample(n=min_count, random_state=42)\n",
    "        balanced_dfs.append(target_subset)\n",
    "    \n",
    "    # Combine and shuffle\n",
    "    balanced_df = pd.concat(balanced_dfs, ignore_index=True)\n",
    "    balanced_df = balanced_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    \n",
    "    # Verify balance\n",
    "    balanced_counts = balanced_df[target_column].value_counts().sort_index()\n",
    "    balanced_percentages = balanced_df[target_column].value_counts(normalize=True).sort_index() * 100\n",
    "    \n",
    "    print(f\"\\nBalanced dataset shape: {balanced_df.shape}\")\n",
    "    print(\"\\nNew target distribution:\")\n",
    "    for target, count in balanced_counts.items():\n",
    "        percentage = balanced_percentages[target]\n",
    "        print(f\"  {target}: {count:,} samples ({percentage:.1f}%)\")\n",
    "    \n",
    "    # Save balanced dataset\n",
    "    balanced_df.to_parquet(save_path)\n",
    "    print(f\"\\nBalanced dataset saved to: {save_path}\")\n",
    "    \n",
    "    return balanced_df\n",
    "\n",
    "def visualize_distribution_comparison(original_df, balanced_df, target_column='target'):\n",
    "    \"\"\"Visualize the comparison between original and balanced distributions\"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Original distribution - counts\n",
    "    original_counts = original_df[target_column].value_counts().sort_index()\n",
    "    axes[0, 0].bar(original_counts.index, original_counts.values, alpha=0.7, color='skyblue')\n",
    "    axes[0, 0].set_title('Original Dataset - Sample Counts')\n",
    "    axes[0, 0].set_xlabel('Target Value')\n",
    "    axes[0, 0].set_ylabel('Count')\n",
    "    \n",
    "    # Original distribution - percentages\n",
    "    original_pct = original_df[target_column].value_counts(normalize=True).sort_index() * 100\n",
    "    axes[0, 1].bar(original_pct.index, original_pct.values, alpha=0.7, color='lightcoral')\n",
    "    axes[0, 1].set_title('Original Dataset - Percentage Distribution')\n",
    "    axes[0, 1].set_xlabel('Target Value')\n",
    "    axes[0, 1].set_ylabel('Percentage (%)')\n",
    "    \n",
    "    # Balanced distribution - counts\n",
    "    balanced_counts = balanced_df[target_column].value_counts().sort_index()\n",
    "    axes[1, 0].bar(balanced_counts.index, balanced_counts.values, alpha=0.7, color='lightgreen')\n",
    "    axes[1, 0].set_title('Balanced Dataset - Sample Counts')\n",
    "    axes[1, 0].set_xlabel('Target Value')\n",
    "    axes[1, 0].set_ylabel('Count')\n",
    "    \n",
    "    # Balanced distribution - percentages\n",
    "    balanced_pct = balanced_df[target_column].value_counts(normalize=True).sort_index() * 100\n",
    "    axes[1, 1].bar(balanced_pct.index, balanced_pct.values, alpha=0.7, color='gold')\n",
    "    axes[1, 1].set_title('Balanced Dataset - Percentage Distribution')\n",
    "    axes[1, 1].set_xlabel('Target Value')\n",
    "    axes[1, 1].set_ylabel('Percentage (%)')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Load and analyze original dataset\n",
    "original_df = load_and_analyze_dataset()\n",
    "\n",
    "# Create balanced dataset\n",
    "balanced_df = create_balanced_dataset(original_df)\n",
    "\n",
    "# Visualize comparison\n",
    "visualize_distribution_comparison(original_df, balanced_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbda3c9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading original dataset...\n",
      "Dataset shape: (2746270, 2415)\n",
      "Columns: ['era', 'data_type', 'feature_shaded_hallucinatory_dactylology', 'feature_itinerant_hexahedral_photoengraver', 'feature_prudent_pileate_oven', 'feature_subalpine_apothegmatical_ajax', 'feature_pistachio_atypical_malison', 'feature_symmetrical_spongy_tricentenary', 'feature_ungrounded_transpontine_winder', 'feature_aseptic_eely_hemiplegia']...\n",
      "\n",
      "Current target distribution:\n",
      "  0.0: 135,668 samples (4.9%)\n",
      "  0.25: 550,995 samples (20.1%)\n",
      "  0.5: 1,373,108 samples (50.0%)\n",
      "  0.75: 550,373 samples (20.0%)\n",
      "  1.0: 136,126 samples (5.0%)\n",
      "Dataset shape: (2746270, 2415)\n",
      "Columns: ['era', 'data_type', 'feature_shaded_hallucinatory_dactylology', 'feature_itinerant_hexahedral_photoengraver', 'feature_prudent_pileate_oven', 'feature_subalpine_apothegmatical_ajax', 'feature_pistachio_atypical_malison', 'feature_symmetrical_spongy_tricentenary', 'feature_ungrounded_transpontine_winder', 'feature_aseptic_eely_hemiplegia']...\n",
      "\n",
      "Current target distribution:\n",
      "  0.0: 135,668 samples (4.9%)\n",
      "  0.25: 550,995 samples (20.1%)\n",
      "  0.5: 1,373,108 samples (50.0%)\n",
      "  0.75: 550,373 samples (20.0%)\n",
      "  1.0: 136,126 samples (5.0%)\n",
      "\n",
      "Creating balanced dataset...\n",
      "Minimum category count: 135,668\n",
      "Will downsample all categories to 135,668 samples each\n",
      "\n",
      "Creating balanced dataset...\n",
      "Minimum category count: 135,668\n",
      "Will downsample all categories to 135,668 samples each\n"
     ]
    }
   ],
   "source": [
    "# Execute dataset creation\n",
    "original_df = load_and_analyze_dataset()\n",
    "balanced_df = create_balanced_dataset(original_df)\n",
    "visualize_distribution_comparison(original_df, balanced_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd716f6",
   "metadata": {},
   "source": [
    "## Model Training Setup\n",
    "\n",
    "Now we'll set up training functions and train models with the balanced dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6829b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== STEP 2: TRAINING SETUP =====\n",
    "\n",
    "def create_callbacks(model_name, patience=15):\n",
    "    \"\"\"Create training callbacks\"\"\"\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    \n",
    "    callbacks = [\n",
    "        EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=patience,\n",
    "            restore_best_weights=True,\n",
    "            verbose=1\n",
    "        ),\n",
    "        ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.5,\n",
    "            patience=5,\n",
    "            min_lr=0.00001,\n",
    "            verbose=1\n",
    "        ),\n",
    "        ModelCheckpoint(\n",
    "            filepath=f'../../exports/{model_name}_balanced_{timestamp}.keras',\n",
    "            monitor='val_loss',\n",
    "            save_best_only=True,\n",
    "            verbose=1\n",
    "        )\n",
    "    ]\n",
    "    return callbacks\n",
    "\n",
    "def prepare_data(df, feature_columns=None, validation_split=0.2):\n",
    "    \"\"\"Prepare training and validation data\"\"\"\n",
    "    if feature_columns is None:\n",
    "        # Exclude non-feature columns\n",
    "        exclude_cols = ['target', 'era', 'data_type'] if 'era' in df.columns else ['target']\n",
    "        feature_columns = [col for col in df.columns if col not in exclude_cols]\n",
    "    \n",
    "    X = df[feature_columns].values.astype(np.float32)\n",
    "    y = df['target'].values.astype(np.float32)\n",
    "    \n",
    "    # Split data\n",
    "    n_train = int(len(X) * (1 - validation_split))\n",
    "    \n",
    "    X_train, X_val = X[:n_train], X[n_train:]\n",
    "    y_train, y_val = y[:n_train], y[n_train:]\n",
    "    \n",
    "    print(f\"Training samples: {len(X_train):,}\")\n",
    "    print(f\"Validation samples: {len(X_val):,}\")\n",
    "    print(f\"Number of features: {X.shape[1]}\")\n",
    "    \n",
    "    return X_train, X_val, y_train, y_val, feature_columns\n",
    "\n",
    "def get_optimizer(optimizer_name, learning_rate=0.001):\n",
    "    \"\"\"Get optimizer by name\"\"\"\n",
    "    optimizers = {\n",
    "        'Adam': Adam(learning_rate=learning_rate),\n",
    "        'SGD': SGD(learning_rate=learning_rate),\n",
    "        'RMSprop': RMSprop(learning_rate=learning_rate)\n",
    "    }\n",
    "    return optimizers.get(optimizer_name, Adam(learning_rate=learning_rate))\n",
    "\n",
    "def train_model(model_fn, model_name, optimizer_name, X_train, X_val, y_train, y_val, \n",
    "               epochs=100, batch_size=512, learning_rate=0.001):\n",
    "    \"\"\"Train a single model\"\"\"\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Training {model_name} with {optimizer_name}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    # Create model\n",
    "    input_shape = (X_train.shape[1],)\n",
    "    model = model_fn(input_shape)\n",
    "    \n",
    "    # Compile model\n",
    "    optimizer = get_optimizer(optimizer_name, learning_rate)\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss='mae',\n",
    "        metrics=['mse']\n",
    "    )\n",
    "    \n",
    "    print(f\"Model architecture:\")\n",
    "    model.summary()\n",
    "    \n",
    "    # Create callbacks\n",
    "    callbacks = create_callbacks(f\"{model_name}_{optimizer_name}\")\n",
    "    \n",
    "    # Train model\n",
    "    start_time = datetime.now()\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_val, y_val),\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "    end_time = datetime.now()\n",
    "    \n",
    "    training_time = (end_time - start_time).total_seconds()\n",
    "    print(f\"Training completed in {training_time:.1f} seconds\")\n",
    "    \n",
    "    return model, history, training_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a987a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== STEP 3: EVALUATION FUNCTIONS =====\n",
    "\n",
    "def evaluate_model(model, X_val, y_val, model_name):\n",
    "    \"\"\"Evaluate model performance\"\"\"\n",
    "    y_pred = model.predict(X_val)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    mae = np.mean(np.abs(y_val - y_pred.flatten()))\n",
    "    mse = np.mean((y_val - y_pred.flatten()) ** 2)\n",
    "    rmse = np.sqrt(mse)\n",
    "    \n",
    "    # Calculate correlation\n",
    "    correlation = np.corrcoef(y_val, y_pred.flatten())[0, 1]\n",
    "    \n",
    "    # Calculate per-target performance\n",
    "    target_performance = {}\n",
    "    for target_val in np.unique(y_val):\n",
    "        mask = y_val == target_val\n",
    "        if np.sum(mask) > 0:\n",
    "            target_mae = np.mean(np.abs(y_val[mask] - y_pred.flatten()[mask]))\n",
    "            target_performance[target_val] = target_mae\n",
    "    \n",
    "    results = {\n",
    "        'model': model_name,\n",
    "        'mae': mae,\n",
    "        'mse': mse,\n",
    "        'rmse': rmse,\n",
    "        'correlation': correlation,\n",
    "        'target_performance': target_performance\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nEvaluation Results for {model_name}:\")\n",
    "    print(f\"  MAE: {mae:.4f}\")\n",
    "    print(f\"  MSE: {mse:.4f}\")\n",
    "    print(f\"  RMSE: {rmse:.4f}\")\n",
    "    print(f\"  Correlation: {correlation:.4f}\")\n",
    "    print(f\"  Per-target MAE:\")\n",
    "    for target, target_mae in target_performance.items():\n",
    "        print(f\"    {target}: {target_mae:.4f}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "def plot_training_history(history, model_name):\n",
    "    \"\"\"Plot training history\"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Loss plot\n",
    "    axes[0].plot(history.history['loss'], label='Training Loss', alpha=0.8)\n",
    "    axes[0].plot(history.history['val_loss'], label='Validation Loss', alpha=0.8)\n",
    "    axes[0].set_title(f'{model_name} - Training Loss')\n",
    "    axes[0].set_xlabel('Epoch')\n",
    "    axes[0].set_ylabel('MAE Loss')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # MSE plot\n",
    "    if 'mse' in history.history:\n",
    "        axes[1].plot(history.history['mse'], label='Training MSE', alpha=0.8)\n",
    "        axes[1].plot(history.history['val_mse'], label='Validation MSE', alpha=0.8)\n",
    "        axes[1].set_title(f'{model_name} - Training MSE')\n",
    "        axes[1].set_xlabel('Epoch')\n",
    "        axes[1].set_ylabel('MSE')\n",
    "        axes[1].legend()\n",
    "        axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_predictions_vs_actual(y_true, y_pred, model_name):\n",
    "    \"\"\"Plot predictions vs actual values\"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Scatter plot\n",
    "    axes[0].scatter(y_true, y_pred, alpha=0.5)\n",
    "    axes[0].plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()], 'r--', lw=2)\n",
    "    axes[0].set_xlabel('Actual Values')\n",
    "    axes[0].set_ylabel('Predicted Values')\n",
    "    axes[0].set_title(f'{model_name} - Predictions vs Actual')\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Residuals plot\n",
    "    residuals = y_true - y_pred.flatten()\n",
    "    axes[1].scatter(y_pred, residuals, alpha=0.5)\n",
    "    axes[1].axhline(y=0, color='r', linestyle='--')\n",
    "    axes[1].set_xlabel('Predicted Values')\n",
    "    axes[1].set_ylabel('Residuals')\n",
    "    axes[1].set_title(f'{model_name} - Residuals Plot')\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30751122",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "Prepare the balanced dataset for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de20167",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for training\n",
    "print(\"Preparing training data from balanced dataset...\")\n",
    "X_train, X_val, y_train, y_val, feature_columns = prepare_data(balanced_df)\n",
    "\n",
    "print(f\"\\nData preparation complete!\")\n",
    "print(f\"Feature columns: {len(feature_columns)}\")\n",
    "print(f\"Feature names (first 10): {feature_columns[:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd860a9",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "\n",
    "Train different model architectures with three optimizers (Adam, SGD, RMSprop) using the balanced dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459df301",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== STEP 4: TRAIN MODELS =====\n",
    "\n",
    "# Define models to train\n",
    "models_to_train = {\n",
    "    'BestModel': best_model,\n",
    "    'CorrelationModel': correlation_model,\n",
    "    'DeepModel': deep_model\n",
    "}\n",
    "\n",
    "# Define optimizers to test\n",
    "optimizers_to_test = ['Adam', 'SGD', 'RMSprop']\n",
    "\n",
    "# Training configuration\n",
    "training_config = {\n",
    "    'epochs': 100,\n",
    "    'batch_size': 512,\n",
    "    'learning_rate': 0.001\n",
    "}\n",
    "\n",
    "print(f\"Training {len(models_to_train)} models with {len(optimizers_to_test)} optimizers each\")\n",
    "print(f\"Total combinations: {len(models_to_train) * len(optimizers_to_test)}\")\n",
    "print(f\"Training configuration: {training_config}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a6a720",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute training\n",
    "training_results = []\n",
    "trained_models = {}\n",
    "\n",
    "for model_name, model_fn in models_to_train.items():\n",
    "    for optimizer_name in optimizers_to_test:\n",
    "        try:\n",
    "            # Train model\n",
    "            model, history, training_time = train_model(\n",
    "                model_fn=model_fn,\n",
    "                model_name=model_name,\n",
    "                optimizer_name=optimizer_name,\n",
    "                X_train=X_train,\n",
    "                X_val=X_val,\n",
    "                y_train=y_train,\n",
    "                y_val=y_val,\n",
    "                **training_config\n",
    "            )\n",
    "            \n",
    "            # Evaluate model\n",
    "            eval_results = evaluate_model(\n",
    "                model=model,\n",
    "                X_val=X_val,\n",
    "                y_val=y_val,\n",
    "                model_name=f\"{model_name}_{optimizer_name}\"\n",
    "            )\n",
    "            \n",
    "            # Add training time and configuration to results\n",
    "            eval_results['training_time'] = training_time\n",
    "            eval_results['optimizer'] = optimizer_name\n",
    "            eval_results['epochs_trained'] = len(history.history['loss'])\n",
    "            eval_results['final_train_loss'] = history.history['loss'][-1]\n",
    "            eval_results['final_val_loss'] = history.history['val_loss'][-1]\n",
    "            \n",
    "            training_results.append(eval_results)\n",
    "            trained_models[f\"{model_name}_{optimizer_name}\"] = {\n",
    "                'model': model,\n",
    "                'history': history\n",
    "            }\n",
    "            \n",
    "            # Plot training history\n",
    "            plot_training_history(history, f\"{model_name}_{optimizer_name}\")\n",
    "            \n",
    "            # Plot predictions\n",
    "            y_pred = model.predict(X_val)\n",
    "            plot_predictions_vs_actual(y_val, y_pred, f\"{model_name}_{optimizer_name}\")\n",
    "            \n",
    "            print(f\"✅ {model_name} with {optimizer_name} completed successfully\\n\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error training {model_name} with {optimizer_name}: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "print(f\"\\n🎉 Training completed! {len(training_results)} models trained successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a05fdcc",
   "metadata": {},
   "source": [
    "## Results Analysis\n",
    "\n",
    "Analyze and compare the performance of all trained models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0794c6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== STEP 5: ANALYZE RESULTS =====\n",
    "\n",
    "# Convert results to DataFrame for analysis\n",
    "if training_results:\n",
    "    results_df = pd.DataFrame(training_results)\n",
    "    \n",
    "    # Extract model name and optimizer for better analysis\n",
    "    results_df[['model_name', 'optimizer']] = results_df['model'].str.split('_', n=1, expand=True)\n",
    "    \n",
    "    print(\"Training Results Summary:\")\n",
    "    print(\"=\" * 50)\n",
    "    display(results_df[['model_name', 'optimizer', 'mae', 'correlation', 'training_time']].round(4))\n",
    "    \n",
    "    # Best performers by metric\n",
    "    print(\"\\nBest Performers:\")\n",
    "    print(\"=\" * 30)\n",
    "    \n",
    "    # Best by MAE (lower is better)\n",
    "    best_mae = results_df.loc[results_df['mae'].idxmin()]\n",
    "    print(f\"Best MAE: {best_mae['model']} (MAE: {best_mae['mae']:.4f})\")\n",
    "    \n",
    "    # Best by correlation (higher is better)\n",
    "    best_corr = results_df.loc[results_df['correlation'].idxmax()]\n",
    "    print(f\"Best Correlation: {best_corr['model']} (Correlation: {best_corr['correlation']:.4f})\")\n",
    "    \n",
    "    # Fastest training\n",
    "    fastest = results_df.loc[results_df['training_time'].idxmin()]\n",
    "    print(f\"Fastest Training: {fastest['model']} (Time: {fastest['training_time']:.1f}s)\")\n",
    "else:\n",
    "    print(\"No training results available for analysis.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df25860",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison visualizations\n",
    "if training_results:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    # MAE comparison\n",
    "    mae_pivot = results_df.pivot(index='model_name', columns='optimizer', values='mae')\n",
    "    sns.heatmap(mae_pivot, annot=True, fmt='.4f', cmap='YlOrRd_r', ax=axes[0, 0])\n",
    "    axes[0, 0].set_title('MAE by Model and Optimizer (Lower is Better)')\n",
    "    \n",
    "    # Correlation comparison\n",
    "    corr_pivot = results_df.pivot(index='model_name', columns='optimizer', values='correlation')\n",
    "    sns.heatmap(corr_pivot, annot=True, fmt='.4f', cmap='YlGn', ax=axes[0, 1])\n",
    "    axes[0, 1].set_title('Correlation by Model and Optimizer (Higher is Better)')\n",
    "    \n",
    "    # Training time comparison\n",
    "    time_pivot = results_df.pivot(index='model_name', columns='optimizer', values='training_time')\n",
    "    sns.heatmap(time_pivot, annot=True, fmt='.1f', cmap='YlOrRd_r', ax=axes[1, 0])\n",
    "    axes[1, 0].set_title('Training Time (seconds) by Model and Optimizer')\n",
    "    \n",
    "    # Overall performance (MAE vs Correlation)\n",
    "    for optimizer in optimizers_to_test:\n",
    "        opt_data = results_df[results_df['optimizer'] == optimizer]\n",
    "        axes[1, 1].scatter(opt_data['mae'], opt_data['correlation'], \n",
    "                          label=optimizer, s=100, alpha=0.7)\n",
    "        \n",
    "        # Add model labels\n",
    "        for _, row in opt_data.iterrows():\n",
    "            axes[1, 1].annotate(row['model_name'], \n",
    "                               (row['mae'], row['correlation']),\n",
    "                               xytext=(5, 5), textcoords='offset points',\n",
    "                               fontsize=8, alpha=0.8)\n",
    "    \n",
    "    axes[1, 1].set_xlabel('MAE (Lower is Better)')\n",
    "    axes[1, 1].set_ylabel('Correlation (Higher is Better)')\n",
    "    axes[1, 1].set_title('MAE vs Correlation by Optimizer')\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a1d24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-target performance analysis\n",
    "if training_results:\n",
    "    print(\"\\nPer-Target Performance Analysis:\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    target_values = [0.0, 0.25, 0.5, 0.75, 1.0]\n",
    "    target_performance_data = []\n",
    "    \n",
    "    for result in training_results:\n",
    "        for target_val, target_mae in result['target_performance'].items():\n",
    "            target_performance_data.append({\n",
    "                'model': result['model'],\n",
    "                'model_name': result['model'].split('_')[0],\n",
    "                'optimizer': result['optimizer'],\n",
    "                'target_value': target_val,\n",
    "                'target_mae': target_mae\n",
    "            })\n",
    "    \n",
    "    target_df = pd.DataFrame(target_performance_data)\n",
    "    \n",
    "    # Create per-target heatmap\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    target_pivot = target_df.pivot_table(\n",
    "        index='model', \n",
    "        columns='target_value', \n",
    "        values='target_mae',\n",
    "        aggfunc='mean'\n",
    "    )\n",
    "    \n",
    "    sns.heatmap(target_pivot, annot=True, fmt='.4f', cmap='YlOrRd_r')\n",
    "    plt.title('Per-Target MAE Performance (Lower is Better)')\n",
    "    plt.xlabel('Target Value')\n",
    "    plt.ylabel('Model')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Check for balanced performance across targets\n",
    "    print(\"\\nTarget Performance Balance Check:\")\n",
    "    for model in target_df['model'].unique():\n",
    "        model_targets = target_df[target_df['model'] == model]\n",
    "        target_std = model_targets['target_mae'].std()\n",
    "        target_mean = model_targets['target_mae'].mean()\n",
    "        print(f\"{model}: Mean MAE = {target_mean:.4f}, Std = {target_std:.4f}\")\n",
    "        if target_std < 0.02:  # Low standard deviation indicates balanced performance\n",
    "            print(f\"  ✅ Well-balanced across all targets\")\n",
    "        else:\n",
    "            print(f\"  ⚠️ Some variation in target performance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a926aa",
   "metadata": {},
   "source": [
    "## Summary and Recommendations\n",
    "\n",
    "Based on the training results with the balanced dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fed6e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary and recommendations\n",
    "if training_results:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"TRAINING SUMMARY WITH BALANCED DATASET\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Overall statistics\n",
    "    print(f\"\\n📊 Training Statistics:\")\n",
    "    print(f\"   Models trained: {len(set(r['model_name'] for r in training_results))}\")\n",
    "    print(f\"   Optimizers tested: {len(set(r['optimizer'] for r in training_results))}\")\n",
    "    print(f\"   Total combinations: {len(training_results)}\")\n",
    "    print(f\"   Average training time: {np.mean([r['training_time'] for r in training_results]):.1f}s\")\n",
    "    \n",
    "    # Performance ranges\n",
    "    mae_values = [r['mae'] for r in training_results]\n",
    "    corr_values = [r['correlation'] for r in training_results]\n",
    "    \n",
    "    print(f\"\\n📈 Performance Ranges:\")\n",
    "    print(f\"   MAE: {min(mae_values):.4f} - {max(mae_values):.4f}\")\n",
    "    print(f\"   Correlation: {min(corr_values):.4f} - {max(corr_values):.4f}\")\n",
    "    \n",
    "    # Top 3 performers by MAE\n",
    "    top_mae = sorted(training_results, key=lambda x: x['mae'])[:3]\n",
    "    print(f\"\\n🏆 Top 3 Models by MAE:\")\n",
    "    for i, model in enumerate(top_mae, 1):\n",
    "        print(f\"   {i}. {model['model']} - MAE: {model['mae']:.4f}, Correlation: {model['correlation']:.4f}\")\n",
    "    \n",
    "    # Benefits of balanced training\n",
    "    print(f\"\\n✅ Benefits of Balanced Dataset Training:\")\n",
    "    print(f\"   • Equal representation of all target values (0, 0.25, 0.5, 0.75, 1.0)\")\n",
    "    print(f\"   • Reduced bias towards middle values (0.5)\")\n",
    "    print(f\"   • Better generalization across all prediction ranges\")\n",
    "    print(f\"   • More reliable performance metrics\")\n",
    "    \n",
    "    # Save results\n",
    "    results_df.to_csv('../../exports/balanced_training_results.csv', index=False)\n",
    "    print(f\"\\n💾 Results saved to: ../../exports/balanced_training_results.csv\")\n",
    "    \n",
    "    print(f\"\\n🎯 Recommendation: Use the best performing model-optimizer combination\")\n",
    "    print(f\"   for production, considering both MAE and correlation scores.\")\n",
    "else:\n",
    "    print(\"No results to summarize.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659850d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== STEP 2: MODEL PREPARATION AND CONFIGURATION =====\n",
    "\n",
    "def get_feature_columns(df):\n",
    "    \"\"\"Get feature columns excluding target and metadata columns\"\"\"\n",
    "    exclude_cols = ['target', 'era', 'data_type', 'id'] if 'era' in df.columns else ['target', 'id']\n",
    "    feature_cols = [col for col in df.columns if col not in exclude_cols]\n",
    "    print(f\"Feature columns: {len(feature_cols)} features\")\n",
    "    return feature_cols\n",
    "\n",
    "def prepare_data(df, feature_cols, test_size=0.2, val_size=0.1):\n",
    "    \"\"\"Prepare training, validation, and test data\"\"\"\n",
    "    X = df[feature_cols].values.astype(np.float32)\n",
    "    y = df['target'].values.astype(np.float32)\n",
    "    \n",
    "    # First split: train+val vs test\n",
    "    X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    # Second split: train vs val\n",
    "    val_size_adjusted = val_size / (1 - test_size)  # Adjust val_size for remaining data\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_train_val, y_train_val, test_size=val_size_adjusted, random_state=42, stratify=y_train_val\n",
    "    )\n",
    "    \n",
    "    print(f\"Data splits:\")\n",
    "    print(f\"  Training: {X_train.shape[0]:,} samples\")\n",
    "    print(f\"  Validation: {X_val.shape[0]:,} samples\")\n",
    "    print(f\"  Test: {X_test.shape[0]:,} samples\")\n",
    "    \n",
    "    return (X_train, y_train), (X_val, y_val), (X_test, y_test)\n",
    "\n",
    "# Model configurations\n",
    "MODEL_CONFIGS = {\n",
    "    'BestModel': {\n",
    "        'model_fn': best_model,\n",
    "        'description': 'Advanced model with distribution-aware layers and attention mechanisms'\n",
    "    },\n",
    "    'CorrelationModel': {\n",
    "        'model_fn': correlation_model,\n",
    "        'description': 'Model focused on capturing feature correlations and interactions'\n",
    "    },\n",
    "    'DeepModel': {\n",
    "        'model_fn': deep_model,\n",
    "        'description': 'Deep model with feature interactions and multi-head attention'\n",
    "    }\n",
    "}\n",
    "\n",
    "# Optimizer configurations\n",
    "OPTIMIZER_CONFIGS = {\n",
    "    'Adam': {\n",
    "        'optimizer_fn': lambda lr: Adam(learning_rate=lr, beta_1=0.9, beta_2=0.999),\n",
    "        'lr': 0.001\n",
    "    },\n",
    "    'SGD': {\n",
    "        'optimizer_fn': lambda lr: SGD(learning_rate=lr, momentum=0.9),\n",
    "        'lr': 0.01\n",
    "    },\n",
    "    'RMSprop': {\n",
    "        'optimizer_fn': lambda lr: RMSprop(learning_rate=lr),\n",
    "        'lr': 0.001\n",
    "    }\n",
    "}\n",
    "\n",
    "# Loss function configurations\n",
    "LOSS_CONFIGS = {\n",
    "    'MAE': MeanAbsoluteError(),\n",
    "    'CorrelationLoss': correlation_loss,\n",
    "    'CorrelationAware': CorrelationAwareLoss(distribution_penalty=0.2)\n",
    "}\n",
    "\n",
    "print(\"Model configurations prepared:\")\n",
    "for name, config in MODEL_CONFIGS.items():\n",
    "    print(f\"  - {name}: {config['description']}\")\n",
    "\n",
    "print(\"\\nOptimizer configurations prepared:\")\n",
    "for name, config in OPTIMIZER_CONFIGS.items():\n",
    "    print(f\"  - {name}: Learning rate {config['lr']}\")\n",
    "\n",
    "print(\"\\nLoss function configurations prepared:\")\n",
    "for name in LOSS_CONFIGS.keys():\n",
    "    print(f\"  - {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a6d5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== STEP 3: TRAINING FUNCTIONS =====\n",
    "\n",
    "def create_callbacks(model_name, optimizer_name, loss_name, patience=15):\n",
    "    \"\"\"Create training callbacks\"\"\"\n",
    "    callbacks = [\n",
    "        EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=patience,\n",
    "            restore_best_weights=True,\n",
    "            verbose=1\n",
    "        ),\n",
    "        ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.5,\n",
    "            patience=8,\n",
    "            min_lr=1e-7,\n",
    "            verbose=1\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    # Create model checkpoint\n",
    "    checkpoint_path = f\"../../exports/{model_name}_{optimizer_name}_{loss_name}_best.keras\"\n",
    "    os.makedirs(os.path.dirname(checkpoint_path), exist_ok=True)\n",
    "    \n",
    "    callbacks.append(\n",
    "        ModelCheckpoint(\n",
    "            checkpoint_path,\n",
    "            monitor='val_loss',\n",
    "            save_best_only=True,\n",
    "            verbose=1\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    return callbacks, checkpoint_path\n",
    "\n",
    "def calculate_correlation(y_true, y_pred):\n",
    "    \"\"\"Calculate Pearson correlation coefficient\"\"\"\n",
    "    return np.corrcoef(y_true, y_pred.flatten())[0, 1] if len(y_true) > 1 else 0.0\n",
    "\n",
    "def evaluate_model(model, X_test, y_test, model_name):\n",
    "    \"\"\"Comprehensive model evaluation\"\"\"\n",
    "    print(f\"\\nEvaluating {model_name}...\")\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test, verbose=0)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    mae = np.mean(np.abs(y_test - y_pred.flatten()))\n",
    "    mse = np.mean((y_test - y_pred.flatten()) ** 2)\n",
    "    correlation = calculate_correlation(y_test, y_pred)\n",
    "    \n",
    "    # Calculate per-target performance\n",
    "    target_performance = {}\n",
    "    for target_val in np.unique(y_test):\n",
    "        mask = y_test == target_val\n",
    "        if mask.sum() > 0:\n",
    "            target_mae = np.mean(np.abs(y_test[mask] - y_pred.flatten()[mask]))\n",
    "            target_corr = calculate_correlation(y_test[mask], y_pred[mask]) if mask.sum() > 1 else 0.0\n",
    "            target_performance[target_val] = {\n",
    "                'count': mask.sum(),\n",
    "                'mae': target_mae,\n",
    "                'correlation': target_corr\n",
    "            }\n",
    "    \n",
    "    results = {\n",
    "        'model_name': model_name,\n",
    "        'mae': mae,\n",
    "        'mse': mse,\n",
    "        'correlation': correlation,\n",
    "        'target_performance': target_performance,\n",
    "        'predictions': y_pred.flatten(),\n",
    "        'targets': y_test\n",
    "    }\n",
    "    \n",
    "    print(f\"  MAE: {mae:.4f}\")\n",
    "    print(f\"  MSE: {mse:.4f}\")\n",
    "    print(f\"  Correlation: {correlation:.4f}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "def train_single_model(model_config, optimizer_config, loss_config, train_data, val_data, \n",
    "                      model_name, optimizer_name, loss_name, epochs=100, batch_size=512):\n",
    "    \"\"\"Train a single model configuration\"\"\"\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Training: {model_name} + {optimizer_name} + {loss_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    X_train, y_train = train_data\n",
    "    X_val, y_val = val_data\n",
    "    \n",
    "    # Create model\n",
    "    input_shape = (X_train.shape[1],)\n",
    "    model = model_config['model_fn'](input_shape)\n",
    "    \n",
    "    # Create optimizer\n",
    "    optimizer = optimizer_config['optimizer_fn'](optimizer_config['lr'])\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=loss_config,\n",
    "        metrics=['mae']\n",
    "    )\n",
    "    \n",
    "    print(f\"Model: {model.name}\")\n",
    "    print(f\"Input shape: {input_shape}\")\n",
    "    print(f\"Total parameters: {model.count_params():,}\")\n",
    "    \n",
    "    # Create callbacks\n",
    "    callbacks, checkpoint_path = create_callbacks(model_name, optimizer_name, loss_name)\n",
    "    \n",
    "    # Train model\n",
    "    start_time = datetime.now()\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_val, y_val),\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "    training_time = datetime.now() - start_time\n",
    "    \n",
    "    print(f\"Training completed in: {training_time}\")\n",
    "    print(f\"Model saved to: {checkpoint_path}\")\n",
    "    \n",
    "    # Load best model\n",
    "    model.load_weights(checkpoint_path)\n",
    "    \n",
    "    return model, history, checkpoint_path\n",
    "\n",
    "def plot_training_history(histories, title_prefix=\"Training History\"):\n",
    "    \"\"\"Plot training histories for comparison\"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Plot loss\n",
    "    for name, history in histories.items():\n",
    "        axes[0].plot(history.history['loss'], label=f'{name} - Train', alpha=0.7)\n",
    "        axes[0].plot(history.history['val_loss'], label=f'{name} - Val', alpha=0.7, linestyle='--')\n",
    "    \n",
    "    axes[0].set_title('Model Loss')\n",
    "    axes[0].set_xlabel('Epoch')\n",
    "    axes[0].set_ylabel('Loss')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True)\n",
    "    \n",
    "    # Plot MAE\n",
    "    for name, history in histories.items():\n",
    "        if 'mae' in history.history:\n",
    "            axes[1].plot(history.history['mae'], label=f'{name} - Train', alpha=0.7)\n",
    "        if 'val_mae' in history.history:\n",
    "            axes[1].plot(history.history['val_mae'], label=f'{name} - Val', alpha=0.7, linestyle='--')\n",
    "    \n",
    "    axes[1].set_title('Model MAE')\n",
    "    axes[1].set_xlabel('Epoch')\n",
    "    axes[1].set_ylabel('MAE')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True)\n",
    "    \n",
    "    plt.suptitle(title_prefix)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"Training functions defined and ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63328a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== STEP 4: PREPARE DATA FOR TRAINING =====\n",
    "\n",
    "# Get feature columns\n",
    "feature_cols = get_feature_columns(balanced_df)\n",
    "print(f\"Using {len(feature_cols)} features for training\")\n",
    "\n",
    "# Prepare data splits\n",
    "(X_train, y_train), (X_val, y_val), (X_test, y_test) = prepare_data(balanced_df, feature_cols)\n",
    "\n",
    "# Verify data shapes and distributions\n",
    "print(f\"\\nData shapes:\")\n",
    "print(f\"  X_train: {X_train.shape}, y_train: {y_train.shape}\")\n",
    "print(f\"  X_val: {X_val.shape}, y_val: {y_val.shape}\")\n",
    "print(f\"  X_test: {X_test.shape}, y_test: {y_test.shape}\")\n",
    "\n",
    "# Check target distribution in splits\n",
    "print(f\"\\nTarget distribution in training set:\")\n",
    "train_target_dist = pd.Series(y_train).value_counts(normalize=True).sort_index() * 100\n",
    "for target, pct in train_target_dist.items():\n",
    "    print(f\"  {target}: {pct:.1f}%\")\n",
    "\n",
    "print(f\"\\nTarget distribution in validation set:\")\n",
    "val_target_dist = pd.Series(y_val).value_counts(normalize=True).sort_index() * 100\n",
    "for target, pct in val_target_dist.items():\n",
    "    print(f\"  {target}: {pct:.1f}%\")\n",
    "\n",
    "print(f\"\\nTarget distribution in test set:\")\n",
    "test_target_dist = pd.Series(y_test).value_counts(normalize=True).sort_index() * 100\n",
    "for target, pct in test_target_dist.items():\n",
    "    print(f\"  {target}: {pct:.1f}%\")\n",
    "\n",
    "print(\"\\nData preparation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46cb4773",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== STEP 5: COMPREHENSIVE MODEL TRAINING =====\n",
    "\n",
    "# Training configuration\n",
    "TRAINING_CONFIG = {\n",
    "    'epochs': 100,\n",
    "    'batch_size': 512,\n",
    "    'models_to_train': ['BestModel', 'CorrelationModel'],  # Start with 2 models for faster execution\n",
    "    'optimizers_to_use': ['Adam', 'SGD'],  # Use 2 optimizers\n",
    "    'losses_to_use': ['MAE', 'CorrelationAware']  # Use 2 loss functions\n",
    "}\n",
    "\n",
    "print(f\"Training Configuration:\")\n",
    "print(f\"  Epochs: {TRAINING_CONFIG['epochs']}\")\n",
    "print(f\"  Batch size: {TRAINING_CONFIG['batch_size']}\")\n",
    "print(f\"  Models: {TRAINING_CONFIG['models_to_train']}\")\n",
    "print(f\"  Optimizers: {TRAINING_CONFIG['optimizers_to_use']}\")\n",
    "print(f\"  Loss functions: {TRAINING_CONFIG['losses_to_use']}\")\n",
    "\n",
    "# Storage for results\n",
    "training_results = []\n",
    "trained_models = {}\n",
    "training_histories = {}\n",
    "\n",
    "print(f\"\\nStarting comprehensive training...\")\n",
    "print(f\"Total combinations: {len(TRAINING_CONFIG['models_to_train']) * len(TRAINING_CONFIG['optimizers_to_use']) * len(TRAINING_CONFIG['losses_to_use'])}\")\n",
    "\n",
    "# Main training loop\n",
    "for model_name in TRAINING_CONFIG['models_to_train']:\n",
    "    for optimizer_name in TRAINING_CONFIG['optimizers_to_use']:\n",
    "        for loss_name in TRAINING_CONFIG['losses_to_use']:\n",
    "            \n",
    "            # Create unique identifier for this configuration\n",
    "            config_id = f\"{model_name}_{optimizer_name}_{loss_name}\"\n",
    "            \n",
    "            try:\n",
    "                # Get configurations\n",
    "                model_config = MODEL_CONFIGS[model_name]\n",
    "                optimizer_config = OPTIMIZER_CONFIGS[optimizer_name]\n",
    "                loss_config = LOSS_CONFIGS[loss_name]\n",
    "                \n",
    "                # Train model\n",
    "                model, history, checkpoint_path = train_single_model(\n",
    "                    model_config=model_config,\n",
    "                    optimizer_config=optimizer_config,\n",
    "                    loss_config=loss_config,\n",
    "                    train_data=(X_train, y_train),\n",
    "                    val_data=(X_val, y_val),\n",
    "                    model_name=model_name,\n",
    "                    optimizer_name=optimizer_name,\n",
    "                    loss_name=loss_name,\n",
    "                    epochs=TRAINING_CONFIG['epochs'],\n",
    "                    batch_size=TRAINING_CONFIG['batch_size']\n",
    "                )\n",
    "                \n",
    "                # Evaluate model\n",
    "                evaluation_results = evaluate_model(model, X_test, y_test, config_id)\n",
    "                \n",
    "                # Store results\n",
    "                result_record = {\n",
    "                    'config_id': config_id,\n",
    "                    'model_name': model_name,\n",
    "                    'optimizer_name': optimizer_name,\n",
    "                    'loss_name': loss_name,\n",
    "                    'checkpoint_path': checkpoint_path,\n",
    "                    'final_train_loss': history.history['loss'][-1],\n",
    "                    'final_val_loss': history.history['val_loss'][-1],\n",
    "                    'best_val_loss': min(history.history['val_loss']),\n",
    "                    'epochs_trained': len(history.history['loss']),\n",
    "                    'test_mae': evaluation_results['mae'],\n",
    "                    'test_mse': evaluation_results['mse'],\n",
    "                    'test_correlation': evaluation_results['correlation'],\n",
    "                    'timestamp': datetime.now().isoformat()\n",
    "                }\n",
    "                \n",
    "                training_results.append(result_record)\n",
    "                trained_models[config_id] = model\n",
    "                training_histories[config_id] = history\n",
    "                \n",
    "                print(f\"✅ {config_id} completed successfully\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"❌ {config_id} failed: {str(e)}\")\n",
    "                # Log the error but continue with next configuration\n",
    "                error_record = {\n",
    "                    'config_id': config_id,\n",
    "                    'model_name': model_name,\n",
    "                    'optimizer_name': optimizer_name,\n",
    "                    'loss_name': loss_name,\n",
    "                    'error': str(e),\n",
    "                    'timestamp': datetime.now().isoformat()\n",
    "                }\n",
    "                training_results.append(error_record)\n",
    "                continue\n",
    "\n",
    "print(f\"\\n🎉 Training completed!\")\n",
    "print(f\"Successfully trained: {len([r for r in training_results if 'error' not in r])} models\")\n",
    "print(f\"Failed: {len([r for r in training_results if 'error' in r])} models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601a7731",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== STEP 6: ANALYZE TRAINING RESULTS =====\n",
    "\n",
    "# Convert results to DataFrame for analysis\n",
    "results_df = pd.DataFrame([r for r in training_results if 'error' not in r])\n",
    "errors_df = pd.DataFrame([r for r in training_results if 'error' in r])\n",
    "\n",
    "if len(results_df) > 0:\n",
    "    print(\"\\n📊 TRAINING RESULTS SUMMARY\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Display results table\n",
    "    display_cols = ['config_id', 'test_mae', 'test_correlation', 'best_val_loss', 'epochs_trained']\n",
    "    results_display = results_df[display_cols].copy()\n",
    "    results_display = results_display.sort_values('test_correlation', ascending=False)\n",
    "    \n",
    "    print(\"\\nTop performers by correlation:\")\n",
    "    print(results_display.head(10).to_string(index=False))\n",
    "    \n",
    "    # Find best models\n",
    "    best_by_correlation = results_df.loc[results_df['test_correlation'].idxmax()]\n",
    "    best_by_mae = results_df.loc[results_df['test_mae'].idxmin()]\n",
    "    \n",
    "    print(f\"\\n🏆 BEST MODELS:\")\n",
    "    print(f\"Best Correlation: {best_by_correlation['config_id']} (r={best_by_correlation['test_correlation']:.4f})\")\n",
    "    print(f\"Best MAE: {best_by_mae['config_id']} (MAE={best_by_mae['test_mae']:.4f})\")\n",
    "    \n",
    "    # Model architecture comparison\n",
    "    print(f\"\\n📈 MODEL ARCHITECTURE COMPARISON:\")\n",
    "    model_comparison = results_df.groupby('model_name').agg({\n",
    "        'test_correlation': ['mean', 'std', 'max'],\n",
    "        'test_mae': ['mean', 'std', 'min']\n",
    "    }).round(4)\n",
    "    print(model_comparison)\n",
    "    \n",
    "    # Optimizer comparison\n",
    "    print(f\"\\n⚙️ OPTIMIZER COMPARISON:\")\n",
    "    optimizer_comparison = results_df.groupby('optimizer_name').agg({\n",
    "        'test_correlation': ['mean', 'std', 'max'],\n",
    "        'test_mae': ['mean', 'std', 'min']\n",
    "    }).round(4)\n",
    "    print(optimizer_comparison)\n",
    "    \n",
    "    # Loss function comparison\n",
    "    print(f\"\\n📉 LOSS FUNCTION COMPARISON:\")\n",
    "    loss_comparison = results_df.groupby('loss_name').agg({\n",
    "        'test_correlation': ['mean', 'std', 'max'],\n",
    "        'test_mae': ['mean', 'std', 'min']\n",
    "    }).round(4)\n",
    "    print(loss_comparison)\n",
    "    \n",
    "else:\n",
    "    print(\"❌ No successful training results to analyze\")\n",
    "\n",
    "if len(errors_df) > 0:\n",
    "    print(f\"\\n⚠️ TRAINING ERRORS:\")\n",
    "    for _, error in errors_df.iterrows():\n",
    "        print(f\"  {error['config_id']}: {error['error']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f20707",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== STEP 7: COMPREHENSIVE VISUALIZATIONS =====\n",
    "\n",
    "def plot_model_comparison(results_df):\n",
    "    \"\"\"Create comprehensive comparison plots\"\"\"\n",
    "    if len(results_df) == 0:\n",
    "        print(\"No results to plot\")\n",
    "        return\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
    "    fig.suptitle('Comprehensive Model Training Results', fontsize=16)\n",
    "    \n",
    "    # 1. Correlation vs MAE scatter plot\n",
    "    scatter = axes[0, 0].scatter(results_df['test_mae'], results_df['test_correlation'], \n",
    "                                c=range(len(results_df)), cmap='viridis', alpha=0.7, s=100)\n",
    "    axes[0, 0].set_xlabel('Test MAE')\n",
    "    axes[0, 0].set_ylabel('Test Correlation')\n",
    "    axes[0, 0].set_title('Correlation vs MAE Performance')\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add annotations for best models\n",
    "    for idx, row in results_df.iterrows():\n",
    "        axes[0, 0].annotate(row['config_id'], (row['test_mae'], row['test_correlation']),\n",
    "                           xytext=(5, 5), textcoords='offset points', fontsize=8, alpha=0.7)\n",
    "    \n",
    "    # 2. Model architecture comparison\n",
    "    model_means = results_df.groupby('model_name')['test_correlation'].mean()\n",
    "    model_stds = results_df.groupby('model_name')['test_correlation'].std()\n",
    "    axes[0, 1].bar(model_means.index, model_means.values, yerr=model_stds.values, \n",
    "                   capsize=5, alpha=0.7, color=['skyblue', 'lightcoral', 'lightgreen'])\n",
    "    axes[0, 1].set_title('Average Correlation by Model Architecture')\n",
    "    axes[0, 1].set_ylabel('Test Correlation')\n",
    "    axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # 3. Optimizer comparison\n",
    "    opt_means = results_df.groupby('optimizer_name')['test_correlation'].mean()\n",
    "    opt_stds = results_df.groupby('optimizer_name')['test_correlation'].std()\n",
    "    axes[0, 2].bar(opt_means.index, opt_means.values, yerr=opt_stds.values, \n",
    "                   capsize=5, alpha=0.7, color=['orange', 'purple', 'brown'])\n",
    "    axes[0, 2].set_title('Average Correlation by Optimizer')\n",
    "    axes[0, 2].set_ylabel('Test Correlation')\n",
    "    \n",
    "    # 4. Loss function comparison\n",
    "    loss_means = results_df.groupby('loss_name')['test_correlation'].mean()\n",
    "    loss_stds = results_df.groupby('loss_name')['test_correlation'].std()\n",
    "    axes[1, 0].bar(loss_means.index, loss_means.values, yerr=loss_stds.values, \n",
    "                   capsize=5, alpha=0.7, color=['gold', 'pink'])\n",
    "    axes[1, 0].set_title('Average Correlation by Loss Function')\n",
    "    axes[1, 0].set_ylabel('Test Correlation')\n",
    "    axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # 5. Training efficiency (epochs vs performance)\n",
    "    scatter2 = axes[1, 1].scatter(results_df['epochs_trained'], results_df['test_correlation'],\n",
    "                                 c=results_df['test_mae'], cmap='viridis_r', alpha=0.7, s=100)\n",
    "    axes[1, 1].set_xlabel('Epochs Trained')\n",
    "    axes[1, 1].set_ylabel('Test Correlation')\n",
    "    axes[1, 1].set_title('Training Efficiency (Color = MAE)')\n",
    "    plt.colorbar(scatter2, ax=axes[1, 1], label='Test MAE')\n",
    "    \n",
    "    # 6. Validation vs Test performance\n",
    "    axes[1, 2].scatter(results_df['best_val_loss'], results_df['test_mae'], alpha=0.7, s=100)\n",
    "    axes[1, 2].set_xlabel('Best Validation Loss')\n",
    "    axes[1, 2].set_ylabel('Test MAE')\n",
    "    axes[1, 2].set_title('Validation vs Test Performance')\n",
    "    axes[1, 2].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_training_histories_summary(training_histories):\n",
    "    \"\"\"Plot training histories for all models\"\"\"\n",
    "    if len(training_histories) == 0:\n",
    "        print(\"No training histories to plot\")\n",
    "        return\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Plot all training and validation losses\n",
    "    for config_id, history in training_histories.items():\n",
    "        epochs = range(1, len(history.history['loss']) + 1)\n",
    "        axes[0].plot(epochs, history.history['loss'], label=f'{config_id} (train)', alpha=0.7)\n",
    "        axes[0].plot(epochs, history.history['val_loss'], label=f'{config_id} (val)', \n",
    "                    alpha=0.7, linestyle='--')\n",
    "    \n",
    "    axes[0].set_title('All Training Histories - Loss')\n",
    "    axes[0].set_xlabel('Epoch')\n",
    "    axes[0].set_ylabel('Loss')\n",
    "    axes[0].legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot final convergence comparison\n",
    "    final_losses = []\n",
    "    config_names = []\n",
    "    for config_id, history in training_histories.items():\n",
    "        final_losses.append(history.history['val_loss'][-1])\n",
    "        config_names.append(config_id)\n",
    "    \n",
    "    axes[1].bar(range(len(final_losses)), final_losses, alpha=0.7)\n",
    "    axes[1].set_title('Final Validation Loss Comparison')\n",
    "    axes[1].set_xlabel('Model Configuration')\n",
    "    axes[1].set_ylabel('Final Validation Loss')\n",
    "    axes[1].set_xticks(range(len(config_names)))\n",
    "    axes[1].set_xticklabels(config_names, rotation=45, ha='right')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def analyze_predictions(trained_models, X_test, y_test, results_df):\n",
    "    \"\"\"Analyze predictions from the best models\"\"\"\n",
    "    if len(trained_models) == 0 or len(results_df) == 0:\n",
    "        print(\"No models to analyze\")\n",
    "        return\n",
    "    \n",
    "    # Get best model by correlation\n",
    "    best_config_id = results_df.loc[results_df['test_correlation'].idxmax(), 'config_id']\n",
    "    best_model = trained_models[best_config_id]\n",
    "    \n",
    "    print(f\"\\n🔍 ANALYZING BEST MODEL: {best_config_id}\")\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = best_model.predict(X_test, verbose=0).flatten()\n",
    "    \n",
    "    # Create detailed analysis plots\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    fig.suptitle(f'Detailed Analysis: {best_config_id}', fontsize=16)\n",
    "    \n",
    "    # 1. Prediction vs True scatter plot\n",
    "    axes[0, 0].scatter(y_test, y_pred, alpha=0.6, s=50)\n",
    "    axes[0, 0].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "    axes[0, 0].set_xlabel('True Values')\n",
    "    axes[0, 0].set_ylabel('Predicted Values')\n",
    "    axes[0, 0].set_title('Predictions vs True Values')\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add correlation info\n",
    "    correlation = np.corrcoef(y_test, y_pred)[0, 1]\n",
    "    axes[0, 0].text(0.05, 0.95, f'r = {correlation:.4f}', transform=axes[0, 0].transAxes,\n",
    "                    bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))\n",
    "    \n",
    "    # 2. Residuals plot\n",
    "    residuals = y_test - y_pred\n",
    "    axes[0, 1].scatter(y_pred, residuals, alpha=0.6, s=50)\n",
    "    axes[0, 1].axhline(y=0, color='r', linestyle='--')\n",
    "    axes[0, 1].set_xlabel('Predicted Values')\n",
    "    axes[0, 1].set_ylabel('Residuals')\n",
    "    axes[0, 1].set_title('Residuals Plot')\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Distribution comparison\n",
    "    axes[1, 0].hist(y_test, bins=20, alpha=0.7, label='True', density=True)\n",
    "    axes[1, 0].hist(y_pred, bins=20, alpha=0.7, label='Predicted', density=True)\n",
    "    axes[1, 0].set_xlabel('Value')\n",
    "    axes[1, 0].set_ylabel('Density')\n",
    "    axes[1, 0].set_title('Distribution Comparison')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Per-target performance\n",
    "    target_performance = []\n",
    "    target_values = np.unique(y_test)\n",
    "    \n",
    "    for target_val in target_values:\n",
    "        mask = y_test == target_val\n",
    "        if mask.sum() > 0:\n",
    "            target_mae = np.mean(np.abs(y_test[mask] - y_pred[mask]))\n",
    "            target_performance.append(target_mae)\n",
    "    \n",
    "    axes[1, 1].bar(target_values, target_performance, alpha=0.7)\n",
    "    axes[1, 1].set_xlabel('Target Value')\n",
    "    axes[1, 1].set_ylabel('MAE')\n",
    "    axes[1, 1].set_title('Per-Target Performance')\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print detailed performance by target\n",
    "    print(f\"\\n📋 DETAILED PERFORMANCE BY TARGET:\")\n",
    "    for target_val in target_values:\n",
    "        mask = y_test == target_val\n",
    "        if mask.sum() > 0:\n",
    "            target_mae = np.mean(np.abs(y_test[mask] - y_pred[mask]))\n",
    "            target_corr = np.corrcoef(y_test[mask], y_pred[mask])[0, 1] if mask.sum() > 1 else 0.0\n",
    "            print(f\"  Target {target_val}: {mask.sum():,} samples, MAE={target_mae:.4f}, r={target_corr:.4f}\")\n",
    "\n",
    "# Generate all visualizations if we have results\n",
    "if len(results_df) > 0:\n",
    "    print(\"\\n📊 Generating comprehensive visualizations...\")\n",
    "    \n",
    "    # Main comparison plots\n",
    "    plot_model_comparison(results_df)\n",
    "    \n",
    "    # Training histories\n",
    "    plot_training_histories_summary(training_histories)\n",
    "    \n",
    "    # Detailed prediction analysis\n",
    "    analyze_predictions(trained_models, X_test, y_test, results_df)\n",
    "    \n",
    "    print(\"\\n✅ All visualizations completed!\")\n",
    "else:\n",
    "    print(\"\\n❌ No results available for visualization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90fad7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== STEP 8: SAVE RESULTS AND EXPORT MODELS =====\n",
    "\n",
    "def save_training_results(results_df, training_histories, save_dir=\"../../exports\"):\n",
    "    \"\"\"Save training results and metadata\"\"\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    # Save results CSV\n",
    "    results_path = os.path.join(save_dir, \"balanced_training_results.csv\")\n",
    "    results_df.to_csv(results_path, index=False)\n",
    "    print(f\"Training results saved to: {results_path}\")\n",
    "    \n",
    "    # Save training histories\n",
    "    histories_path = os.path.join(save_dir, \"training_histories.json\")\n",
    "    histories_data = {}\n",
    "    for config_id, history in training_histories.items():\n",
    "        histories_data[config_id] = {\n",
    "            'loss': history.history['loss'],\n",
    "            'val_loss': history.history['val_loss'],\n",
    "            'mae': history.history.get('mae', []),\n",
    "            'val_mae': history.history.get('val_mae', [])\n",
    "        }\n",
    "    \n",
    "    with open(histories_path, 'w') as f:\n",
    "        json.dump(histories_data, f, indent=2)\n",
    "    print(f\"Training histories saved to: {histories_path}\")\n",
    "    \n",
    "    # Save training configuration\n",
    "    config_path = os.path.join(save_dir, \"training_config.json\")\n",
    "    config_data = {\n",
    "        'training_config': TRAINING_CONFIG,\n",
    "        'model_configs': {name: config['description'] for name, config in MODEL_CONFIGS.items()},\n",
    "        'data_info': {\n",
    "            'balanced_dataset_shape': balanced_df.shape,\n",
    "            'train_samples': len(X_train),\n",
    "            'val_samples': len(X_val),\n",
    "            'test_samples': len(X_test),\n",
    "            'num_features': len(feature_cols)\n",
    "        },\n",
    "        'timestamp': datetime.now().isoformat()\n",
    "    }\n",
    "    \n",
    "    with open(config_path, 'w') as f:\n",
    "        json.dump(config_data, f, indent=2)\n",
    "    print(f\"Training configuration saved to: {config_path}\")\n",
    "\n",
    "def create_model_summary_report(results_df, save_dir=\"../../exports\"):\n",
    "    \"\"\"Create a comprehensive summary report\"\"\"\n",
    "    if len(results_df) == 0:\n",
    "        print(\"No results to summarize\")\n",
    "        return\n",
    "    \n",
    "    report_path = os.path.join(save_dir, \"model_training_report.md\")\n",
    "    \n",
    "    with open(report_path, 'w') as f:\n",
    "        f.write(\"# Balanced Dataset Model Training Report\\n\\n\")\n",
    "        f.write(f\"Generated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\\n\")\n",
    "        \n",
    "        f.write(\"## Training Overview\\n\\n\")\n",
    "        f.write(f\"- **Total models trained**: {len(results_df)}\\n\")\n",
    "        f.write(f\"- **Dataset**: Balanced train.parquet with equal target distribution\\n\")\n",
    "        f.write(f\"- **Training samples**: {len(X_train):,}\\n\")\n",
    "        f.write(f\"- **Validation samples**: {len(X_val):,}\\n\")\n",
    "        f.write(f\"- **Test samples**: {len(X_test):,}\\n\")\n",
    "        f.write(f\"- **Features**: {len(feature_cols)}\\n\\n\")\n",
    "        \n",
    "        f.write(\"## Best Models\\n\\n\")\n",
    "        best_corr = results_df.loc[results_df['test_correlation'].idxmax()]\n",
    "        best_mae = results_df.loc[results_df['test_mae'].idxmin()]\n",
    "        \n",
    "        f.write(f\"### Best by Correlation\\n\")\n",
    "        f.write(f\"- **Model**: {best_corr['config_id']}\\n\")\n",
    "        f.write(f\"- **Correlation**: {best_corr['test_correlation']:.4f}\\n\")\n",
    "        f.write(f\"- **MAE**: {best_corr['test_mae']:.4f}\\n\")\n",
    "        f.write(f\"- **Epochs**: {best_corr['epochs_trained']}\\n\\n\")\n",
    "        \n",
    "        f.write(f\"### Best by MAE\\n\")\n",
    "        f.write(f\"- **Model**: {best_mae['config_id']}\\n\")\n",
    "        f.write(f\"- **MAE**: {best_mae['test_mae']:.4f}\\n\")\n",
    "        f.write(f\"- **Correlation**: {best_mae['test_correlation']:.4f}\\n\")\n",
    "        f.write(f\"- **Epochs**: {best_mae['epochs_trained']}\\n\\n\")\n",
    "        \n",
    "        f.write(\"## All Results\\n\\n\")\n",
    "        f.write(\"| Config ID | Model | Optimizer | Loss | Test Correlation | Test MAE | Epochs |\\n\")\n",
    "        f.write(\"|-----------|-------|-----------|------|------------------|----------|--------|\\n\")\n",
    "        \n",
    "        for _, row in results_df.sort_values('test_correlation', ascending=False).iterrows():\n",
    "            f.write(f\"| {row['config_id']} | {row['model_name']} | {row['optimizer_name']} | \"\n",
    "                   f\"{row['loss_name']} | {row['test_correlation']:.4f} | {row['test_mae']:.4f} | \"\n",
    "                   f\"{row['epochs_trained']} |\\n\")\n",
    "        \n",
    "        f.write(\"\\n## Model Architecture Comparison\\n\\n\")\n",
    "        model_comparison = results_df.groupby('model_name').agg({\n",
    "            'test_correlation': ['mean', 'std', 'max'],\n",
    "            'test_mae': ['mean', 'std', 'min']\n",
    "        }).round(4)\n",
    "        \n",
    "        f.write(\"| Model | Avg Correlation | Std Correlation | Max Correlation | Avg MAE | Std MAE | Min MAE |\\n\")\n",
    "        f.write(\"|-------|-----------------|-----------------|-----------------|---------|---------|---------|\\n\")\n",
    "        \n",
    "        for model_name in model_comparison.index:\n",
    "            row = model_comparison.loc[model_name]\n",
    "            f.write(f\"| {model_name} | {row[('test_correlation', 'mean')]} | \"\n",
    "                   f\"{row[('test_correlation', 'std')]} | {row[('test_correlation', 'max')]} | \"\n",
    "                   f\"{row[('test_mae', 'mean')]} | {row[('test_mae', 'std')]} | \"\n",
    "                   f\"{row[('test_mae', 'min')]} |\\n\")\n",
    "    \n",
    "    print(f\"Summary report saved to: {report_path}\")\n",
    "\n",
    "# Save all results if we have them\n",
    "if len(results_df) > 0:\n",
    "    print(\"\\n💾 Saving training results and creating reports...\")\n",
    "    \n",
    "    # Save results and metadata\n",
    "    save_training_results(results_df, training_histories)\n",
    "    \n",
    "    # Create summary report\n",
    "    create_model_summary_report(results_df)\n",
    "    \n",
    "    print(\"\\n✅ All results saved successfully!\")\n",
    "    \n",
    "    # Print final summary\n",
    "    print(f\"\\n🎯 FINAL TRAINING SUMMARY:\")\n",
    "    print(f\"{'='*50}\")\n",
    "    print(f\"Total successful models: {len(results_df)}\")\n",
    "    print(f\"Best correlation: {results_df['test_correlation'].max():.4f}\")\n",
    "    print(f\"Best MAE: {results_df['test_mae'].min():.4f}\")\n",
    "    print(f\"Average correlation: {results_df['test_correlation'].mean():.4f}\")\n",
    "    print(f\"Average MAE: {results_df['test_mae'].mean():.4f}\")\n",
    "    print(f\"\\nAll models and results saved to: ../../exports/\")\n",
    "    print(f\"Balanced dataset saved to: ../../data/train_balanced.parquet\")\n",
    "    \n",
    "else:\n",
    "    print(\"\\n❌ No successful training results to save\")\n",
    "\n",
    "print(\"\\n🏁 Comprehensive training notebook execution completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d65aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the balanced dataset\n",
    "print(\"=== LOADING BALANCED DATASET ===\")\n",
    "\n",
    "# Load balanced training data\n",
    "balanced_data_path = \"../../data/train_balanced.parquet\"\n",
    "if os.path.exists(balanced_data_path):\n",
    "    train_df = pd.read_parquet(balanced_data_path)\n",
    "    print(f\"✅ Balanced dataset loaded: {train_df.shape}\")\n",
    "    \n",
    "    # Verify the distribution is balanced\n",
    "    target_dist = train_df['target'].value_counts().sort_index()\n",
    "    target_pct = (target_dist / len(train_df) * 100).round(1)\n",
    "    \n",
    "    print(\"\\nTarget distribution verification:\")\n",
    "    for target, count in target_dist.items():\n",
    "        pct = target_pct[target]\n",
    "        print(f\"  {target}: {count:,} samples ({pct}%)\")\n",
    "    \n",
    "    # Quick visualization\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    target_pct.plot(kind='bar', color='skyblue', alpha=0.7)\n",
    "    plt.title('Balanced Dataset - Target Distribution')\n",
    "    plt.xlabel('Target Value')\n",
    "    plt.ylabel('Percentage (%)')\n",
    "    plt.xticks(rotation=0)\n",
    "    for i, v in enumerate(target_pct.values):\n",
    "        plt.text(i, v + 0.5, f'{v}%', ha='center', va='bottom')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "else:\n",
    "    print(f\"❌ Balanced dataset not found at {balanced_data_path}\")\n",
    "    print(\"Please run the create_dataset.ipynb notebook first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ddb1afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup data handler and prepare features\n",
    "print(\"\\n=== PREPARING DATA FOR TRAINING ===\")\n",
    "\n",
    "if 'train_df' in locals():\n",
    "    # Create a custom data handler for our balanced dataset\n",
    "    class BalancedDataHandler:\n",
    "        def __init__(self, df):\n",
    "            self.df = df\n",
    "            \n",
    "        def get_feature_columns(self, feature_set='all'):\n",
    "            \"\"\"Get feature columns based on the feature set\"\"\"\n",
    "            # Exclude non-feature columns\n",
    "            exclude_cols = ['target', 'era', 'data_type'] if 'era' in self.df.columns else ['target']\n",
    "            feature_cols = [col for col in self.df.columns if col not in exclude_cols]\n",
    "            \n",
    "            if feature_set == 'small':\n",
    "                # Use first 50 features\n",
    "                return feature_cols[:50]\n",
    "            elif feature_set == 'medium':\n",
    "                # Use first 100 features\n",
    "                return feature_cols[:100]\n",
    "            else:  # 'all'\n",
    "                return feature_cols\n",
    "        \n",
    "        def prepare_data(self, feature_set='all', validation_split=0.2):\n",
    "            \"\"\"Prepare training and validation data\"\"\"\n",
    "            feature_cols = self.get_feature_columns(feature_set)\n",
    "            \n",
    "            # Get features and target\n",
    "            X = self.df[feature_cols].values.astype(np.float32)\n",
    "            y = self.df['target'].values.astype(np.float32)\n",
    "            \n",
    "            print(f\"Features shape: {X.shape}\")\n",
    "            print(f\"Target shape: {y.shape}\")\n",
    "            print(f\"Feature set: {feature_set} ({len(feature_cols)} features)\")\n",
    "            \n",
    "            # Split data for validation\n",
    "            if validation_split > 0:\n",
    "                split_idx = int(len(X) * (1 - validation_split))\n",
    "                \n",
    "                # Shuffle indices to ensure random split while maintaining balance\n",
    "                indices = np.random.permutation(len(X))\n",
    "                train_indices = indices[:split_idx]\n",
    "                val_indices = indices[split_idx:]\n",
    "                \n",
    "                X_train, X_val = X[train_indices], X[val_indices]\n",
    "                y_train, y_val = y[train_indices], y[val_indices]\n",
    "                \n",
    "                print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "                print(f\"Validation set: {X_val.shape[0]} samples\")\n",
    "                \n",
    "                # Verify both sets maintain target balance\n",
    "                print(\"\\nTraining set target distribution:\")\n",
    "                train_target_dist = pd.Series(y_train).value_counts().sort_index()\n",
    "                for target, count in train_target_dist.items():\n",
    "                    pct = count / len(y_train) * 100\n",
    "                    print(f\"  {target}: {count} ({pct:.1f}%)\")\n",
    "                \n",
    "                return X_train, X_val, y_train, y_val, feature_cols\n",
    "            else:\n",
    "                return X, None, y, None, feature_cols\n",
    "    \n",
    "    # Initialize data handler\n",
    "    data_handler = BalancedDataHandler(train_df)\n",
    "    \n",
    "    # Prepare data with different feature sets\n",
    "    feature_set = 'all'  # Change to 'small' or 'medium' if needed\n",
    "    X_train, X_val, y_train, y_val, feature_cols = data_handler.prepare_data(\n",
    "        feature_set=feature_set, \n",
    "        validation_split=0.2\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n✅ Data preparation complete!\")\n",
    "else:\n",
    "    print(\"❌ Training data not loaded. Please run the previous cell.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59c2c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "print(\"\\n=== TRAINING CONFIGURATION ===\")\n",
    "\n",
    "# Training parameters\n",
    "TRAINING_CONFIG = {\n",
    "    'epochs': 100,\n",
    "    'batch_size': 512,\n",
    "    'learning_rate': 0.001,\n",
    "    'validation_split': 0.2,\n",
    "    'early_stopping_patience': 15,\n",
    "    'reduce_lr_patience': 5,\n",
    "    'reduce_lr_factor': 0.5,\n",
    "    'min_lr': 1e-6\n",
    "}\n",
    "\n",
    "# Models to train\n",
    "MODELS_TO_TRAIN = {\n",
    "    'BestModel': best_model,\n",
    "    'CorrelationModel': correlation_model,\n",
    "    'DeepModel': deep_model\n",
    "}\n",
    "\n",
    "# Optimizers to test\n",
    "OPTIMIZERS = {\n",
    "    'Adam': Adam(learning_rate=TRAINING_CONFIG['learning_rate']),\n",
    "    'SGD': SGD(learning_rate=TRAINING_CONFIG['learning_rate'], momentum=0.9),\n",
    "    'RMSprop': RMSprop(learning_rate=TRAINING_CONFIG['learning_rate'])\n",
    "}\n",
    "\n",
    "print(\"Training configuration:\")\n",
    "for key, value in TRAINING_CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "print(f\"\\nModels to train: {list(MODELS_TO_TRAIN.keys())}\")\n",
    "print(f\"Optimizers to test: {list(OPTIMIZERS.keys())}\")\n",
    "print(f\"Total combinations: {len(MODELS_TO_TRAIN)} × {len(OPTIMIZERS)} = {len(MODELS_TO_TRAIN) * len(OPTIMIZERS)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5022cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training functions\n",
    "def create_callbacks(model_name, optimizer_name):\n",
    "    \"\"\"Create training callbacks\"\"\"\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    # Create directories\n",
    "    log_dir = f\"../../logs/{model_name}_{optimizer_name}_{timestamp}\"\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "    os.makedirs(\"../../exports\", exist_ok=True)\n",
    "    \n",
    "    callbacks = [\n",
    "        EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=TRAINING_CONFIG['early_stopping_patience'],\n",
    "            restore_best_weights=True,\n",
    "            verbose=1\n",
    "        ),\n",
    "        ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=TRAINING_CONFIG['reduce_lr_factor'],\n",
    "            patience=TRAINING_CONFIG['reduce_lr_patience'],\n",
    "            min_lr=TRAINING_CONFIG['min_lr'],\n",
    "            verbose=1\n",
    "        ),\n",
    "        ModelCheckpoint(\n",
    "            filepath=f\"../../exports/{model_name}_{optimizer_name}_balanced.keras\",\n",
    "            monitor='val_loss',\n",
    "            save_best_only=True,\n",
    "            verbose=1\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    return callbacks, log_dir\n",
    "\n",
    "def train_single_model(model_fn, model_name, optimizer_name, optimizer):\n",
    "    \"\"\"Train a single model with given optimizer\"\"\"\n",
    "    print(f\"\\n🚀 Training {model_name} with {optimizer_name}...\")\n",
    "    \n",
    "    try:\n",
    "        # Create model\n",
    "        input_shape = (len(feature_cols),)\n",
    "        model = model_fn(input_shape)\n",
    "        \n",
    "        # Compile model\n",
    "        model.compile(\n",
    "            optimizer=optimizer,\n",
    "            loss='mae',\n",
    "            metrics=['mae', 'mse']\n",
    "        )\n",
    "        \n",
    "        print(f\"Model architecture: {model.count_params():,} parameters\")\n",
    "        \n",
    "        # Create callbacks\n",
    "        callbacks, log_dir = create_callbacks(model_name, optimizer_name)\n",
    "        \n",
    "        # Train model\n",
    "        history = model.fit(\n",
    "            X_train, y_train,\n",
    "            validation_data=(X_val, y_val),\n",
    "            epochs=TRAINING_CONFIG['epochs'],\n",
    "            batch_size=TRAINING_CONFIG['batch_size'],\n",
    "            callbacks=callbacks,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        # Save training history\n",
    "        history_df = pd.DataFrame(history.history)\n",
    "        history_df.to_csv(f\"{log_dir}/training_history.csv\", index=False)\n",
    "        \n",
    "        # Evaluate model\n",
    "        train_loss = model.evaluate(X_train, y_train, verbose=0)\n",
    "        val_loss = model.evaluate(X_val, y_val, verbose=0)\n",
    "        \n",
    "        # Calculate predictions for analysis\n",
    "        train_pred = model.predict(X_train, verbose=0)\n",
    "        val_pred = model.predict(X_val, verbose=0)\n",
    "        \n",
    "        # Calculate correlations\n",
    "        train_corr = np.corrcoef(y_train, train_pred.flatten())[0, 1]\n",
    "        val_corr = np.corrcoef(y_val, val_pred.flatten())[0, 1]\n",
    "        \n",
    "        results = {\n",
    "            'model': model_name,\n",
    "            'optimizer': optimizer_name,\n",
    "            'train_loss': train_loss[0],\n",
    "            'val_loss': val_loss[0],\n",
    "            'train_mae': train_loss[1],\n",
    "            'val_mae': val_loss[1],\n",
    "            'train_correlation': train_corr,\n",
    "            'val_correlation': val_corr,\n",
    "            'epochs_trained': len(history.history['loss']),\n",
    "            'log_dir': log_dir\n",
    "        }\n",
    "        \n",
    "        print(f\"✅ {model_name} + {optimizer_name} completed!\")\n",
    "        print(f\"   Final val_loss: {val_loss[0]:.4f}\")\n",
    "        print(f\"   Val correlation: {val_corr:.4f}\")\n",
    "        print(f\"   Epochs trained: {len(history.history['loss'])}\")\n",
    "        \n",
    "        return results, history, model\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error training {model_name} with {optimizer_name}: {str(e)}\")\n",
    "        return None, None, None\n",
    "\n",
    "print(\"✅ Training functions defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f54fda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute training\n",
    "print(\"\\n=== STARTING TRAINING ===\")\n",
    "print(f\"Training {len(MODELS_TO_TRAIN) * len(OPTIMIZERS)} model-optimizer combinations...\")\n",
    "\n",
    "all_results = []\n",
    "all_histories = {}\n",
    "all_models = {}\n",
    "\n",
    "for model_name, model_fn in MODELS_TO_TRAIN.items():\n",
    "    for optimizer_name, optimizer in OPTIMIZERS.items():\n",
    "        combination_name = f\"{model_name}_{optimizer_name}\"\n",
    "        \n",
    "        # Train the model\n",
    "        results, history, model = train_single_model(\n",
    "            model_fn, model_name, optimizer_name, optimizer\n",
    "        )\n",
    "        \n",
    "        if results is not None:\n",
    "            all_results.append(results)\n",
    "            all_histories[combination_name] = history\n",
    "            all_models[combination_name] = model\n",
    "        \n",
    "        print(f\"\\nProgress: {len(all_results)}/{len(MODELS_TO_TRAIN) * len(OPTIMIZERS)} completed\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "print(f\"\\n🎉 Training completed! {len(all_results)} models trained successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b00699",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze results\n",
    "print(\"\\n=== TRAINING RESULTS ANALYSIS ===\")\n",
    "\n",
    "if all_results:\n",
    "    # Create results DataFrame\n",
    "    results_df = pd.DataFrame(all_results)\n",
    "    print(\"\\nTraining Results Summary:\")\n",
    "    print(results_df[['model', 'optimizer', 'val_loss', 'val_correlation', 'epochs_trained']].to_string(index=False))\n",
    "    \n",
    "    # Find best results\n",
    "    best_by_loss = results_df.loc[results_df['val_loss'].idxmin()]\n",
    "    best_by_corr = results_df.loc[results_df['val_correlation'].idxmax()]\n",
    "    \n",
    "    print(f\"\\n🏆 BEST RESULTS:\")\n",
    "    print(f\"Best by validation loss: {best_by_loss['model']} + {best_by_loss['optimizer']} (loss: {best_by_loss['val_loss']:.4f})\")\n",
    "    print(f\"Best by correlation: {best_by_corr['model']} + {best_by_corr['optimizer']} (corr: {best_by_corr['val_correlation']:.4f})\")\n",
    "    \n",
    "    # Create comparison plots\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    \n",
    "    # 1. Validation Loss by Model-Optimizer combination\n",
    "    ax1 = axes[0, 0]\n",
    "    results_pivot = results_df.pivot(index='model', columns='optimizer', values='val_loss')\n",
    "    sns.heatmap(results_pivot, annot=True, fmt='.4f', cmap='YlOrRd_r', ax=ax1)\n",
    "    ax1.set_title('Validation Loss by Model-Optimizer')\n",
    "    \n",
    "    # 2. Validation Correlation by Model-Optimizer combination\n",
    "    ax2 = axes[0, 1]\n",
    "    corr_pivot = results_df.pivot(index='model', columns='optimizer', values='val_correlation')\n",
    "    sns.heatmap(corr_pivot, annot=True, fmt='.4f', cmap='YlGn', ax=ax2)\n",
    "    ax2.set_title('Validation Correlation by Model-Optimizer')\n",
    "    \n",
    "    # 3. Training epochs by combination\n",
    "    ax3 = axes[1, 0]\n",
    "    epoch_pivot = results_df.pivot(index='model', columns='optimizer', values='epochs_trained')\n",
    "    sns.heatmap(epoch_pivot, annot=True, fmt='d', cmap='YlOrRd', ax=ax3)\n",
    "    ax3.set_title('Epochs Trained by Model-Optimizer')\n",
    "    \n",
    "    # 4. Loss vs Correlation scatter plot\n",
    "    ax4 = axes[1, 1]\n",
    "    for model in results_df['model'].unique():\n",
    "        model_data = results_df[results_df['model'] == model]\n",
    "        ax4.scatter(model_data['val_loss'], model_data['val_correlation'], \n",
    "                   label=model, s=100, alpha=0.7)\n",
    "        \n",
    "        # Add optimizer labels\n",
    "        for _, row in model_data.iterrows():\n",
    "            ax4.annotate(row['optimizer'], \n",
    "                        (row['val_loss'], row['val_correlation']),\n",
    "                        xytext=(5, 5), textcoords='offset points', \n",
    "                        fontsize=8, alpha=0.7)\n",
    "    \n",
    "    ax4.set_xlabel('Validation Loss')\n",
    "    ax4.set_ylabel('Validation Correlation')\n",
    "    ax4.set_title('Loss vs Correlation Trade-off')\n",
    "    ax4.legend()\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Save results\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    results_df.to_csv(f\"../../exports/balanced_training_results_{timestamp}.csv\", index=False)\n",
    "    print(f\"\\n📊 Results saved to: exports/balanced_training_results_{timestamp}.csv\")\n",
    "    \n",
    "else:\n",
    "    print(\"❌ No successful training results to analyze.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bedea7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves for the best performing models\n",
    "print(\"\\n=== TRAINING CURVES ANALYSIS ===\")\n",
    "\n",
    "if all_results and all_histories:\n",
    "    # Get the best model by validation loss\n",
    "    best_model_info = results_df.loc[results_df['val_loss'].idxmin()]\n",
    "    best_combination = f\"{best_model_info['model']}_{best_model_info['optimizer']}\"\n",
    "    \n",
    "    print(f\"Plotting training curves for best model: {best_combination}\")\n",
    "    \n",
    "    # Plot training curves for all models\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # 1. Training & Validation Loss for best model\n",
    "    ax1 = axes[0, 0]\n",
    "    if best_combination in all_histories:\n",
    "        history = all_histories[best_combination]\n",
    "        ax1.plot(history.history['loss'], label='Training Loss', alpha=0.8)\n",
    "        ax1.plot(history.history['val_loss'], label='Validation Loss', alpha=0.8)\n",
    "        ax1.set_title(f'Learning Curves - {best_combination}')\n",
    "        ax1.set_xlabel('Epoch')\n",
    "        ax1.set_ylabel('Loss')\n",
    "        ax1.legend()\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Compare final validation loss across all combinations\n",
    "    ax2 = axes[0, 1]\n",
    "    model_names = [f\"{r['model']}_{r['optimizer']}\" for r in all_results]\n",
    "    val_losses = [r['val_loss'] for r in all_results]\n",
    "    bars = ax2.bar(range(len(model_names)), val_losses, alpha=0.7)\n",
    "    ax2.set_title('Final Validation Loss Comparison')\n",
    "    ax2.set_xlabel('Model-Optimizer Combination')\n",
    "    ax2.set_ylabel('Validation Loss')\n",
    "    ax2.set_xticks(range(len(model_names)))\n",
    "    ax2.set_xticklabels(model_names, rotation=45, ha='right')\n",
    "    \n",
    "    # Highlight the best model\n",
    "    best_idx = val_losses.index(min(val_losses))\n",
    "    bars[best_idx].set_color('gold')\n",
    "    \n",
    "    # 3. Compare correlations\n",
    "    ax3 = axes[1, 0]\n",
    "    val_corrs = [r['val_correlation'] for r in all_results]\n",
    "    bars2 = ax3.bar(range(len(model_names)), val_corrs, alpha=0.7, color='lightgreen')\n",
    "    ax3.set_title('Validation Correlation Comparison')\n",
    "    ax3.set_xlabel('Model-Optimizer Combination')\n",
    "    ax3.set_ylabel('Correlation')\n",
    "    ax3.set_xticks(range(len(model_names)))\n",
    "    ax3.set_xticklabels(model_names, rotation=45, ha='right')\n",
    "    \n",
    "    # Highlight the best correlation\n",
    "    best_corr_idx = val_corrs.index(max(val_corrs))\n",
    "    bars2[best_corr_idx].set_color('darkgreen')\n",
    "    \n",
    "    # 4. Training progress comparison (epochs needed)\n",
    "    ax4 = axes[1, 1]\n",
    "    epochs_trained = [r['epochs_trained'] for r in all_results]\n",
    "    bars3 = ax4.bar(range(len(model_names)), epochs_trained, alpha=0.7, color='lightcoral')\n",
    "    ax4.set_title('Training Epochs Needed')\n",
    "    ax4.set_xlabel('Model-Optimizer Combination')\n",
    "    ax4.set_ylabel('Epochs')\n",
    "    ax4.set_xticks(range(len(model_names)))\n",
    "    ax4.set_xticklabels(model_names, rotation=45, ha='right')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Summary statistics\n",
    "    print(f\"\\n📈 TRAINING SUMMARY:\")\n",
    "    print(f\"Average validation loss: {np.mean(val_losses):.4f} ± {np.std(val_losses):.4f}\")\n",
    "    print(f\"Average correlation: {np.mean(val_corrs):.4f} ± {np.std(val_corrs):.4f}\")\n",
    "    print(f\"Average epochs needed: {np.mean(epochs_trained):.1f} ± {np.std(epochs_trained):.1f}\")\n",
    "    \n",
    "else:\n",
    "    print(\"❌ No training histories available for plotting.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78288428",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model performance across target ranges\n",
    "print(\"\\n=== TARGET-SPECIFIC PERFORMANCE ANALYSIS ===\")\n",
    "\n",
    "if all_results and all_models:\n",
    "    # Get the best model\n",
    "    best_model_info = results_df.loc[results_df['val_loss'].idxmin()]\n",
    "    best_combination = f\"{best_model_info['model']}_{best_model_info['optimizer']}\"\n",
    "    best_model = all_models[best_combination]\n",
    "    \n",
    "    print(f\"Analyzing target-specific performance for: {best_combination}\")\n",
    "    \n",
    "    # Get predictions\n",
    "    val_predictions = best_model.predict(X_val, verbose=0).flatten()\n",
    "    \n",
    "    # Analyze performance by target value\n",
    "    target_analysis = []\n",
    "    unique_targets = sorted(np.unique(y_val))\n",
    "    \n",
    "    for target_val in unique_targets:\n",
    "        # Get indices for this target value\n",
    "        target_mask = y_val == target_val\n",
    "        target_true = y_val[target_mask]\n",
    "        target_pred = val_predictions[target_mask]\n",
    "        \n",
    "        if len(target_true) > 0:\n",
    "            mae = np.mean(np.abs(target_true - target_pred))\n",
    "            mse = np.mean((target_true - target_pred) ** 2)\n",
    "            correlation = np.corrcoef(target_true, target_pred)[0, 1] if len(target_true) > 1 else 0\n",
    "            \n",
    "            target_analysis.append({\n",
    "                'target_value': target_val,\n",
    "                'n_samples': len(target_true),\n",
    "                'mae': mae,\n",
    "                'mse': mse,\n",
    "                'correlation': correlation,\n",
    "                'mean_prediction': np.mean(target_pred),\n",
    "                'std_prediction': np.std(target_pred)\n",
    "            })\n",
    "    \n",
    "    # Create DataFrame for analysis\n",
    "    target_df = pd.DataFrame(target_analysis)\n",
    "    print(\"\\nPerformance by Target Value:\")\n",
    "    print(target_df.round(4).to_string(index=False))\n",
    "    \n",
    "    # Visualize target-specific performance\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # 1. MAE by target value\n",
    "    ax1 = axes[0, 0]\n",
    "    ax1.bar(target_df['target_value'], target_df['mae'], alpha=0.7, color='lightcoral')\n",
    "    ax1.set_title('MAE by Target Value')\n",
    "    ax1.set_xlabel('Target Value')\n",
    "    ax1.set_ylabel('Mean Absolute Error')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Correlation by target value\n",
    "    ax2 = axes[0, 1]\n",
    "    ax2.bar(target_df['target_value'], target_df['correlation'], alpha=0.7, color='lightgreen')\n",
    "    ax2.set_title('Correlation by Target Value')\n",
    "    ax2.set_xlabel('Target Value')\n",
    "    ax2.set_ylabel('Correlation')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Prediction vs True scatter for each target\n",
    "    ax3 = axes[1, 0]\n",
    "    colors = plt.cm.viridis(np.linspace(0, 1, len(unique_targets)))\n",
    "    for i, target_val in enumerate(unique_targets):\n",
    "        target_mask = y_val == target_val\n",
    "        ax3.scatter(y_val[target_mask], val_predictions[target_mask], \n",
    "                   alpha=0.6, label=f'Target {target_val}', color=colors[i])\n",
    "    \n",
    "    # Add perfect prediction line\n",
    "    min_val, max_val = y_val.min(), y_val.max()\n",
    "    ax3.plot([min_val, max_val], [min_val, max_val], 'r--', alpha=0.8, label='Perfect Prediction')\n",
    "    ax3.set_xlabel('True Values')\n",
    "    ax3.set_ylabel('Predicted Values')\n",
    "    ax3.set_title('Predictions vs True Values by Target')\n",
    "    ax3.legend()\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Prediction distribution by target\n",
    "    ax4 = axes[1, 1]\n",
    "    for i, target_val in enumerate(unique_targets):\n",
    "        target_mask = y_val == target_val\n",
    "        target_preds = val_predictions[target_mask]\n",
    "        ax4.hist(target_preds, alpha=0.6, label=f'Target {target_val}', \n",
    "                color=colors[i], bins=20)\n",
    "        # Add vertical line for true target value\n",
    "        ax4.axvline(target_val, color=colors[i], linestyle='--', alpha=0.8)\n",
    "    \n",
    "    ax4.set_xlabel('Predicted Values')\n",
    "    ax4.set_ylabel('Frequency')\n",
    "    ax4.set_title('Distribution of Predictions by Target')\n",
    "    ax4.legend()\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Check if model is learning vs memorizing distribution\n",
    "    overall_correlation = np.corrcoef(y_val, val_predictions)[0, 1]\n",
    "    prediction_std = np.std(val_predictions)\n",
    "    target_std = np.std(y_val)\n",
    "    \n",
    "    print(f\"\\n🎯 LEARNING ASSESSMENT:\")\n",
    "    print(f\"Overall correlation: {overall_correlation:.4f}\")\n",
    "    print(f\"Prediction std: {prediction_std:.4f} (target std: {target_std:.4f})\")\n",
    "    print(f\"Std ratio: {prediction_std/target_std:.4f}\")\n",
    "    \n",
    "    if overall_correlation > 0.3 and 0.5 < prediction_std/target_std < 1.5:\n",
    "        print(\"✅ Model appears to be learning meaningful patterns!\")\n",
    "    elif prediction_std < 0.01:\n",
    "        print(\"⚠️ Model may be predicting constant values\")\n",
    "    elif abs(prediction_std - target_std) < 0.001 and overall_correlation < 0.1:\n",
    "        print(\"⚠️ Model may be memorizing distribution without learning\")\n",
    "    else:\n",
    "        print(\"🤔 Model learning is unclear - review individual target performance\")\n",
    "        \n",
    "else:\n",
    "    print(\"❌ No trained models available for evaluation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2cc117",
   "metadata": {},
   "source": [
    "## Training Summary\n",
    "\n",
    "### Key Findings:\n",
    "\n",
    "1. **Best Model Performance**: The analysis above shows which model-optimizer combination performed best on the balanced dataset\n",
    "\n",
    "2. **Target-Specific Performance**: Unlike training on imbalanced data, the balanced dataset ensures the model learns patterns across all target ranges equally\n",
    "\n",
    "3. **Learning vs Memorization**: The target-specific analysis helps identify whether the model is actually learning relationships or just memorizing distributions\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. **Use the best performing model** for further experimentation\n",
    "2. **Compare with models trained on imbalanced data** to see the improvement\n",
    "3. **Test on validation/test sets** to confirm generalization\n",
    "4. **Fine-tune hyperparameters** for the best model-optimizer combination\n",
    "\n",
    "### Benefits of Balanced Training:\n",
    "\n",
    "- ✅ **Equal representation** of all target categories\n",
    "- ✅ **Reduced bias** towards middle values (0.5)\n",
    "- ✅ **Better learning** across all prediction ranges\n",
    "- ✅ **More robust evaluation** of model performance\n",
    "\n",
    "The trained models are saved in the `exports/` directory and can be loaded for further use or evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7787b1a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
