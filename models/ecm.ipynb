{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"  # hides GPU from TensorFlow\n",
    "os.environ[\"TF_XLA_FLAGS\"] = \"--tf_xla_enable_xla_devices=false\"  # disables broken XLA\n",
    "\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import cloudpickle\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Input\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from numerapi import NumerAPI\n",
    "from numerai_tools.scoring import numerai_corr, correlation_contribution\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "napi = NumerAPI()\n",
    "\n",
    "#tf.config.optimizer.set_jit(False)\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "#print(device_lib.list_local_devices())\n",
    "\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "optimizer = SGD(learning_rate=0.01)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EfficientCategoricalModel:\n",
    "    def __init__(self, data_path_train = None, data_path_val = None, data_path_metadata = None,data_path_meta_model = None, output_path='exports', batch_size=64, subset_features = \"small\", model = None, ):\n",
    "        self.data_path_train = data_path_train\n",
    "        self.data_path_val = data_path_val\n",
    "        self.data_path_metadata = data_path_metadata\n",
    "        self.data_path_meta_model = data_path_meta_model\n",
    "        self.output_path = output_path\n",
    "        self.batch_size = batch_size\n",
    "        self._subset_features = subset_features\n",
    "        self.feature_count = None\n",
    "        self.n_categories = 5  # Categories 0-4\n",
    "        self.external_model = model  # Store the external model if provided\n",
    "        os.makedirs(output_path, exist_ok=True)\n",
    "        self._feature_set = None\n",
    "        self._target_set = None\n",
    "        self.target_mapping = {0.0: 0, 0.25: 1, 0.5: 2, 0.75: 3, 1.0: 4}  # Map float targets to integers\n",
    "        self.inverse_target_mapping = {0: 0.0, 1: 0.25, 2: 0.5, 3: 0.75, 4: 1.0}  # For converting back\n",
    "        self._validation = None\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "    def _download_data(self):\n",
    "        # list the datasets and available versions\n",
    "        all_datasets = napi.list_datasets()\n",
    "        dataset_versions = list(set(d.split('/')[0] for d in all_datasets))\n",
    "        print(\"Available versions:\\n\", dataset_versions)\n",
    "\n",
    "        # Maybe add this as parameter later to change versions\n",
    "        # Set data version to one of the latest datasets\n",
    "        DATA_VERSION = \"v5.0\"\n",
    "        \n",
    "        # Print all files available for download for our version\n",
    "        current_version_files = [f for f in all_datasets if f.startswith(DATA_VERSION)]\n",
    "        print(\"Available\", DATA_VERSION, \"files:\\n\", current_version_files)\n",
    "\n",
    "        # download the feature metadata file\n",
    "        napi.download_dataset(f\"{DATA_VERSION}/features.json\")\n",
    "        napi.download_dataset(f\"{DATA_VERSION}/validation.parquet\")\n",
    "\n",
    "        self.data_path_train = f\"{DATA_VERSION}/features.json\"\n",
    "        self.data_path_val = f\"{DATA_VERSION}/validation.parquet\"\n",
    "\n",
    "\n",
    "    def _get_dataset_info(self):\n",
    "        \"\"\"Get basic information about the dataset\"\"\"\n",
    "        # read the metadata and display\n",
    "        feature_metadata = json.load(open(self.data_path_metadata))\n",
    "        for metadata in feature_metadata:\n",
    "          print(metadata, len(feature_metadata[metadata]))\n",
    "\n",
    "        feature_sets = feature_metadata[\"feature_sets\"]\n",
    "        for feature_set in [\"small\", \"medium\", \"all\"]:\n",
    "          print(feature_set, len(feature_sets[feature_set]))\n",
    "\n",
    "        feature_set = feature_sets[self._subset_features]\n",
    "        self._feature_set = feature_set\n",
    "        \n",
    "        # Open the parquet file\n",
    "        parquet_file = pq.ParquetFile(self.data_path_train)\n",
    "        \n",
    "        # Read only the first row group for a quick sample\n",
    "        df_sample = pd.read_parquet(self.data_path_train,\n",
    "                                   columns=[\"era\", \"target\"] + self._feature_set)\n",
    "\n",
    "        df_sample = df_sample.head(1)\n",
    "        # Identify feature and target columns\n",
    "        feature_cols = [col for col in df_sample.columns if col.startswith('feature_')]\n",
    "        target_cols = [col for col in df_sample.columns if col.startswith('target')]\n",
    "        \n",
    "        total_rows = parquet_file.metadata.num_rows\n",
    "        \n",
    "        self.feature_cols = feature_cols\n",
    "        self.target_cols = target_cols\n",
    "        self.feature_count = len(feature_cols)\n",
    "        self.total_rows = total_rows\n",
    "        \n",
    "        print(f\"Dataset has {total_rows:,} rows\")\n",
    "        print(f\"Found {self.feature_count} feature columns and {len(target_cols)} target columns\")\n",
    "\n",
    "\n",
    " #is not used\n",
    "    def _load_data():\n",
    "        # read the metadata and display\n",
    "        feature_metadata = json.load(open(f\"{DATA_VERSION}/features.json\"))\n",
    "        for metadata in feature_metadata:\n",
    "          print(metadata, len(feature_metadata[metadata]))\n",
    "\n",
    "        feature_sets = feature_metadata[\"feature_sets\"]\n",
    "        for feature_set in [\"small\", \"medium\", \"all\"]:\n",
    "          print(feature_set, len(feature_sets[feature_set]))\n",
    "\n",
    "        feature_set = feature_set[subset_features]\n",
    "        self._feature_set = feature_set\n",
    "\n",
    "        # Download the training data - this will take a few minutes\n",
    "        napi.download_dataset(f\"{DATA_VERSION}/train.parquet\")\n",
    "        \n",
    "        # Load only the \"medium\" feature set to\n",
    "        # Use the \"all\" feature set to use all features\n",
    "        self._train = pd.read_parquet(\n",
    "            f\"{DATA_VERSION}/train.parquet\",\n",
    "            columns=[\"era\", \"target\"] + feature_set\n",
    "        )\n",
    "        self._target_set = self._train[\"target\"]\n",
    "\n",
    "        \n",
    "\n",
    "    def plot_data(self):\n",
    "        # Plot the number of rows per era\n",
    "        slef._train.groupby(\"era\").size().plot(\n",
    "        title=\"Number of rows per era\",\n",
    "        figsize=(5, 3),\n",
    "        xlabel=\"Era\"\n",
    "        )\n",
    "\n",
    "        # Plot density histogram of the target\n",
    "        train[\"target\"].plot(\n",
    "          kind=\"hist\",\n",
    "          title=\"Target\",\n",
    "          figsize=(5, 3),\n",
    "          xlabel=\"Value\",\n",
    "          density=True,\n",
    "          bins=50\n",
    "        )\n",
    "\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 3))\n",
    "        first_era = train[train[\"era\"] == train[\"era\"].unique()[0]]\n",
    "        last_era = train[train[\"era\"] == train[\"era\"].unique()[-1]]\n",
    "        last_era[feature_set[-1]].plot(\n",
    "           title=\"5 equal bins\",\n",
    "           kind=\"hist\",\n",
    "           density=True,\n",
    "           bins=50,\n",
    "           ax=ax1\n",
    "        )\n",
    "        first_era[feature_set[-1]].plot(\n",
    "           title=\"missing data\",\n",
    "           kind=\"hist\",\n",
    "           density=True,\n",
    "           bins=50,\n",
    "           ax=ax2\n",
    "        )\n",
    "\n",
    "\n",
    "    def _create_default_model(self):\n",
    "        \"\"\"Create a simple but efficient model for categorical data\"\"\"\n",
    "        model = Sequential([\n",
    "            # Input layer\n",
    "            Input(shape=(self.feature_count,), dtype=tf.float32),\n",
    "                \n",
    "            # Hidden layers\n",
    "            Dense(128, activation='relu'),\n",
    "            #BatchNormalization(),\n",
    "            Dropout(0.3),\n",
    "            \n",
    "            Dense(64, activation='relu'),\n",
    "            #BatchNormalization(),\n",
    "            Dropout(0.2),\n",
    "            \n",
    "            # Output layer - 5 classes for our target values\n",
    "            # Output layer -1 for continous value between 0 and 1\n",
    "            Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "        \n",
    "        # Use sparse categorical crossentropy since targets are integers\n",
    "        #optimizer = Adam(learning_rate=0.001)\n",
    "        model.compile(\n",
    "            optimizer=optimizer,\n",
    "            loss='mae',\n",
    "            metrics=['mae']\n",
    "        )\n",
    "        \n",
    "        return model\n",
    "                \n",
    "                        \n",
    "\n",
    "    def export_model(self):\n",
    "        \"\"\"Simple model export\"\"\"\n",
    "        model_path = os.path.join(self.output_path, 'model.keras')\n",
    "        self.model.save(model_path)\n",
    "        print(f\"Model saved to: {model_path}\")\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    def _create_dataset_pipeline(self, data_path, is_training=True):\n",
    "        \"\"\"Create an efficient TF dataset pipeline that processes data in batches\"\"\"\n",
    "        \n",
    "        # Define dataset batch generator\n",
    "        def generator():\n",
    "            parquet_file = pq.ParquetFile(data_path)\n",
    "            \n",
    "            # Use smaller read batches to reduce memory pressure\n",
    "            read_batch_size = min(10000, self.total_rows // 50)\n",
    "            \n",
    "            for batch in parquet_file.iter_batches(batch_size=read_batch_size):\n",
    "                df_batch = batch.to_pandas()\n",
    "                \n",
    "                # Extract features and target with explicit alignment\n",
    "                X_batch = df_batch[self.feature_cols].values\n",
    "                X_batch = np.ascontiguousarray(X_batch, dtype=np.float32)\n",
    "                \n",
    "                # Convert target values to uint8 integers (0-4) with explicit alignment\n",
    "                # We keep the targert value in its original form\n",
    "                # \n",
    "                y_batch = df_batch[self.target_cols].values.squeeze()\n",
    "                y_batch = np.ascontiguousarray(y_batch ,dtype = np.float32)\n",
    "\n",
    "                # Map float values to integers\n",
    "                \"\"\"\n",
    "                for float_val, int_val in self.target_mapping.items():\n",
    "                    y_batch[np.isclose(y_float, float_val)] = int_val\n",
    "                \"\"\"\n",
    "                \n",
    "                # Yield batches with explicit alignment\n",
    "                for i in range(0, len(X_batch), self.batch_size):\n",
    "                    end_idx = min(i + self.batch_size, len(X_batch))\n",
    "                    # Create properly aligned copies\n",
    "                    x = np.ascontiguousarray(X_batch[i:end_idx])\n",
    "                    y = np.ascontiguousarray(y_batch[i:end_idx])\n",
    "                    yield x, y\n",
    "                \n",
    "                # Free memory\n",
    "                del df_batch, X_batch, y_batch\n",
    "                gc.collect()\n",
    "        \n",
    "        # Define output shapes and types\n",
    "        output_signature = (\n",
    "            tf.TensorSpec(shape=(None, self.feature_count), dtype=tf.float32),\n",
    "            tf.TensorSpec(shape=(None,), dtype=tf.float32)\n",
    "        )\n",
    "        \n",
    "        # Create dataset\n",
    "        dataset = tf.data.Dataset.from_generator(\n",
    "            generator,\n",
    "            output_signature=output_signature\n",
    "        )\n",
    "        \n",
    "        # Configure dataset for performance - reduce shuffle buffer size\n",
    "        dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
    "        \n",
    "        if is_training:\n",
    "            # Use a smaller shuffle buffer to avoid memory issues\n",
    "            dataset = dataset.shuffle(buffer_size=1000)\n",
    "            \n",
    "        return dataset  \n",
    "\n",
    "\n",
    "    def train(self, validation_split = 0.1 , epochs = 5):\n",
    "        \"\"\"Train the model using memory-efficient batch processing\"\"\"\n",
    "        \n",
    "        if self.data_path_train == None:\n",
    "            self._download_data()\n",
    "    \n",
    "        #self._load_data()\n",
    "        self._get_dataset_info()\n",
    "        \n",
    "        # Create full dataset pipeline\n",
    "        print(\"Creating dataset pipeline...\")\n",
    "        full_dataset = self._create_dataset_pipeline(self.data_path_train)\n",
    "        \n",
    "        # Calculate steps for validation data\n",
    "       \n",
    "        train_size = int(self.total_rows * (1 - validation_split))\n",
    "        \n",
    "        steps_per_epoch = train_size // self.batch_size\n",
    "        \n",
    "        validation_steps = max(1, (self.total_rows - train_size) // self.batch_size)\n",
    "        \n",
    "        # Manually split the dataset\n",
    "        val_dataset = full_dataset.take(validation_steps)\n",
    "        \n",
    "        train_dataset = full_dataset.skip(validation_steps)\n",
    "        \n",
    "        if self.external_model is not None:\n",
    "            print(\"Using provided external model...\")\n",
    "            model = self.external_model\n",
    "        else:\n",
    "            print(\"No model provided, creating default model...\")\n",
    "            model = self._create_default_model()\n",
    "\n",
    "        model.summary()\n",
    "        \n",
    "        # Callbacks for training\n",
    "        callbacks = [\n",
    "            EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True, verbose=1),\n",
    "            ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, min_lr=0.0001, verbose=1),\n",
    "        ]\n",
    "        \n",
    "        # Train with reduced steps to avoid memory issues\n",
    "        print(f\"Training model for {epochs} epochs with batch size {self.batch_size}...\")\n",
    "        history = model.fit(\n",
    "            train_dataset,\n",
    "            epochs=epochs,\n",
    "            steps_per_epoch=min(steps_per_epoch, 200),  \n",
    "            validation_data=val_dataset,\n",
    "            validation_steps=min(validation_steps, 50),\n",
    "            callbacks=callbacks,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        # Store model as instance variable\n",
    "        self.model = model\n",
    "        \n",
    "        # Simple export\n",
    "        self.export_model()\n",
    "        \n",
    "        return model, history\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Convert categorical predictions back to original float values\"\"\"\n",
    "        # Ensure proper alignment\n",
    "        X = np.ascontiguousarray(X, dtype=np.uint8)\n",
    "        \n",
    "        # Get raw predictions\n",
    "        raw_preds = self.model.predict(X)\n",
    "        \n",
    "        # Convert to class indices (0-4)\n",
    "        class_indices = np.argmax(raw_preds, axis=1)\n",
    "        \n",
    "        # Map back to float values (0.0-1.0)\n",
    "        float_predictions = np.vectorize(self.inverse_target_mapping.get)(class_indices)\n",
    "        \n",
    "        return float_predictions\n",
    "    \n",
    "\n",
    "    def validate_model(self):\n",
    "        # Load the validation data and filter for data_type == \"validation\n",
    "\n",
    "        # Load the validation data and filter for data_type == \"validation\"\n",
    "\n",
    "\n",
    "        validation = pd.read_parquet(\n",
    "            self.data_path_val,\n",
    "            columns=[\"era\", \"data_type\", \"target\"] + self._feature_set\n",
    "        )\n",
    "        validation = validation[validation[\"data_type\"] == \"validation\"]\n",
    "        del validation[\"data_type\"]\n",
    "\n",
    "        train =  pd.read_parquet(\n",
    "            self.data_path_train,\n",
    "            columns=[\"era\", \"target\"] + self._feature_set\n",
    "        )\n",
    "        \n",
    "        \n",
    "        # Downsample to every 4th era to reduce memory usage and speedup evaluation (suggested for Colab free tier)\n",
    "        # Comment out the line below to use all the data (slower and higher memory usage, but more accurate evaluation)\n",
    "        validation = validation[validation[\"era\"].isin(validation[\"era\"].unique()[::4])]\n",
    "        \n",
    "        # Eras are 1 week apart, but targets look 20 days (o 4 weeks/eras) into the future,\n",
    "        # so we need to \"embargo\" the first 4 eras following our last train era to avoid \"data leakage\"\n",
    "        last_train_era = int(train[\"era\"].unique()[-1])\n",
    "        eras_to_embargo = [str(era).zfill(4) for era in [last_train_era + i for i in range(4)]]\n",
    "        validation = validation[~validation[\"era\"].isin(eras_to_embargo)]\n",
    "        \n",
    "        # Generate predictions against the out-of-sample validation features\n",
    "        # This will take a few minutes üçµ\n",
    "        validation[\"prediction\"] = self.model.predict(validation[self._feature_set]).squeeze()\n",
    "\n",
    "        self._validation = validation\n",
    "\n",
    "        return validation\n",
    "\n",
    "    def performance_eval(self):\n",
    "\n",
    "        if self._validation is None:\n",
    "            print(\"Please run validation before evaluating the performance!\")\n",
    "            return None\n",
    "        \n",
    "        self._validation[\"meta_model\"] = pd.read_parquet(\n",
    "            self.data_path_meta_model\n",
    "        )[\"numerai_meta_model\"]\n",
    "\n",
    "        validation = self._validation\n",
    "\n",
    "        # Compute the per-era corr between our predictions and the target values\n",
    "        per_era_corr = validation.groupby(\"era\").apply(\n",
    "            lambda x: numerai_corr(x[[\"prediction\"]].dropna(), x[\"target\"].dropna())\n",
    "        )\n",
    "        \n",
    "        # Compute the per-era mmc between our predictions, the meta model, and the target values\n",
    "        per_era_mmc = validation.dropna().groupby(\"era\").apply(\n",
    "            lambda x: correlation_contribution(x[[\"prediction\"]], x[\"meta_model\"], x[\"target\"])\n",
    "        )\n",
    "        \n",
    "        \n",
    "        # Plot the per-era correlation\n",
    "        per_era_corr.plot(\n",
    "          title=\"Validation CORR\",\n",
    "          kind=\"bar\",\n",
    "          figsize=(8, 4),\n",
    "          xticks=[],\n",
    "          legend=False,\n",
    "          snap=False\n",
    "        )\n",
    "        per_era_mmc.plot(\n",
    "          title=\"Validation MMC\",\n",
    "          kind=\"bar\",\n",
    "          figsize=(8, 4),\n",
    "          xticks=[],\n",
    "          legend=False,\n",
    "          snap=False\n",
    "        )\n",
    "\n",
    "        # Plot the cumulative per-era correlation\n",
    "        per_era_corr.cumsum().plot(\n",
    "          title=\"Cumulative Validation CORR\",\n",
    "          kind=\"line\",\n",
    "          figsize=(8, 4),\n",
    "          legend=False\n",
    "        )\n",
    "        per_era_mmc.cumsum().plot(\n",
    "          title=\"Cumulative Validation MMC\",\n",
    "          kind=\"line\",\n",
    "          figsize=(8, 4),\n",
    "          legend=False\n",
    "        )\n",
    "\n",
    "        # Compute performance metrics\n",
    "        corr_mean = per_era_corr.mean()\n",
    "        corr_std = per_era_corr.std(ddof=0)\n",
    "        corr_sharpe = corr_mean / corr_std\n",
    "        corr_max_drawdown = (per_era_corr.cumsum().expanding(min_periods=1).max() - per_era_corr.cumsum()).max()\n",
    "        \n",
    "        mmc_mean = per_era_mmc.mean()\n",
    "        mmc_std = per_era_mmc.std(ddof=0)\n",
    "        mmc_sharpe = mmc_mean / mmc_std\n",
    "        mmc_max_drawdown = (per_era_mmc.cumsum().expanding(min_periods=1).max() - per_era_mmc.cumsum()).max()\n",
    "        \n",
    "        pd.DataFrame({\n",
    "            \"mean\": [corr_mean, mmc_mean],\n",
    "            \"std\": [corr_std, mmc_std],\n",
    "            \"sharpe\": [corr_sharpe, mmc_sharpe],\n",
    "            \"max_drawdown\": [corr_max_drawdown, mmc_max_drawdown]\n",
    "        }, index=[\"CORR\", \"MMC\"]).T\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    print(\"Initializing efficient categorical model training...\")\n",
    "\n",
    "    data_path = \"../data/data/train.parquet\"\n",
    "    val_path = \"../data/data/validation.parquet\"\n",
    "    meta_path = \"../data/data/features.json\"\n",
    "    meta_model = \"../data/data/meta_model.parquet\"\n",
    "    # Initialize model with smaller batch size\n",
    "    efficient_model = EfficientCategoricalModel(\n",
    "       \n",
    "        data_path_train = data_path, data_path_val = val_path,data_path_metadata = meta_path,data_path_meta_model = meta_model,\n",
    "        batch_size=64  # Small power-of-2 batch size for memory alignment\n",
    "        )\n",
    "        \n",
    "        # Train with fewer epochs\n",
    "    model, history = efficient_model.train(epochs=5)\n",
    "\n",
    "    \n",
    "\n",
    "        \n",
    "    print(\"Training complete!\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error occurred: {str(e)}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validate = efficient_model.validate_model()\n",
    "validate[[\"era\", \"prediction\", \"target\"]]\n",
    "efficient_model.performance_eval()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
